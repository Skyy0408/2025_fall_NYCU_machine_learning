{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2673913e-2d05-4516-8844-7c76395d71dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "## Written Assignment\n",
    "\n",
    "1. Read [Deep Learning: An Introduction for Applied Mathematicians](https://epubs.siam.org/doi/10.1137/18M1165748).\n",
    "   Consider a network as defined in (3.1) and (3.2). Assume that $n_L=1$, find an algorithm to calculate $\\nabla a^{[L]}(x)$.\n",
    "   \n",
    "<div align=\"center\">\n",
    "    \n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "1 & x^{\\{k\\}} \\text{ is the current training data point.} \\\\[10pt]\n",
    "2 & a^{[1]} = x^{\\{k\\}} \\\\[10pt]\n",
    "3 & \\text{For } l=2 \\text{ upto } L \\\\[10pt]\n",
    "4 & \\quad z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\\\[10pt]\n",
    "5 & \\quad a^{[l]} = \\sigma(z^{[l]}) \\\\[10pt]\n",
    "6 & \\quad D^{[l]} = \\text{diag}(\\sigma'(z^{[l]})) \\\\[10pt]\n",
    "7 & \\text{end} \\\\[10pt]\n",
    "8 & \\text{\\# Define } \\nabla a^{[L]} = \\begin{pmatrix} \\dfrac{\\partial a^{[L]}}{\\partial w_{11}} & \\dfrac{\\partial a^{[L]}}{\\partial w_{21}} & \\cdots & \\dfrac{\\partial a^{[L]}}{\\partial w_{n_l 1}} & \\dfrac{\\partial a^{[L]}}{\\partial b} \\end{pmatrix}^T \\\\[10pt]\n",
    "9 & \\text{For } i=1 \\text{ upto } n_{l}-1 \\\\[10pt]\n",
    "10 & \\quad a_i = \\sigma'(z_i^{[L]})a^{[l-1]} \\\\[10pt]\n",
    "11 & \\text{end} \\\\[10pt]\n",
    "12 & a_{n_l} = \\sigma'(z_j^{[L]})\n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: black; color: white; border: 1px solid #444444; border-radius: 5px; padding: 15px 20px; margin: 15px 0;\">\n",
    "    <b><i>Proof.</i></b>\n",
    "    <br><br>\n",
    "    <br><br>\n",
    "    First, observe that $\\dfrac{\\partial a_j^{[l]}}{\\partial z_j}=\\dfrac{\\partial\\sigma(z_j^{[L]})}{\\partial z_j}=\\sigma'(z_j^{[L]})$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\dfrac{\\partial a_j^{[L]}}{\\partial w_{mn}}&=&\\dfrac{\\partial a_j}{\\partial z_j}\\cdot\\dfrac{\\partial z_j}{\\partial w_{mn}}\\\\[10pt]\n",
    "&=&\\sigma'(z_j^{[L]})\\dfrac{\\partial}{\\partial w_{mn}}\\displaystyle\\sum_{n=1}^{n_{L-1}}\\left(w_{jn}a_n^{[L-1]}+b_j^{[L]}\\right)\\\\[10pt]\n",
    "&=&\\sigma'(z_j^{[L]})a_n^{[L-1]}\\\\[10pt]\n",
    "\\dfrac{\\partial a_j^{[L]}}{\\partial b_j}&=&\\sigma'(z_j^{[L]})\n",
    "\\end{array}\n",
    "$$\n",
    "    <span style=\"float: right;\">â–¡</span>\n",
    "</div>\n",
    "\n",
    "2. I was wondering how to fix the problem(the oscillation on the both sides) in Figure 10 below. I've tried to change the way I select the data but it doesn't work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110b72f-8ff0-43e2-ae7e-9ef86a6cf6aa",
   "metadata": {},
   "source": [
    "<hr style=\"border-style: dashed; border-color: white; border-width: 0.8px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf2e01-29a6-4f5f-a757-d14629809e03",
   "metadata": {},
   "source": [
    "## Programming Assignment\n",
    "We are asked to approximate $f(x)=\\dfrac{1}{1+25x^2}$ in $[-1,1]$ and write a short report.\n",
    "\n",
    "I think that there might be something strange if I just describe it briefly. \n",
    "\n",
    "Thus, a summary of each approach is provided below. For a more detailed process, please refer to the corresponding notebooks.\n",
    "\n",
    "# Approximate by polynomials\n",
    "I have tried to change almost every hyperparameters and the results are really bad, the model doesn't learn well and it can't add layers in the hidden part because we can increase the degree of the polynomial to have the same effect. \n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=24): 0.019440\n",
    "Final Max Error: 0.360167\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_24_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 6</b>: Polynomial Degree:24 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I have tried a lot in adjusting the hyperparameters but it was invalid; the MSE remained at $10^{-2}$ and the Max Error also remained at $10^{-1}$, which showed that the failure of the model didn't depends on the insufficiency of the training rounds or the data.\n",
    "\n",
    "I deduced that the main reason is the defect of the polynomial model.\n",
    "\n",
    "Thus, I try another method.\n",
    "\n",
    "# Approximate by Trigonometric Functions\n",
    "The way I understand the first part is something like Taylor Series, which is reminiscent of Fourier Series.\n",
    "\n",
    "In the other words, I just try to use Trigonometric Functions to deal with this problem.\n",
    "\n",
    "Before I start, note that the Runge function is an even function:\n",
    "\n",
    "$$\n",
    "f(-x)=\\dfrac{1}{1+25(-x)^2}=\\dfrac{1}{1+25x^2}=f(x)\n",
    "$$\n",
    "\n",
    "Thus, I just use the cosine functions as my hypothesis.\n",
    "\n",
    "However, it had a horrible beggining.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.014785\n",
    "Final Max Error: 0.311562\n",
    "```\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Fourier_0.01_20000_1001.png\" alt=\"Fourier 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 7</b>: Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I found that the shape did not match my expectations as well as I had imagined, and I have tried to add more data, but it wasn't effective.\n",
    "\n",
    "The result did not improve significantly until I add layers in the hidden part, and the middle part almost match the shape of the target, nevertheless, it oscillates severely on both sides, especially for the lowest learning rate, which is shown in Figure 12.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000074\n",
    "Final Max Error: 0.022620\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I found that the learning rate I had set was too large for this model, and it made the gradient \"explode\". \n",
    "\n",
    "Thus, I decrease the learning rate.\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000265\n",
    "Final Max Error: 0.076985\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.001 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.001_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 11</b>: Fourier Learning rate:0.001 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.003896\n",
    "Final Max Error: 0.226449\n",
    "```\n",
    "<figure id=\"Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.0001_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 12</b>: Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "After adjusting the hyperparameters, I find that even if the graph in Figure 10 almost matches the function we want to approach, but we can also see the oscillations on both sides.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000074\n",
    "Final Max Error: 0.022620\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I consider this to be a form of overfitting, however, a better performence in the middle part is undeniable.\n",
    "\n",
    "Therefore, I try to adjust the way I select the data.\n",
    "\n",
    "```python\n",
    "x_center = jnp.linspace(-0.5, 0.5, datanum//2)\n",
    "x_left = jnp.linspace(-1.0, -0.5, datanum//4)\n",
    "x_right = jnp.linspace(0.5, 1.0, datanum//4)\n",
    "x_train = jnp.concatenate([x_center, x_left, x_right])\n",
    "```\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.001097\n",
    "Final Max Error: 0.093484\n",
    "```\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10000\">\n",
    "    <img src=\"Fourier_0.01_20000_10000_3_s.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10'</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10000.</figcaption>\n",
    "</figure>\n",
    "\n",
    "However, it did not work, also the oscillation on both sides still existed.\n",
    "\n",
    "Then, I try another function we have learned in class, the sigmoid function.\n",
    "\n",
    "\n",
    "# Approximate by Sigmoid Function\n",
    "At first, I start it with a single layer in the hidden part, and its results are better than the others.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000715\n",
    "Final Max Error: 0.074811\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 15</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000003\n",
    "Final Max Error: 0.004661\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_10001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 16</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "The most fascinating part is even if I just tried to enhance the number of data, the MSE dip to $0.000003$.\n",
    "\n",
    "In other words, even before deepening the network, providing sufficient data dramatically improved the model's performance.\n",
    "\n",
    "Then, I tried to add more layers to the hidden part.\n",
    "\n",
    "The program below is the revised version, which adds more layers.\n",
    "\n",
    "We can find that the middle part has a gap but the \"awful oscillation\" didn't appear again.\n",
    "\n",
    "After this, I try to add some layers in the hidden part, and we can get this beautiful result.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000001\n",
    "Final Max Error: 0.002345\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 20</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can find that it perfectly match the true graph, and the loss decreased to $10^{-6}$. \n",
    "\n",
    "Thus, I decided to use it to take the extra task, compute the derivative, and the results are presented below.\n",
    "\n",
    "* Extra: Compute the Derivative\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000001\n",
    "Final Max Error: 0.002345\n",
    "--- Final Result ---\n",
    "Final MSE(Derivative): 0.000435\n",
    "Final Max Error(Derivative): 0.073273\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3_d.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 20'</b>: Based on 20.</figcaption>\n",
    "</figure>\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3_d'.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 21</b>: Compute Derivative.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<hr style=\"border-style: dashed; border-color: white; border-width: 2px;\">\n",
    "\n",
    "# Program\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 40000\n",
    "datanum = 10001\n",
    "batch_size = 32 # Mini-Batch (I only use it in the Sigmoid model.)\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)), 'b1': jax.random.normal(b1_key, (16,)),\n",
    "    'w2': jax.random.normal(w2_key, (16, 16)), 'b2': jax.random.normal(b2_key, (16,)),\n",
    "    'w3': jax.random.normal(w3_key, (16, 1)), 'b3': jax.random.normal(b3_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Sigmoid model\n",
    "def deep_sigmoid_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.sigmoid(x @ params['w1'] + params['b1'])\n",
    "    hidden2 = jax.nn.sigmoid(hidden1 @ params['w2'] + params['b2'])\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_sigmoid_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "\n",
    "num_train = len(x_train)\n",
    "steps_per_epoch = num_train // batch_size\n",
    "\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)  # Shuffle(I only use it in the Sigmoid model.)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_idx = perm[step * batch_size : (step + 1) * batch_size]\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        params = update_step(params, x_batch, y_batch, learning_rate)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_sigmoid_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_sigmoid_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.title('Deep Sigmoid Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Derivative\n",
    "def model_for_grad(x, model_params):\n",
    "    return deep_sigmoid_model(model_params, x).squeeze()\n",
    "\n",
    "grad_f = jax.grad(f)\n",
    "y_d_true = jax.vmap(grad_f)(x_plot)\n",
    "model_derivative_fn = jax.grad(model_for_grad, argnums=0)\n",
    "vectorized_model_derivative = jax.vmap(model_derivative_fn, in_axes=(0, None))\n",
    "y_d_pred = vectorized_model_derivative(x_plot, params)\n",
    "pred_derivative_on_train = vectorized_model_derivative(x_train, params)\n",
    "true_derivative_on_train = jax.vmap(grad_f)(x_train)\n",
    "final_derivative_mse = jnp.mean((pred_derivative_on_train - true_derivative_on_train)**2)\n",
    "max_derivative_error = jnp.max(jnp.abs(y_d_pred-y_d_true))\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE(Derivative): {final_derivative_mse:.6f}\")\n",
    "print(f\"Final Max Error(Derivative): {max_derivative_error:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x_plot, y_d_true, label='True Derivative of Runge Function')\n",
    "plt.plot(x_plot, y_d_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.title(\"Comparison of Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ed436-8ef2-4647-b8b3-db1f6002361d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
