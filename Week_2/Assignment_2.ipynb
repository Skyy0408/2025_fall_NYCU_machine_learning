{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2673913e-2d05-4516-8844-7c76395d71dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Assignment 2\n",
    "\n",
    "## Written Assignment\n",
    "\n",
    "<div align=\"center\">\n",
    "    \n",
    "$$\n",
    "\\begin{array}{cl}\n",
    "1 & x^{\\{k\\}} \\text{ is the current training data point.} \\\\[10pt]\n",
    "2 & a^{[1]} = x^{\\{k\\}} \\\\[10pt]\n",
    "3 & \\text{For } l=2 \\text{ upto } L \\\\[10pt]\n",
    "4 & \\quad z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} \\\\[10pt]\n",
    "5 & \\quad a^{[l]} = \\sigma(z^{[l]}) \\\\[10pt]\n",
    "6 & \\quad D^{[l]} = \\text{diag}(\\sigma'(z^{[l]})) \\\\[10pt]\n",
    "7 & \\text{end} \\\\[10pt]\n",
    "8 & \\text{\\# Define } \\nabla a^{[L]} = \\begin{pmatrix} \\dfrac{\\partial a^{[L]}}{\\partial w_{11}} & \\dfrac{\\partial a^{[L]}}{\\partial w_{21}} & \\cdots & \\dfrac{\\partial a^{[L]}}{\\partial w_{n_l 1}} & \\dfrac{\\partial a^{[L]}}{\\partial b} \\end{pmatrix}^T \\\\[10pt]\n",
    "9 & \\text{For } i=1 \\text{ upto } n_{l}-1 \\\\[10pt]\n",
    "10 & \\quad a_i = \\sigma'(z_i^{[L]})a^{[l-1]} \\\\[10pt]\n",
    "11 & \\text{end} \\\\[10pt]\n",
    "12 & a_{n_l} = \\sigma'(z_j^{[L]})\n",
    "\\end{array}\n",
    "$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: black; color: white; border: 1px solid #444444; border-radius: 5px; padding: 15px 20px; margin: 15px 0;\">\n",
    "    <b><i>Proof.</i></b>\n",
    "    <br><br>\n",
    "    <br><br>\n",
    "    First, observe that $\\dfrac{\\partial a_j^{[l]}}{\\partial z_j}=\\dfrac{\\partial\\sigma(z_j^{[L]})}{\\partial z_j}=\\sigma'(z_j^{[L]})$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\dfrac{\\partial a_j^{[L]}}{\\partial w_{mn}}&=&\\dfrac{\\partial a_j}{\\partial z_j}\\cdot\\dfrac{\\partial z_j}{\\partial w_{mn}}\\\\[10pt]\n",
    "&=&\\sigma'(z_j^{[L]})\\dfrac{\\partial}{\\partial w_{mn}}\\displaystyle\\sum_{n=1}^{n_{L-1}}\\left(w_{jn}a_n^{[L-1]}+b_j^{[L]}\\right)\\\\[10pt]\n",
    "&=&\\sigma'(z_j^{[L]})a_n^{[L-1]}\\\\[10pt]\n",
    "\\dfrac{\\partial a_j^{[L]}}{\\partial b_j}&=&\\sigma'(z_j^{[L]})\n",
    "\\end{array}\n",
    "$$\n",
    "    <span style=\"float: right;\">â–¡</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110b72f-8ff0-43e2-ae7e-9ef86a6cf6aa",
   "metadata": {},
   "source": [
    "<hr style=\"border-style: dashed; border-color: white; border-width: 0.8px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf2e01-29a6-4f5f-a757-d14629809e03",
   "metadata": {},
   "source": [
    "## Programmong Assignment\n",
    "We are asked to approximate $f(x)=\\dfrac{1}{1+25x^2}$ in $[-1,1]$ and write a short report.\n",
    "\n",
    "I think that there might be something strange if I just descibe it briefly. \n",
    "\n",
    "Thus, I roughly put them below, and if you have some questions, you can just read it completely.\n",
    "\n",
    "# Approximate by polynomials\n",
    "I have try to change almost every hyperparamyters and the results are really bad, the model doesn't learn well and it can't add layers in the hidden part because we can increase the degree of the polynomial to have the same effect. \n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=24): 0.019440\n",
    "Final Max Error: 0.360167\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_24_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 6</b>: Polynomial Degree:24 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Thus, I try another method.\n",
    "\n",
    "# Approximate by Trigonometric Functions\n",
    "The way I understand the first part is something like Taylor Series, which is reminiscent of Fourier Series.\n",
    "\n",
    "In the other words, I just try to use Trigonometric Functions to deal with this problem.\n",
    "\n",
    "The result is not really well until I add layers in the hidden part, and the middle part almost match the shape of the target, however, it oscillates severely in the both sides, especially for the lowest learning rate, which is shown in Figure 12.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.003896\n",
    "Final Max Error: 0.226449\n",
    "```\n",
    "<figure id=\"Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.0001_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 12</b>: Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "After I adjusting the hyperparameters, I find that even if the graph in Figure 10 almost match the function we want to approach, but we can also see the osscilations in both sides.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000074\n",
    "Final Max Error: 0.022620\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Then, I try another function we have learned in class, the sigmoid function.\n",
    "\n",
    "\n",
    "# Approximate by Sigmoid Function\n",
    "At first, I start it with a single layer in the hidden part, and its results are better then the others.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000203\n",
    "Final Max Error: 0.040410\n",
    "```\n",
    "\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 16</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can find that the middle part has a gap but the \"awful oscillation\" doesn't appear again.\n",
    "\n",
    "After this, I try to add some layers in the hidden parts, and we can get this beautiful result.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000001\n",
    "Final Max Error: 0.002345\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 14</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can find that it perfectly match the true graph, and the loss decrease to $10^{-6}$. \n",
    "\n",
    "Thus, I decided to use it to take the extra task, compute the derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe81d5-38cf-409c-b8f9-027d3b81d983",
   "metadata": {},
   "source": [
    "<hr style=\"border-style: dashed; border-color: white; border-width: 3px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17143aa9-dea6-474c-a604-147518cd41f4",
   "metadata": {},
   "source": [
    "# Approximate by Polynomials\n",
    "\n",
    "The main program is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446a986-7579-4e1d-9625-2d1b51e8b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "POLYNOMIAL_DEGREE = 12\n",
    "learning_rate = 0.01\n",
    "epochs = 40000\n",
    "datanum = 1000\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key=jax.random.PRNGKey(0)\n",
    "key, w_key, b_key = jax.random.split(key, 3)\n",
    "params = {\n",
    "    'w': jax.random.normal(w_key, (POLYNOMIAL_DEGREE, 1)), \n",
    "    'b': jax.random.normal(b_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Polynomial Model\n",
    "def polynomial_model(params, x):\n",
    "    x_col = x.reshape(-1, 1)\n",
    "    exponents = jnp.arange(1, POLYNOMIAL_DEGREE + 1)\n",
    "    features = jnp.power(x_col, exponents)\n",
    "    return features @ params['w'] + params['b']\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = polynomial_model(params, x).squeeze() \n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params = update_step(params, x_train, y_train, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = polynomial_model(params, x_plot).squeeze()\n",
    "\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(polynomial_model(params, x_train).squeeze() - y_train))\n",
    "\n",
    "# Result\n",
    "print(\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE (Degree={POLYNOMIAL_DEGREE}): {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function', color='blue')\n",
    "plt.plot(x_plot, y_pred, label='Neural Network Prediction', color='red', linestyle='--')\n",
    "plt.scatter(x_train, y_train, s=10, color='gray', alpha=0.5, label='Training Data')\n",
    "plt.title('Function Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33228827-0e9d-4513-9146-b46cd57f7323",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.024220\n",
    "Final Max Error: 0.413023\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_20000_1001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 1</b>: Polynomial Degree:12 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018068\n",
    "Final Max Error: 0.372409\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_1001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 2</b>: Polynomial Degree:12 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I think that the model doesn't learn well, maybe the number of data just to small.\n",
    "\n",
    "Howevere, the results is:\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018105\n",
    "Final Max Error: 0.372532\n",
    "```\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_10001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 3</b>: Polynomial Degree:12 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018109\n",
    "Final Max Error: 0.372544\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 4</b>: Polynomial Degree:12 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "For another direction, increase the number of epoch.\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.014324\n",
    "Final Max Error: 0.328050\n",
    "```\n",
    "<figure id=\"Polynomial 1 Epoch:80000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_80000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 5</b>: Polynomial Degree:12 Epoch:80000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "That is, we can't expect it more.\n",
    "\n",
    "Plus, even if we try to stack the hidden layer, it just modify the degree of our $\\sigma$.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=24): 0.019440\n",
    "Final Max Error: 0.360167\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_24_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 6</b>: Polynomial Degree:24 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Thus, I try to use another way to approximate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e55201-103c-4f25-b448-aeb90ca55c05",
   "metadata": {},
   "source": [
    "# Approximate by Trigonometric Functions\n",
    "The way I understand the first part is something like Taylor Series, which is reminiscent of Fourier Series.\n",
    "\n",
    "In the other words, I just try to use Trigonometric Functions to deal with this problem.\n",
    "\n",
    "The main proram is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399c4ae-a9ba-46f1-b65f-075ceaebaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "datanum = 1001\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key=jax.random.PRNGKey(0)\n",
    "key, w1_key, w2_key, b_key = jax.random.split(key, 4)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)), \n",
    "    'w2': jax.random.normal(w2_key, (16, 1)), \n",
    "    'b': jax.random.normal(b_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Trigonometric Model\n",
    "def trigonometric_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden = jnp.cos(x @ params['w1'])\n",
    "    return hidden @ params['w2'] + params['b']\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = trigonometric_model(params, x).squeeze() \n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params = update_step(params, x_train, y_train, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = trigonometric_model(params, x_plot).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(trigonometric_model(params, x_train).squeeze() - y_train))\n",
    "\n",
    "# Result\n",
    "print(\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function', color='blue')\n",
    "plt.plot(x_plot, y_pred, label='Neural Network Prediction', color='red', linestyle='--')\n",
    "plt.scatter(x_train, y_train, s=10, color='gray', alpha=0.5, label='Training Data')\n",
    "plt.title('Function Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c812f-b5ad-4fc3-bb5e-cbdf042fb2e2",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.014785\n",
    "Final Max Error: 0.311562\n",
    "```\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Fourier_0.01_20000_1001.png\" alt=\"Fourier 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 7</b>: Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.014769\n",
    "Final Max Error: 0.311453\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001.png\" alt=\"Fourier 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 8</b>: Fourier Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I find that the shape doesn't as match as I imagined, so I try to add more layers in the hidden part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba72e4-4b59-46a6-bb17-77462b469e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "datanum = 10001\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key=jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, 7)\n",
    "params = {\n",
    "    'w1': jax.random.normal(keys[0], (1, 16)),\n",
    "    \n",
    "    'w2': jax.random.normal(keys[1], (16, 16)),\n",
    "    'b2': jax.random.normal(keys[2], (16,)),\n",
    "\n",
    "    'w3': jax.random.normal(keys[3], (16, 16)),                                                                   # Added\n",
    "    'b3': jax.random.normal(keys[4], (16,)),                                                                      # Added\n",
    "\n",
    "    'w4': jax.random.normal(keys[5], (16, 1)),                                                                    # Added\n",
    "    'b4': jax.random.normal(keys[6], (1,))                                                                        # Added\n",
    "}\n",
    "\n",
    "# Construct the Trigonometric Model\n",
    "def deep_trigonometric_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    features = jnp.cos(x @ params['w1'])\n",
    "    hidden1 = jnp.cos(features @ params['w2'] + params['b2'])\n",
    "    hidden2 = jnp.cos(hidden1 @ params['w3'] + params['b3'])\n",
    "    output = hidden2 @ params['w4'] + params['b4']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_trigonometric_model(params, x).squeeze() \n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params = update_step(params, x_train, y_train, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_trigonometric_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_trigonometric_model(params, x_train).squeeze()\n",
    "final_mse = jnp.mean((y_pred_train - y_train)**2)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "print(\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function', color='blue')\n",
    "plt.plot(x_plot, y_pred, label='Neural Network Prediction', color='red', linestyle='--')\n",
    "plt.scatter(x_train, y_train, s=10, color='gray', alpha=0.5, label='Training Data')\n",
    "plt.title('Function Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20229f-07b8-4e64-b103-5d51edf5d40e",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.001284\n",
    "Final Max Error: 0.091240\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:2 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_2.png\" alt=\"Fourier 2 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 9</b>: Fourier Learning rate:0.01 Layer:2 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "It matches my assumption, although there exists some weird osscilation on the right hand side, this results make me tend to add a layer in the hidden part.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000074\n",
    "Final Max Error: 0.022620\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I find that the learning rate I set is too large to it and it makes the gradient \"explode\". \n",
    "\n",
    "Thus, I decrease the learning rate.\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000265\n",
    "Final Max Error: 0.076985\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.001 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.001_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 11</b>: Fourier Learning rate:0.001 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.003896\n",
    "Final Max Error: 0.226449\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.0001_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 12</b>: Fourier Learning rate:0.0001 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The middle part almost match the shape of the target, however, it oscillates severely in the both sides, especially for the lowest learning rate, which is shown in Figure 12.\n",
    "\n",
    "After this, I just removed one layer, which means that we have 2 layers hidden now, and try to give it more data.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000896\n",
    "Final Max Error: 0.081194\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:2 Epoch:40000 Datanum:1000001\">\n",
    "    <img src=\"Fourier_0.01_40000_1000001_2.png\" alt=\"Fourier 2 Layers Random with huge data\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 13</b>: Fourier Learning rate:0.01 Layer:2 Epoch:40000 Datanum:1000001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.002676\n",
    "Final Max Error: 0.135742\n",
    "```\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.001 Layer:2 Epoch:40000 Datanum:1000001\">\n",
    "    <img src=\"Fourier_0.001_40000_1000001_2.png\" alt=\"Fourier 2 Layers Random with huge data\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 14</b>: Fourier Learning rate:0.001 Layer:2 Epoch:40000 Datanum:1000001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In conclusion, even if the graph in Figure 10 almost match the function we want to approach, but we can also see the osscilations in both sides.\n",
    "\n",
    "Then, I try another function we have learned in class, the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92390671-bea0-4939-9c5d-48b73cfb7dcc",
   "metadata": {},
   "source": [
    "<hr style=\"border-style: dashed; border-color: white; border-width: 0.8px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50431feb-0c77-4034-b180-b5a7eaf2ef55",
   "metadata": {},
   "source": [
    "# Approximate by Sigmoid Function\n",
    "\n",
    "The main proram is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9dd16-74e9-49b4-9915-a1198659847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1 / (1 + 25 * x**2)\n",
    "    \n",
    "# The Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "datanum = 10001\n",
    "batch_size = 32 \n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "# Construct the Sigmoid Model\n",
    "def sigmoid_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden = sigmoid(x @ params['w1'] + params['b1'])\n",
    "    return hidden @ params['w2'] + params['b2']\n",
    "    \n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key = jax.random.split(key, 5)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)),\n",
    "    'b1': jax.random.normal(b1_key, (16,)),\n",
    "    \n",
    "    'w2': jax.random.normal(w2_key, (16, 1)),\n",
    "    'b2': jax.random.normal(b2_key, (1,))\n",
    "}\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = sigmoid_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "num_train = len(x_train)\n",
    "steps_per_epoch = num_train // batch_size\n",
    "\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_idx = perm[step * batch_size : (step + 1) * batch_size]\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        params = update_step(params, x_batch, y_batch, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "\n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = sigmoid_model(params, x_plot).squeeze()\n",
    "y_pred_train = sigmoid_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='1-Hidden-Layer NN', linestyle='--')\n",
    "plt.title('Single Hidden Layer Network')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e0ece-9e82-4147-9208-168e2a18a916",
   "metadata": {},
   "source": [
    "I start with single hidden layer.\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000715\n",
    "Final Max Error: 0.074811\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 15</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000203\n",
    "Final Max Error: 0.040410\n",
    "```\n",
    "\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 16</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Then, again, add layers in the hidden part.\n",
    "\n",
    "The program below is the revised one, which add more layers in the hidden part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807df4a-a172-48c5-adce-9cf07f68f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# The Sigmoid Function \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + jnp.exp(-x))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 40000 \n",
    "datanum = 10001\n",
    "batch_size = 32 \n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)), 'b1': jax.random.normal(b1_key, (16,)),\n",
    "    'w2': jax.random.normal(w2_key, (16, 16)), 'b2': jax.random.normal(b2_key, (16,)),\n",
    "    'w3': jax.random.normal(w3_key, (16, 1)), 'b3': jax.random.normal(b3_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Sigmoid model\n",
    "def deep_sigmoid_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = sigmoid(x @ params['w1'] + params['b1'])\n",
    "    hidden2 = sigmoid(hidden1 @ params['w2'] + params['b2'])\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_sigmoid_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "\n",
    "num_train = len(x_train)\n",
    "steps_per_epoch = num_train // batch_size\n",
    "\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_idx = perm[step * batch_size : (step + 1) * batch_size]\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        params = update_step(params, x_batch, y_batch, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_sigmoid_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_sigmoid_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.title('Deep Sigmoid Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c9f9c-2e4b-4b8d-ba59-61c491f19afb",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000016\n",
    "Final Max Error: 0.010527\n",
    "```\n",
    "\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_1001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 14</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000001\n",
    "Final Max Error: 0.002345\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 14</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can find that it perfectly match the true graph, and the loss decrease to $10^{-6}$. \n",
    "\n",
    "Thus, I decided to use it to take the extra task, compute the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ed001-bca9-4f3a-aa0c-80ce7d4e9691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
