{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc52f64-732b-449f-bd2a-b620d6731deb",
   "metadata": {},
   "source": [
    "# Approximate by Polynomials\n",
    "\n",
    "The main program is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530ccd6-b3c6-4630-a191-185f5bb81e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "POLYNOMIAL_DEGREE = 12\n",
    "learning_rate = 0.01\n",
    "epochs = 40000\n",
    "datanum = 1000\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key=jax.random.PRNGKey(0)\n",
    "key, w_key, b_key = jax.random.split(key, 3)\n",
    "params = {\n",
    "    'w': jax.random.normal(w_key, (POLYNOMIAL_DEGREE, 1)), \n",
    "    'b': jax.random.normal(b_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Polynomial Model\n",
    "def polynomial_model(params, x):\n",
    "    x_col = x.reshape(-1, 1)\n",
    "    exponents = jnp.arange(1, POLYNOMIAL_DEGREE + 1)\n",
    "    features = jnp.power(x_col, exponents)\n",
    "    return features @ params['w'] + params['b']\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = polynomial_model(params, x).squeeze() \n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params = update_step(params, x_train, y_train, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = polynomial_model(params, x_plot).squeeze()\n",
    "\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(polynomial_model(params, x_train).squeeze() - y_train))\n",
    "\n",
    "# Result\n",
    "print(\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE (Degree={POLYNOMIAL_DEGREE}): {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function', color='blue')\n",
    "plt.plot(x_plot, y_pred, label='Neural Network Prediction', color='red', linestyle='--')\n",
    "plt.scatter(x_train, y_train, s=10, color='gray', alpha=0.5, label='Training Data')\n",
    "plt.title('Function Approximation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error Loss')\n",
    "plt.yscale('log') \n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5405fd-b082-43ba-a988-6ccf5d49ea63",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.024220\n",
    "Final Max Error: 0.413023\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_20000_1001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 1</b>: Polynomial Degree:12 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018068\n",
    "Final Max Error: 0.372409\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_1001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 2</b>: Polynomial Degree:12 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I think that the model does not learn well; perhaps the number of data points is too small.\n",
    "\n",
    "However, the result is:\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018105\n",
    "Final Max Error: 0.372532\n",
    "```\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_10001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 3</b>: Polynomial Degree:12 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.018109\n",
    "Final Max Error: 0.372544\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 4</b>: Polynomial Degree:12 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Next, I tried increasing the number of epochs.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=12): 0.014324\n",
    "Final Max Error: 0.328050\n",
    "```\n",
    "<figure id=\"Polynomial 1 Epoch:80000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_12_0.01_80000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 5</b>: Polynomial Degree:12 Epoch:80000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "This shows that we cannot expect much further improvement from this approach.\n",
    "\n",
    "Plus, even if we try to stack the hidden layer, it just modify the degree of our $\\sigma$.\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE (Degree=24): 0.019440\n",
    "Final Max Error: 0.360167\n",
    "```\n",
    "\n",
    "<figure id=\"Polynomial 1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Polynomial_24_0.01_40000_100001.png\" alt=\"Polynomial 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 6</b>: Polynomial Degree:24 Epoch:40000 Datanum:100001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Thus, I tried another way to approximate it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
