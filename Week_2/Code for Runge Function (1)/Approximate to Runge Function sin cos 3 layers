import jax
import jax.numpy as jnp
import matplotlib.pyplot as plt

jax.config.update("jax_enable_x64", True)

# The Runge Function
def f(x):
    return 1/(1+25*x**2)

# Hyperparameters
learning_rate = 0.01
epochs = 20000
datanum = 10001

# Data
x_train = jnp.linspace(-1.0, 1.0, datanum)
y_train = f(x_train)

key=jax.random.PRNGKey(0)
keys = jax.random.split(key, 7)
params = {
    'w1': jax.random.normal(keys[0], (1, 16)),
    
    'w2': jax.random.normal(keys[1], (16, 16)),
    'b2': jax.random.normal(keys[2], (16,)),

    'w3': jax.random.normal(keys[3], (16, 16)),                                                                   # Added
    'b3': jax.random.normal(keys[4], (16,)),                                                                      # Added

    'w4': jax.random.normal(keys[5], (16, 1)),                                                                    # Added
    'b4': jax.random.normal(keys[6], (1,))                                                                        # Added
}

# Construct the Trigonometric Model
def deep_trigonometric_model(params, x):
    x = x.reshape(-1, 1)
    features = jnp.cos(x @ params['w1'])
    hidden1 = jnp.cos(features @ params['w2'] + params['b2'])
    hidden2 = jnp.cos(hidden1 @ params['w3'] + params['b3'])
    output = hidden2 @ params['w4'] + params['b4']
    return output

def loss_fn(params, x, y):
    predictions = deep_trigonometric_model(params, x).squeeze() 
    return jnp.mean((predictions - y)**2)
loss_history = []

@jax.jit
# Gradient Descent
def update_step(params, x, y, learning_rate):
    grads = jax.grad(loss_fn)(params, x, y)
    return jax.tree.map(lambda p, g: p - learning_rate * g, params, grads)

for epoch in range(epochs):
    params = update_step(params, x_train, y_train, learning_rate)
    
    if epoch % 1000 == 0:
        loss = loss_fn(params, x_train, y_train)
        loss_history.append(loss)
        print(f"Epoch {epoch}, Loss: {loss:.6f}")
        
x_plot = jnp.linspace(-1, 1, 500)
y_true = f(x_plot)
y_pred = deep_trigonometric_model(params, x_plot).squeeze()
y_pred_train = deep_trigonometric_model(params, x_train).squeeze()
final_mse = jnp.mean((y_pred_train - y_train)**2)
max_error = jnp.max(jnp.abs(y_pred_train - y_train))

# Result
print("\n--- Final Result ---")
print(f"Final MSE: {final_mse:.6f}")
print(f"Final Max Error: {max_error:.6f}")

# Plotting
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(x_plot, y_true, label='True Runge Function', color='blue')
plt.plot(x_plot, y_pred, label='Neural Network Prediction', color='red', linestyle='--')
plt.scatter(x_train, y_train, s=10, color='gray', alpha=0.5, label='Training Data')
plt.title('Function Approximation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(range(0, epochs, 1000), loss_history)
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Mean Squared Error Loss')
plt.yscale('log') 
plt.grid(True)

plt.tight_layout()
plt.show()