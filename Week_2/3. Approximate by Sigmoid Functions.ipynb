{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c94170-9214-4bb5-b661-73078b7d0425",
   "metadata": {},
   "source": [
    "# Approximate by Sigmoid Function\n",
    "\n",
    "The main program is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d772c-ec74-4bdb-bb70-44cf18c72918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1 / (1 + 25 * x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "datanum = 10001\n",
    "batch_size = 32 \n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "# Construct the Sigmoid Model\n",
    "def sigmoid_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden = jax.nn.sigmoid(x @ params['w1'] + params['b1'])\n",
    "    return hidden @ params['w2'] + params['b2']\n",
    "    \n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key = jax.random.split(key, 5)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)),\n",
    "    'b1': jax.random.normal(b1_key, (16,)),\n",
    "    \n",
    "    'w2': jax.random.normal(w2_key, (16, 1)),\n",
    "    'b2': jax.random.normal(b2_key, (1,))\n",
    "}\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = sigmoid_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "num_train = len(x_train)\n",
    "steps_per_epoch = num_train // batch_size\n",
    "\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_idx = perm[step * batch_size : (step + 1) * batch_size]\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        params = update_step(params, x_batch, y_batch, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "\n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = sigmoid_model(params, x_plot).squeeze()\n",
    "y_pred_train = sigmoid_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='1-Hidden-Layer NN', linestyle='--')\n",
    "plt.title('Single Hidden Layer Network')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b7bb6-2d55-4fea-a030-32d9d32933d6",
   "metadata": {},
   "source": [
    "I started with single hidden layer.\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000715\n",
    "Final Max Error: 0.074811\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 15</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000003\n",
    "Final Max Error: 0.004661\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_10001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 16</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000203\n",
    "Final Max Error: 0.040410\n",
    "```\n",
    "\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_1001.png\" alt=\"Sigmoid 1 Layer\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 17</b>: Sigmoid Learning rate:0.01 Layer:1 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Then, I tried adding more layers to the hidden part.\n",
    "\n",
    "The program below is the revised version, which adds more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082a6f4-bbb7-4e28-89a5-484a94f0a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 40000 \n",
    "datanum = 10001\n",
    "batch_size = 32 \n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)), 'b1': jax.random.normal(b1_key, (16,)),\n",
    "    'w2': jax.random.normal(w2_key, (16, 16)), 'b2': jax.random.normal(b2_key, (16,)),\n",
    "    'w3': jax.random.normal(w3_key, (16, 1)), 'b3': jax.random.normal(b3_key, (1,))\n",
    "}\n",
    "\n",
    "# Construct the Sigmoid model\n",
    "def deep_sigmoid_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.sigmoid(x @ params['w1'] + params['b1'])\n",
    "    hidden2 = jax.nn.sigmoid(hidden1 @ params['w2'] + params['b2'])\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_sigmoid_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "loss_history = []\n",
    "\n",
    "@jax.jit\n",
    "# Gradient Descent\n",
    "def update_step(params, x, y, learning_rate):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, params, grads)\n",
    "\n",
    "\n",
    "num_train = len(x_train)\n",
    "steps_per_epoch = num_train // batch_size\n",
    "\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_idx = perm[step * batch_size : (step + 1) * batch_size]\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        params = update_step(params, x_batch, y_batch, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "x_plot = jnp.linspace(-1, 1, 500)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_sigmoid_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_sigmoid_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "# Result\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.title('Deep Sigmoid Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9686e40-ecd9-4c4f-91a6-91b7b93752ad",
   "metadata": {},
   "source": [
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000016\n",
    "Final Max Error: 0.010527\n",
    "```\n",
    "\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:1001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_1001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 18</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:1001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000046\n",
    "Final Max Error: 0.019953\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_20000_10001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 19</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "```text\n",
    "--- Final Result ---\n",
    "Final MSE: 0.000001\n",
    "Final Max Error: 0.002345\n",
    "```\n",
    "<figure id=\"Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001\">\n",
    "    <img src=\"Sigmoid_0.01_40000_10001_3.png\" alt=\"Sigmoid 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 20</b>: Sigmoid Learning rate:0.01 Layer:3 Epoch:40000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can see that it perfectly matches the true graph, and the loss decrease to $10^{-6}$. \n",
    "\n",
    "Thus, I decided to use it to take the extra task, compute the derivative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
