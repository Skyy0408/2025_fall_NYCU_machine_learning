{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f3c6e-cd3c-433c-9d9e-a70849099c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# JAX and related libraries\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, brier_score_loss, mean_squared_error, mean_absolute_error, max_error\n",
    "\n",
    "# --- 1. QDA Core Functions ---\n",
    "\n",
    "def fit_qda(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Calculates the required parameters for the QDA model from the training data.\n",
    "    \"\"\"\n",
    "    print(\"Fitting QDA parameters from training data...\")\n",
    "    X_train, y_train = jnp.array(X_train), jnp.array(y_train)\n",
    "    classes = np.unique(np.array(y_train))\n",
    "    \n",
    "    priors, means, covariances = {}, {}, {}\n",
    "\n",
    "    for k in classes:\n",
    "        X_k = X_train[y_train == k]\n",
    "        priors[k] = X_k.shape[0] / X_train.shape[0]\n",
    "        means[k] = jnp.mean(X_k, axis=0)\n",
    "        covariances[k] = jnp.cov(X_k, rowvar=False, bias=True)\n",
    "        \n",
    "    print(\"Fitting complete.\")\n",
    "    return priors, means, covariances\n",
    "\n",
    "@jax.jit\n",
    "def predict_qda(params, X):\n",
    "    \"\"\"\n",
    "    Predicts class labels and probabilities for new data using QDA parameters.\n",
    "    \"\"\"\n",
    "    priors, means, covariances = params\n",
    "    scores = []\n",
    "    # Ensure class order is 0, 1\n",
    "    for k in sorted(priors.keys()):\n",
    "        mu_k = means[k]\n",
    "        cov_k = covariances[k]\n",
    "        inv_cov_k = jnp.linalg.inv(cov_k)\n",
    "        log_det_cov_k = jnp.log(jnp.linalg.det(cov_k))\n",
    "        \n",
    "        diff = jax.vmap(lambda x: x - mu_k)(X)\n",
    "        quadratic_term = jnp.sum((diff @ inv_cov_k) * diff, axis=1)\n",
    "        \n",
    "        score_k = -0.5 * quadratic_term - 0.5 * log_det_cov_k + jnp.log(priors[k])\n",
    "        scores.append(score_k)\n",
    "        \n",
    "    scores_stacked = jnp.stack(scores, axis=0)\n",
    "    predictions = jnp.argmax(scores_stacked, axis=0)\n",
    "    probabilities = jax.nn.softmax(scores_stacked, axis=0)\n",
    "    \n",
    "    return predictions, probabilities[1, :]\n",
    "\n",
    "# --- 2. Evaluation and Plotting Function ---\n",
    "def evaluate_and_plot_classification(params, y_true, y_pred, y_prob, X_coords, dataset_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluates the QDA model and saves three plots, including all required metrics.\n",
    "    \"\"\"\n",
    "    print(f\"--- Evaluating on {dataset_name} Set ---\")\n",
    "    \n",
    "    # Classification metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    brier = brier_score_loss(y_true, y_prob)\n",
    "\n",
    "    # --- Added: Regression-style metrics ---\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    max_err = max_error(y_true, y_pred)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Confidence MSE (Brier Score): {brier:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "    # --- Plotting and Saving ---\n",
    "    # Plot 1: Prediction Correctness Distribution\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title1 = f'QDA_Prediction_Correctness_{dataset_name}_Set'\n",
    "    correct_predictions = (y_true == y_pred)\n",
    "    plt.scatter(X_coords[correct_predictions]['longitude'], X_coords[correct_predictions]['latitude'], \n",
    "                c='green', label='Correct', alpha=0.6, s=10)\n",
    "    plt.scatter(X_coords[~correct_predictions]['longitude'], X_coords[~correct_predictions]['latitude'], \n",
    "                c='red', label='Incorrect', alpha=0.6, s=10)\n",
    "    plt.title(f'QDA Prediction Correctness ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.legend(); plt.grid(True)\n",
    "    save_path1 = save_dir / f\"{title1}.png\"; plt.savefig(save_path1, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    print(f\"Plot saved to: {save_path1}\")\n",
    "\n",
    "    # Plot 2: Prediction Confidence Map\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title2 = f'QDA_Prediction_Confidence_{dataset_name}_Set'\n",
    "    scatter = plt.scatter(X_coords['longitude'], X_coords['latitude'], c=y_prob, \n",
    "                          cmap='coolwarm', vmin=0, vmax=1, s=10)\n",
    "    plt.colorbar(scatter, label='Probability of being Valid (Label=1)')\n",
    "    plt.title(f'QDA Prediction Confidence ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path2 = save_dir / f\"{title2}.png\"; plt.savefig(save_path2, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    print(f\"Plot saved to: {save_path2}\")\n",
    "\n",
    "    # Plot 3: Decision Boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title3 = f'QDA_Decision_Boundary_{dataset_name}_Set'\n",
    "    h = .02 # Mesh step size\n",
    "    x_min, x_max = X_coords['longitude'].min() - 0.1, X_coords['longitude'].max() + 0.1\n",
    "    y_min, y_max = X_coords['latitude'].min() - 0.1, X_coords['latitude'].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid_points = jnp.c_[xx.ravel(), yy.ravel()]\n",
    "    Z, _ = predict_qda(params, grid_points)\n",
    "    Z = np.array(Z).reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X_coords['longitude'], X_coords['latitude'], c=y_true, \n",
    "                cmap=plt.cm.coolwarm, s=10, edgecolors='k', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'QDA Decision Boundary ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path3 = save_dir / f\"{title3}.png\"; plt.savefig(save_path3, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    print(f\"Decision Boundary plot saved to: {save_path3}\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Main Execution Function ---\n",
    "def main():\n",
    "    script_dir = Path(__file__).parent.resolve()\n",
    "    data_file_path = script_dir / 'classification_data.csv'\n",
    "    data = pd.read_csv(data_file_path)\n",
    "\n",
    "    X = data[['longitude', 'latitude']]\n",
    "    y = data['label']\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/3), random_state=42, stratify=y_temp)\n",
    "    \n",
    "    # 1. Fit the QDA model\n",
    "    params = fit_qda(X_train.values, y_train.values)\n",
    "    \n",
    "    # 2. Make predictions\n",
    "    y_train_pred, y_train_prob = predict_qda(params, jnp.array(X_train.values))\n",
    "    y_val_pred, y_val_prob = predict_qda(params, jnp.array(X_val.values))\n",
    "    y_test_pred, y_test_prob = predict_qda(params, jnp.array(X_test.values))\n",
    "\n",
    "    # 3. Evaluate and plot\n",
    "    # Pass params to the evaluation function\n",
    "    evaluate_and_plot_classification(params, np.array(y_train), np.array(y_train_pred), np.array(y_train_prob), X_train, 'Training', script_dir)\n",
    "    evaluate_and_plot_classification(params, np.array(y_val), np.array(y_val_pred), np.array(y_val_prob), X_val, 'Validation', script_dir)\n",
    "    evaluate_and_plot_classification(params, np.array(y_test), np.array(y_test_pred), np.array(y_test_prob), X_test, 'Test', script_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31df43d-009d-4ff2-9050-3c77f0547c8f",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "## Comparison of 4 Methods on test set:\n",
    "| Loss Function | Accuracy | Precision | Recall | Brier Score | Epochs to Stop |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| QDA | 0.8271 | 0.8207 | 0.7714 | 0.1301 | - |\n",
    "| Euclidean Distance | 0.9764 | 0.9559 | 0.9914 | 0.0162 | 192 |\n",
    "| Cosine Similarity | **0.9776** | **0.9611** | 0.9886 | 0.0163 | 205 |\n",
    "| Cross-Entropy | 0.9764 | 0.9534 | **0.9943** | **0.0149** | **169** |\n",
    "## Figures of QDA\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Correctness_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Correctness_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Correctness_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Confidence_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Confidence_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Prediction_Confidence_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/QDA_Decision_Boundary_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Decision_Boundary_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures/QDA_Decision_Boundary_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deacf92d-8818-462d-a172-68cd2414c770",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed029af-c3b2-4b99-9878-9d52fa7879cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#\n",
    "#                       Combined Machine Learning Script\n",
    "#\n",
    "# This script combines three separate processes into a single workflow:\n",
    "# 1. Data Labeling:      Processes the raw XML data into CSV files.\n",
    "# 2. Classification:     Trains and evaluates a Neural Network for classification.\n",
    "# 3. Regression:         Trains and evaluates a CNN for regression (inpainting).\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 0. Combined Imports ---\n",
    "import csv\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "#                       PART 1: DATA LABELING\n",
    "#                       (from Data labeling.py)\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "def run_data_labeling():\n",
    "    \"\"\"\n",
    "    Reads the raw weather XML data, converts it into classification and\n",
    "    regression datasets, and exports them into two separate CSV files.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\\nPART 1: Running Data Labeling...\\n{'='*80}\")\n",
    "\n",
    "    # --- 1. Read and parse the source XML data ---\n",
    "    try:\n",
    "        script_dir = Path(__file__).parent.resolve()\n",
    "        xml_file = script_dir / 'O-A0038-003.xml'\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        namespace = {'cwa': 'urn:cwa:gov:tw:cwacommon:0.1'}\n",
    "        content_str = root.find('.//cwa:Content', namespace).text\n",
    "        lines = content_str.strip().split('\\n')\n",
    "        all_floats = []\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            floats_in_line = [float(val) for val in line.split(',') if val.strip()]\n",
    "            all_floats.extend(floats_in_line)\n",
    "        temp_grid = np.array(all_floats).reshape(120, 67)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{xml_file}'. Please ensure it is in the same directory.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading or parsing the XML file: {e}\")\n",
    "        return False\n",
    "\n",
    "    # --- 2. Create the latitude and longitude coordinate grid ---\n",
    "    start_lon, start_lat = 120.00, 21.88\n",
    "    lon_res, lat_res = 0.03, 0.03\n",
    "    lon_points, lat_points = 67, 120\n",
    "    longitudes = start_lon + np.arange(lon_points) * lon_res\n",
    "    latitudes = start_lat + np.arange(lat_points) * lat_res\n",
    "\n",
    "    # --- 3. Generate the classification and regression datasets ---\n",
    "    classification_data = []\n",
    "    regression_data = []\n",
    "    for i in range(lat_points):\n",
    "        for j in range(lon_points):\n",
    "            lon = longitudes[j]\n",
    "            lat = latitudes[i]\n",
    "            temp_value = temp_grid[i, j]\n",
    "            label = 1 if temp_value != -999.0 else 0\n",
    "            classification_data.append({'longitude': lon, 'latitude': lat, 'label': label})\n",
    "            if temp_value != -999.0:\n",
    "                regression_data.append({'longitude': lon, 'latitude': lat, 'value': temp_value})\n",
    "    print(\"Data conversion complete.\")\n",
    "    print(f\"Total entries in classification dataset: {len(classification_data)}.\")\n",
    "    print(f\"Total entries in regression dataset: {len(regression_data)}.\")\n",
    "\n",
    "    # --- 4. Write the data into two separate CSV files ---\n",
    "    classification_csv_file = script_dir / \"classification_data.csv\"\n",
    "    try:\n",
    "        with open(classification_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            fieldnames = ['longitude', 'latitude', 'label']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(classification_data)\n",
    "        print(f\"\\nSuccessfully wrote classification data to: '{classification_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the classification CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "    regression_csv_file = script_dir / \"regression_data.csv\"\n",
    "    try:\n",
    "        with open(regression_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            fieldnames = ['longitude', 'latitude', 'value']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(regression_data)\n",
    "        print(f\"Successfully wrote regression data to: '{regression_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the regression CSV: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "#                       PART 2: CLASSIFICATION\n",
    "#                     (from train_classifier(nn).py)\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "def evaluate_and_plot_nn_cls(model, scaler, X_scaled, y_ohe, dataset_name, save_dir):\n",
    "    print(f\"--- Evaluating on {dataset_name} Set ---\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_prob = model.predict(X_scaled)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true = np.argmax(y_ohe, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    brier = brier_score_loss(y_true, y_prob[:, 1])\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Confidence MSE (Brier Score): {brier:.4f}\\n\")\n",
    "\n",
    "    # For plotting\n",
    "    X_unscaled = scaler.inverse_transform(X_scaled)\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title1 = f'NN_Cls_Prediction_Correctness_{dataset_name}_Set'\n",
    "    correct_predictions = (y_true == y_pred)\n",
    "    plt.scatter(X_unscaled[correct_predictions, 0], X_unscaled[correct_predictions, 1], \n",
    "                c='green', label='Correct', alpha=0.6, s=10)\n",
    "    plt.scatter(X_unscaled[~correct_predictions, 0], X_unscaled[~correct_predictions, 1], \n",
    "                c='red', label='Incorrect', alpha=0.6, s=10)\n",
    "    plt.title(f'NN Classification Prediction Correctness ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.legend(); plt.grid(True)\n",
    "    save_path1 = save_dir / f\"{title1}.png\"\n",
    "    plt.savefig(save_path1, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    print(f\"Plot saved to: {save_path1}\")\n",
    "\n",
    "    # Confidence Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title2 = f'NN_Cls_Prediction_Confidence_{dataset_name}_Set'\n",
    "    scatter = plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=y_prob[:, 1], \n",
    "                          cmap='coolwarm', vmin=0, vmax=1, s=10)\n",
    "    plt.colorbar(scatter, label='Probability of being Valid (Label=1)')\n",
    "    plt.title(f'NN Classification Prediction Confidence ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path2 = save_dir / f\"{title2}.png\"\n",
    "    plt.savefig(save_path2, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    print(f\"Plot saved to: {save_path2}\\n\")\n",
    "\n",
    "def run_classification_nn():\n",
    "    print(f\"\\n{'='*80}\\nPART 2: Running Classification Neural Network...\\n{'='*80}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    seed_value = 42\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    \n",
    "    script_dir = Path(__file__).parent.resolve()\n",
    "    data_file_path = script_dir / 'classification_data.csv'\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{data_file_path}' not found. Aborting Part 2.\")\n",
    "        return\n",
    "\n",
    "    X = data[['longitude', 'latitude']]\n",
    "    y = data['label']\n",
    "\n",
    "    y_ohe = to_categorical(y, num_classes=2)\n",
    "\n",
    "    X_train, X_temp, y_train_ohe, y_temp_ohe = train_test_split(X, y_ohe, test_size=0.3, random_state=42, stratify=y_ohe)\n",
    "    X_val, X_test, y_val_ohe, y_test_ohe = train_test_split(X_temp, y_temp_ohe, test_size=(1/3), random_state=42, stratify=y_temp_ohe)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(2,)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining the Classification Neural Network model...\")\n",
    "    history = model.fit(X_train_scaled, y_train_ohe,\n",
    "                        epochs=1000,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_scaled, y_val_ohe),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2)\n",
    "    print(\"Model training complete.\\n\")\n",
    "\n",
    "    evaluate_and_plot_nn_cls(model, scaler, X_train_scaled, y_train_ohe, 'Training', script_dir)\n",
    "    evaluate_and_plot_nn_cls(model, scaler, X_val_scaled, y_val_ohe, 'Validation', script_dir)\n",
    "    evaluate_and_plot_nn_cls(model, scaler, X_test_scaled, y_test_ohe, 'Test', script_dir)\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "#                       PART 3: REGRESSION (INPAINTING)\n",
    "#                   (from train_regression(inpainting).py)\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "def create_inpainting_dataset_v2(data):\n",
    "    \"\"\"Creates the dataset for the CNN inpainting task (leak-free version).\"\"\"\n",
    "    print(\"Creating corrected inpainting dataset for regression...\")\n",
    "    start_lon, start_lat, lon_res, lat_res = 120.00, 21.88, 0.03, 0.03\n",
    "    lon_points, lat_points = 67, 120\n",
    "    \n",
    "    longitudes = start_lon + np.arange(lon_points) * lon_res\n",
    "    latitudes = start_lat + np.arange(lat_points) * lat_res\n",
    "    lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    coords_flat = np.stack([lon_grid.flatten(), lat_grid.flatten()], axis=1)\n",
    "    scaler.fit(coords_flat)\n",
    "    scaled_coords_flat = scaler.transform(coords_flat)\n",
    "    scaled_lon_grid = scaled_coords_flat[:, 0].reshape(lat_points, lon_points)\n",
    "    scaled_lat_grid = scaled_coords_flat[:, 1].reshape(lat_points, lon_points)\n",
    "\n",
    "    indices = data.index\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=(1/3), random_state=42)\n",
    "    train_df, val_df, test_df = data.loc[train_indices], data.loc[val_indices], data.loc[test_indices]\n",
    "    \n",
    "    y_true_grid_nan = np.full((lat_points, lon_points), np.nan)\n",
    "    for _, row in data.iterrows():\n",
    "        j = int(round((row['longitude'] - start_lon) / lon_res))\n",
    "        i = int(round((row['latitude'] - start_lat) / lat_res))\n",
    "        if 0 <= i < lat_points and 0 <= j < lon_points: y_true_grid_nan[i, j] = row['value']\n",
    "\n",
    "    train_points_mask_channel = np.zeros((lat_points, lon_points))\n",
    "    for _, row in train_df.iterrows():\n",
    "        j = int(round((row['longitude'] - start_lon) / lon_res))\n",
    "        i = int(round((row['latitude'] - start_lat) / lat_res))\n",
    "        if 0 <= i < lat_points and 0 <= j < lon_points: train_points_mask_channel[i, j] = 1.0\n",
    "            \n",
    "    X_grid = np.stack([train_points_mask_channel, scaled_lon_grid, scaled_lat_grid], axis=-1)\n",
    "\n",
    "    train_weight_mask = np.zeros((lat_points, lon_points))\n",
    "    val_weight_mask = np.zeros((lat_points, lon_points))\n",
    "    for index in train_indices:\n",
    "        row = data.loc[index]; j = int(round((row['longitude']-start_lon)/lon_res)); i = int(round((row['latitude']-start_lat)/lat_res))\n",
    "        if 0 <= i < lat_points and 0 <= j < lon_points: train_weight_mask[i, j] = 1.0\n",
    "    for index in val_indices:\n",
    "        row = data.loc[index]; j = int(round((row['longitude']-start_lon)/lon_res)); i = int(round((row['latitude']-start_lat)/lat_res))\n",
    "        if 0 <= i < lat_points and 0 <= j < lon_points: val_weight_mask[i, j] = 1.0\n",
    "\n",
    "    X_grid = X_grid[np.newaxis, ...]\n",
    "    y_grid_for_loss = np.nan_to_num(y_true_grid_nan)[np.newaxis, ..., np.newaxis]\n",
    "    train_weight_mask = train_weight_mask[np.newaxis, ...]\n",
    "    val_weight_mask = val_weight_mask[np.newaxis, ...]\n",
    "\n",
    "    print(\"Inpainting dataset created.\")\n",
    "    return X_grid, y_grid_for_loss, y_true_grid_nan, train_weight_mask, val_weight_mask, (train_df, val_df, test_df)\n",
    "\n",
    "def evaluate_and_plot_inpainting(y_true_grid_nan, y_pred_grid, df, dataset_name, save_dir):\n",
    "    \"\"\"Evaluates the inpainting model and plots the results.\"\"\"\n",
    "    print(f\"--- Evaluating on {dataset_name} Set ---\")\n",
    "    \n",
    "    true_values, pred_values = [], []\n",
    "    start_lon, start_lat, lon_res, lat_res = 120.00, 21.88, 0.03, 0.03\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        j = int(round((row['longitude'] - start_lon) / lon_res))\n",
    "        i = int(round((row['latitude'] - start_lat) / lat_res))\n",
    "        if 0 <= i < y_true_grid_nan.shape[0] and 0 <= j < y_true_grid_nan.shape[1]:\n",
    "            true_values.append(y_true_grid_nan[i, j])\n",
    "            pred_values.append(y_pred_grid[i, j])\n",
    "\n",
    "    mse = mean_squared_error(true_values, pred_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_values, pred_values)\n",
    "    max_err = max_error(true_values, pred_values)\n",
    "    \n",
    "    print(f\"Temperature MSE: {mse:.4f}\")\n",
    "    print(f\"Temperature RMSE: {rmse:.4f} (°C)\")\n",
    "    print(f\"Temperature MAE: {mae:.4f} (°C)\")\n",
    "    print(f\"Max Temperature Error: {max_err:.4f} (°C)\\n\")\n",
    "    \n",
    "    vmin, vmax = np.nanmin(y_true_grid_nan), np.nanmax(y_true_grid_nan)\n",
    "    extent = [start_lon, start_lon + 67*lon_res, start_lat, start_lat + 120*lat_res]\n",
    "\n",
    "    plt.figure(figsize=(8, 10)); title1 = f'Inpainting_Actual_Temperature_{dataset_name}'\n",
    "    plt.imshow(y_true_grid_nan, cmap='viridis', vmin=vmin, vmax=vmax, origin='lower', extent=extent, interpolation='nearest')\n",
    "    plt.colorbar(label='Actual Temperature (°C)'); plt.title(f'Actual Temperature ({dataset_name} points)')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path1 = save_dir / f\"{title1}.png\"; plt.savefig(save_path1, dpi=300); plt.close()\n",
    "    print(f\"Plot saved to: {save_path1}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 10)); title2 = f'Inpainting_Predicted_Temperature_Full'\n",
    "    plt.imshow(y_pred_grid, cmap='viridis', vmin=vmin, vmax=vmax, origin='lower', extent=extent, interpolation='nearest')\n",
    "    plt.colorbar(label='Predicted Temperature (°C)'); plt.title(f'Full Predicted Temperature Map')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path2 = save_dir / f\"{title2}.png\"; plt.savefig(save_path2, dpi=300); plt.close()\n",
    "    print(f\"Plot saved to: {save_path2}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 10)); title3 = f'Inpainting_Temperature_Error_Full'\n",
    "    errors = y_pred_grid - y_true_grid_nan\n",
    "    error_max_abs = np.nanmax(np.abs(errors))\n",
    "    plt.imshow(errors, cmap='coolwarm', vmin=-error_max_abs, vmax=error_max_abs, origin='lower', extent=extent, interpolation='nearest')\n",
    "    plt.colorbar(label='Prediction Error (°C)'); plt.title(f'Full Prediction Error Map')\n",
    "    plt.xlabel('Longitude'); plt.ylabel('Latitude'); plt.grid(True)\n",
    "    save_path3 = save_dir / f\"{title3}.png\"; plt.savefig(save_path3, dpi=300); plt.close()\n",
    "    print(f\"Plot saved to: {save_path3}\\n\")\n",
    "\n",
    "def run_regression_inpainting():\n",
    "    print(f\"\\n{'='*80}\\nPART 3: Running Regression CNN (Inpainting)...\\n{'='*80}\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    seed_value = 42\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    script_dir = Path(__file__).parent.resolve()\n",
    "    data_file_path = script_dir / 'regression_data.csv'\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{data_file_path}' not found. Aborting Part 3.\")\n",
    "        return\n",
    "    \n",
    "    X_grid, y_grid_for_loss, y_true_grid_nan, train_mask, val_mask, (train_df, val_df, test_df) = create_inpainting_dataset_v2(data)\n",
    "\n",
    "    input_shape = X_grid.shape[1:]\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (5, 5), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    outputs = Conv2D(1, (1, 1), padding='same', activation='linear')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    huber_loss = tf.keras.losses.Huber(delta=4.0)\n",
    "    model.compile(optimizer='adam', loss=huber_loss)\n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining the Corrected Inpainting CNN model...\")\n",
    "    history = model.fit(X_grid, y_grid_for_loss,\n",
    "                        epochs=2000,\n",
    "                        batch_size=1,\n",
    "                        sample_weight=train_mask,\n",
    "                        validation_data=(X_grid, y_grid_for_loss, val_mask),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2)\n",
    "    print(\"Model training complete.\\n\")\n",
    "\n",
    "    y_pred_grid_full = model.predict(X_grid)[0, :, :, 0]\n",
    "    \n",
    "    evaluate_and_plot_inpainting(y_true_grid_nan, y_pred_grid_full, train_df, 'Training', script_dir)\n",
    "    evaluate_and_plot_inpainting(y_true_grid_nan, y_pred_grid_full, val_df, 'Validation', script_dir)\n",
    "    evaluate_and_plot_inpainting(y_true_grid_nan, y_pred_grid_full, test_df, 'Test', script_dir)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "#                       MAIN EXECUTION BLOCK\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Step 1: Process raw data and create CSVs\n",
    "    success = run_data_labeling()\n",
    "    \n",
    "    # Step 2 & 3: Run models only if data processing was successful\n",
    "    if success:\n",
    "        run_classification_nn()\n",
    "        run_regression_inpainting()\n",
    "    else:\n",
    "        print(\"\\nData labeling failed. Halting execution of model training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493d94d-de20-4407-ab90-6eb0b2e4a431",
   "metadata": {},
   "source": [
    "<hr style=\"border-style: dashed; border-color: white; border-width: 0.8px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aacddd0-7e75-44bc-a9cc-663c7bc3808c",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "This section loads the necessary libraries for data processing, traditional machine learning, and the JAX/Flax framework. A global random seed is set to ensure reproducibility of the data split and model initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b1414-1d3c-4f22-8501-ace561d8a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# JAX/Flax Libraries for the core NN implementation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Scikit-learn Utilities for data handling and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, brier_score_loss\n",
    ")\n",
    "\n",
    "# Benchmark Classifier Imports (Will be used in Section 3.2)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, inset_axes\n",
    "\n",
    "# Finding files in Google Colab\n",
    "from google.colab import drive\n",
    "\n",
    "# Set global random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d852c22-cdcf-43c6-adaa-02f184629127",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition and Transformation\n",
    "\n",
    "The raw grid data from `O-A0038-003.xml` is parsed, and the coordinates are generated. This section focuses on creating the **Classification Dataset** `(Longitude, Latitude, label)` by identifying valid (`label=1`) versus invalid (`label=0`) temperature readings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13abd81-1610-41e8-835d-2fb73511efe0",
   "metadata": {},
   "source": [
    "### XML Parsing and Grid Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8719ebc-a626-4a84-896c-4022519e53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Characteristics ---\n",
    "drive.mount('/content/drive')\n",
    "xml_file = '/content/drive/MyDrive/Meteorological Data/O-A0038-003.xml'\n",
    "lat_points = 120\n",
    "lon_points = 67\n",
    "start_lon = 120.00\n",
    "start_lat = 21.88\n",
    "resolution = 0.03\n",
    "INVALID_VALUE = -999.0\n",
    "\n",
    "# Read and parse the XML data\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "namespace = {'cwa': 'urn:cwa:gov:tw:cwacommon:0.1'}\n",
    "content_str = root.find('.//cwa:Content', namespace).text\n",
    "\n",
    "# Parse the temperature grid into a NumPy array (120 rows x 67 columns)\n",
    "lines = content_str.strip().split('\\n')\n",
    "all_floats = []\n",
    "for line in lines:\n",
    "    if not line.strip():\n",
    "        continue\n",
    "    floats_in_line = [float(val) for val in line.split(',') if val.strip()]\n",
    "    all_floats.extend(floats_in_line)\n",
    "\n",
    "temp_grid = np.array(all_floats).reshape(lat_points, lon_points)\n",
    "\n",
    "print(f\"Temperature grid shape: {temp_grid.shape}\")\n",
    "valid_temps = temp_grid[temp_grid != INVALID_VALUE]\n",
    "print(f\"Valid temperature range: {valid_temps.min():.2f}°C to {valid_temps.max():.2f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1cf5db-cfbc-4bda-bc41-852193540e18",
   "metadata": {},
   "source": [
    "### Classification Dataset Creation\n",
    "\n",
    "The core classification task is to predict data validity. The dataset is structured as: `(Longitude,Latitude,label)`, where `label=0` for `Value=−999.0` and `label=1` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11375fc-281b-430a-b408-4834cd636891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate coordinates\n",
    "longitudes = start_lon + np.arange(lon_points) * resolution\n",
    "latitudes = start_lat + np.arange(lat_points) * resolution\n",
    "\n",
    "classification_data = []\n",
    "\n",
    "for i in range(lat_points):\n",
    "    for j in range(lon_points):\n",
    "        lon = longitudes[j]\n",
    "        lat = latitudes[i]\n",
    "        temp_value = temp_grid[i, j]\n",
    "\n",
    "        # Classification Dataset Rule: label=0 if invalid, label=1 if valid\n",
    "        label = 1 if temp_value != INVALID_VALUE else 0\n",
    "        classification_data.append({'longitude': lon, 'latitude': lat, 'label': label})\n",
    "\n",
    "data_cls = pd.DataFrame(classification_data)\n",
    "\n",
    "print(\"\\n--- Classification Data Summary ---\")\n",
    "print(f\"Total entries: {len(data_cls)}\")\n",
    "print(f\"Valid data points (label=1): {(data_cls['label'] == 1).sum()}\")\n",
    "print(f\"Invalid data points (label=0): {(data_cls['label'] == 0).sum()}\")\n",
    "print(data_cls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065861c2-b234-4e68-ae3b-614c8e0b8137",
   "metadata": {},
   "source": [
    "```\n",
    "--- Classification Data Summary ---\n",
    "Total entries: 8040\n",
    "Valid data points (label=1): 3495\n",
    "Invalid data points (label=0): 4545\n",
    "   longitude  latitude  label\n",
    "0     120.00     21.88      0\n",
    "1     120.03     21.88      0\n",
    "2     120.06     21.88      0\n",
    "3     120.09     21.88      0\n",
    "4     120.12     21.88      0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec27714-8e82-4919-9503-1a816940677f",
   "metadata": {},
   "source": [
    "### Data Splitting and Scaling\n",
    "\n",
    "The data is split into Train (70%), Validation (20%), and Test (10%) sets with **stratified sampling** to maintain the balance of valid/invalid data points across the sets. Features (Longitude, Latitude) are then standardized for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58f7e8-d2a9-4e29-bf21-4c1543a15673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X_cls = data_cls[['longitude', 'latitude']].values\n",
    "y_cls = data_cls['label'].values\n",
    "\n",
    "# Split data: 70% train, 20% validation, 10% test (stratified)\n",
    "X_train_cls, X_temp_cls, y_train_cls, y_temp_cls = train_test_split(\n",
    "    X_cls, y_cls, test_size=0.3, random_state=seed, stratify=y_cls\n",
    ")\n",
    "X_val_cls, X_test_cls, y_val_cls, y_test_cls = train_test_split(\n",
    "    X_temp_cls, y_temp_cls, test_size=(1/3), random_state=seed, stratify=y_temp_cls\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train_cls)}\")\n",
    "print(f\"Validation set size: {len(X_val_cls)}\")\n",
    "print(f\"Test set size: {len(X_test_cls)}\")\n",
    "\n",
    "# Standardize features (fit only on training data)\n",
    "scaler_cls = StandardScaler()\n",
    "X_train_scaled_cls = scaler_cls.fit_transform(X_train_cls)\n",
    "X_val_scaled_cls = scaler_cls.transform(X_val_cls)\n",
    "X_test_scaled_cls = scaler_cls.transform(X_test_cls)\n",
    "\n",
    "# Convert to JAX arrays for neural network training\n",
    "X_train_jax = jnp.array(X_train_scaled_cls)\n",
    "y_train_jax = jnp.array(y_train_cls)\n",
    "X_val_jax = jnp.array(X_val_scaled_cls)\n",
    "y_val_jax = jnp.array(y_val_cls)\n",
    "X_test_jax = jnp.array(X_test_scaled_cls)\n",
    "y_test_jax = jnp.array(y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afcc2b-93e1-41cf-955c-1a209bc482db",
   "metadata": {},
   "source": [
    "```\n",
    "Training set size: 5628\n",
    "Validation set size: 1608\n",
    "Test set size: 804\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd508d7-8246-430d-bd37-4f1fbb0f3d13",
   "metadata": {},
   "source": [
    "## 3. Classification Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b5ee81-6c39-4b5c-a50b-2e45a6ef1aec",
   "metadata": {},
   "source": [
    "### Benchmark Model Training\n",
    "\n",
    "Before implementing the JAX Neural Network, a variety of traditional and ensemble classifiers are trained to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce3285-8e27-4df7-a423-05319f998c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for training and evaluation\n",
    "all_results_cls = {}\n",
    "def fit_qda(X_train, y_train):\n",
    "    X_train, y_train = jnp.array(X_train), jnp.array(y_train)\n",
    "    classes = np.unique(np.array(y_train))\n",
    "    priors, means, covariances = {}, {}, {}\n",
    "    for k in classes:\n",
    "        X_k = X_train[y_train == k]\n",
    "        priors[k] = X_k.shape[0] / X_train.shape[0]\n",
    "        means[k] = jnp.mean(X_k, axis=0)\n",
    "        covariances[k] = jnp.cov(X_k, rowvar=False, bias=True)\n",
    "    return priors, means, covariances\n",
    "\n",
    "@jax.jit\n",
    "def predict_qda(params, X):\n",
    "    priors, means, covariances = params\n",
    "    scores = []\n",
    "    for k in sorted(priors.keys()):\n",
    "        mu_k, cov_k = means[k], covariances[k]\n",
    "        inv_cov_k = jnp.linalg.inv(cov_k)\n",
    "        log_det_cov_k = jnp.log(jnp.linalg.det(cov_k))\n",
    "        diff = jax.vmap(lambda x: x - mu_k)(X)\n",
    "        quadratic_term = jnp.sum((diff @ inv_cov_k) * diff, axis=1)\n",
    "        score_k = -0.5 * quadratic_term - 0.5 * log_det_cov_k + jnp.log(priors[k])\n",
    "        scores.append(score_k)\n",
    "    scores_stacked = jnp.stack(scores, axis=0)\n",
    "    predictions = jnp.argmax(scores_stacked, axis=0)\n",
    "    probabilities = jax.nn.softmax(scores_stacked, axis=0)\n",
    "    return predictions, probabilities\n",
    "\n",
    "# 2. Wrapping QDA as Scikit-learn \n",
    "\n",
    "class JAX_QDA_Wrapper:\n",
    "    def __init__(self):\n",
    "        self.params_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "        self.params_ = fit_qda(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        predictions, _ = predict_qda(self.params_, jnp.array(X))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        # Transpose\n",
    "        _, probabilities = predict_qda(self.params_, jnp.array(X))\n",
    "        return np.array(probabilities).T\n",
    "\n",
    "def evaluate_classifier(model, model_name, X_train, y_train, X_test, y_test, results_dict):\n",
    "    \"\"\"Trains and evaluates a classification model using scikit-learn metrics.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        # Get probabilities for the positive class (class 1)\n",
    "        y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "        test_brier = brier_score_loss(y_test, y_test_prob)\n",
    "    else:\n",
    "        # If the model doesn't support probabilities, set Brier score to NaN\n",
    "        test_brier = np.nan\n",
    "\n",
    "    # Metrics\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    test_prec = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_rec = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Training Time: {train_time:.4f}s\")\n",
    "    print(f\"Test Metrics: Acc={test_acc:.4f}, Prec={test_prec:.4f}, Rec={test_rec:.4f}, F1={test_f1:.4f}, Brier={test_brier:.4f}\")\n",
    "\n",
    "    results_dict[model_name] = {\n",
    "        'train_time': train_time,\n",
    "        'test_acc': test_acc,\n",
    "        'test_prec': test_prec,\n",
    "        'test_rec': test_rec,\n",
    "        'test_f1': test_f1,\n",
    "        'test_brier': test_brier\n",
    "    }\n",
    "    return model\n",
    "\n",
    "print(\"Starting Classification Benchmark Models Training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define models \n",
    "models = {\n",
    "    \"QDA (JAX)\": JAX_QDA_Wrapper(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=seed, max_depth=10),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=seed, max_depth=10, n_jobs=-1),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Support Vector Machine\": SVC(random_state=seed, probability=True), \n",
    "    \"XGBoost\": xgb.XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=seed, eval_metric='logloss'),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=seed, verbose=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, random_state=seed, verbose=False)\n",
    "}\n",
    "\n",
    "# Train and evaluate all benchmark models\n",
    "for name, model in models.items():\n",
    "    evaluate_classifier(\n",
    "        model, name,\n",
    "        X_train_scaled_cls, y_train_cls,\n",
    "        X_test_scaled_cls, y_test_cls,\n",
    "        all_results_cls\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Benchmark Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec82bd7-8884-4dd2-9151-a341d61dc9ab",
   "metadata": {},
   "source": [
    "```\n",
    "Starting Classification Benchmark Models Training...\n",
    "============================================================\n",
    "\n",
    "============================================================\n",
    "Training QDA (JAX)\n",
    "============================================================\n",
    "Training Time: 0.9934s\n",
    "Test Metrics: Acc=0.8271, Prec=0.8207, Rec=0.7714, F1=0.7953, Brier=0.1301\n",
    "\n",
    "============================================================\n",
    "Training K-Nearest Neighbors\n",
    "============================================================\n",
    "Training Time: 0.0031s\n",
    "Test Metrics: Acc=0.9776, Prec=0.9798, Rec=0.9686, F1=0.9741, Brier=0.0147\n",
    "\n",
    "============================================================\n",
    "Training Decision Tree\n",
    "============================================================\n",
    "Training Time: 0.0053s\n",
    "Test Metrics: Acc=0.9614, Prec=0.9518, Rec=0.9600, F1=0.9559, Brier=0.0339\n",
    "\n",
    "============================================================\n",
    "Training Random Forest\n",
    "============================================================\n",
    "Training Time: 0.3540s\n",
    "Test Metrics: Acc=0.9776, Prec=0.9770, Rec=0.9714, F1=0.9742, Brier=0.0195\n",
    "\n",
    "============================================================\n",
    "Training Naive Bayes\n",
    "============================================================\n",
    "Training Time: 0.0019s\n",
    "Test Metrics: Acc=0.8072, Prec=0.8854, Rec=0.6400, F1=0.7430, Brier=0.1701\n",
    "\n",
    "============================================================\n",
    "Training Support Vector Machine\n",
    "============================================================\n",
    "Training Time: 0.9255s\n",
    "Test Metrics: Acc=0.9577, Prec=0.9675, Rec=0.9343, F1=0.9506, Brier=0.0289\n",
    "\n",
    "============================================================\n",
    "Training XGBoost\n",
    "============================================================\n",
    "Training Time: 0.0589s\n",
    "Test Metrics: Acc=0.9764, Prec=0.9825, Rec=0.9629, F1=0.9726, Brier=0.0180\n",
    "\n",
    "============================================================\n",
    "Training LightGBM\n",
    "============================================================\n",
    "Training Time: 0.0636s\n",
    "Test Metrics: Acc=0.9776, Prec=0.9826, Rec=0.9657, F1=0.9741, Brier=0.0177\n",
    "\n",
    "============================================================\n",
    "Training CatBoost\n",
    "============================================================\n",
    "Training Time: 0.1294s\n",
    "Test Metrics: Acc=0.9677, Prec=0.9709, Rec=0.9543, F1=0.9625, Brier=0.0265\n",
    "\n",
    "============================================================\n",
    "Classification Benchmark Training Complete!\n",
    "============================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334ba92-a7d5-4ae0-8898-204d73fc06df",
   "metadata": {},
   "source": [
    "### Neural Network (Flax/JAX) Training\n",
    "\n",
    "The core neural network model uses the **JAX** framework for high-performance computing, with **Flax** providing a convenient API for defining the architecture and **Optax** for the optimization routine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc95a2-f888-443e-96a7-fd8747138b17",
   "metadata": {},
   "source": [
    "#### Model Architecture and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa6e1d1-8fe6-42e3-b185-e610c5abcd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple Feedforward Neural Network (FNN)\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"A simple feedforward neural network for classification.\"\"\"\n",
    "    hidden_dim: int = 32\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "        return x  # Returns logits\n",
    "\n",
    "# Initialize the model and calculate parameters\n",
    "model_cls = Classifier(hidden_dim=32, num_classes=2)\n",
    "key, init_key = random.split(key)\n",
    "params_cls = model_cls.init(init_key, jnp.ones((1, 2)))\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params_cls))\n",
    "\n",
    "print(f\"Neural Network Architecture: Input(2) -> Dense({model_cls.hidden_dim}) -> ReLU -> Dense({model_cls.hidden_dim}) -> ReLU -> Dense({model_cls.num_classes})\")\n",
    "print(f\"Total Parameters: {param_count}\")\n",
    "\n",
    "# Define the training state (parameters + optimizer)\n",
    "learning_rate = 0.001\n",
    "tx = optax.adam(learning_rate)\n",
    "state_cls = train_state.TrainState.create(\n",
    "    apply_fn=model_cls.apply,\n",
    "    params=params_cls['params'],\n",
    "    tx=tx\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c55c2-e2b0-4f97-a8f8-8edd746da4d3",
   "metadata": {},
   "source": [
    "#### JAX/Flax Core Functions\n",
    "\n",
    "JAX's functional programming paradigm allows defining pure, *JIT-compiled functions* for the training and evaluation loops, drastically improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491a9c3-8592-4c56-b129-ebcc830613f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels):\n",
    "    \"\"\"Compute categorical cross-entropy loss.\"\"\"\n",
    "    y_one_hot = jax.nn.one_hot(labels, num_classes=2)\n",
    "    log_probs = jax.nn.log_softmax(logits)\n",
    "    return -jnp.mean(jnp.sum(y_one_hot * log_probs, axis=-1))\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "    \"\"\"Compute loss and accuracy.\"\"\"\n",
    "    loss = cross_entropy_loss(logits, labels)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
    "    return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "    \"\"\"Single JAX training step, calculating gradients and updating state.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch_x)\n",
    "        return cross_entropy_loss(logits, batch_y)\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch_x, batch_y):\n",
    "    \"\"\"Single JAX evaluation step.\"\"\"\n",
    "    logits = model_cls.apply({'params': params}, batch_x)\n",
    "    return compute_metrics(logits, batch_y)\n",
    "\n",
    "@jax.jit\n",
    "def predict_fn(params, X):\n",
    "    \"\"\"JAX prediction step (returns logits).\"\"\"\n",
    "    return model_cls.apply({'params': params}, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d360de-9270-465d-ad25-41a9b082322b",
   "metadata": {},
   "source": [
    "#### Training Loop with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dd447-04e5-48b4-ba4c-e21c641803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "best_val_loss = float('inf')\n",
    "best_params = state_cls.params\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "print(\"\\nStarting Neural Network Training...\\n\")\n",
    "nn_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle and create mini-batches\n",
    "    key, subkey = random.split(key)\n",
    "    perm = random.permutation(subkey, len(X_train_jax))\n",
    "    X_train_shuffled = X_train_jax[perm]\n",
    "    y_train_shuffled = y_train_jax[perm]\n",
    "\n",
    "    for i in range(0, len(X_train_jax), batch_size):\n",
    "        x_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "        state_cls, _ = train_step(state_cls, x_batch, y_batch)\n",
    "\n",
    "    # Compute and store metrics\n",
    "    train_metrics = eval_step(state_cls.params, X_train_jax, y_train_jax)\n",
    "    val_metrics = eval_step(state_cls.params, X_val_jax, y_val_jax)\n",
    "\n",
    "    train_loss = float(train_metrics['loss'])\n",
    "    val_loss = float(val_metrics['loss'])\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(float(train_metrics['accuracy']))\n",
    "    val_accs.append(float(val_metrics['accuracy']))\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_params = state_cls.params\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={train_loss:.4f}, Train Acc={train_accs[-1]:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_accs[-1]:.4f}\")\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "nn_train_time = time.time() - nn_start_time\n",
    "params_nn_cls = best_params\n",
    "print(f\"\\nNeural Network Training Complete! Time: {nn_train_time:.4f}s, Epochs: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190c074-4848-4b76-a98b-2d0390db80ac",
   "metadata": {},
   "source": [
    "```\n",
    "Starting Neural Network Training...\n",
    "\n",
    "\n",
    "Early stopping at epoch 40\n",
    "\n",
    "Neural Network Training Complete! Time: 12.0536s, Epochs: 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9171ffa-4d37-47f4-be8e-d520ba108eb5",
   "metadata": {},
   "source": [
    "#### Neural Network Test Evaluation\n",
    "\n",
    "The final, best-performing parameters are used to evaluate performance on the unseen test set, and results are added to the comparison dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb457f5a-3e2c-4538-b62c-ae2e961c309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the JAX/Flax classifier on the test set\n",
    "logits_test = predict_fn(params_nn_cls, X_test_jax)\n",
    "y_test_pred_nn = np.array(jnp.argmax(logits_test, axis=-1))\n",
    "y_test_prob_nn = np.array(jax.nn.softmax(logits_test))\n",
    "\n",
    "nn_test_acc = accuracy_score(y_test_cls, y_test_pred_nn)\n",
    "nn_test_prec = precision_score(y_test_cls, y_test_pred_nn, zero_division=0)\n",
    "nn_test_rec = recall_score(y_test_cls, y_test_pred_nn, zero_division=0)\n",
    "nn_test_f1 = f1_score(y_test_cls, y_test_pred_nn, zero_division=0)\n",
    "nn_test_brier = brier_score_loss(y_test_cls, y_test_prob_nn[:, 1])\n",
    "\n",
    "# Add to results\n",
    "all_results_cls['Neural Network (Flax)'] = {\n",
    "    'train_time': nn_train_time,\n",
    "    'test_acc': nn_test_acc,\n",
    "    'test_prec': nn_test_prec,\n",
    "    'test_rec': nn_test_rec,\n",
    "    'test_f1': nn_test_f1,\n",
    "    'test_brier': nn_test_brier\n",
    "}\n",
    "\n",
    "# Detailed output\n",
    "cm = confusion_matrix(y_test_cls, y_test_pred_nn)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NEURAL NETWORK DETAILED EVALUATION (Classification)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"  Accuracy:  {nn_test_acc:.4f}\")\n",
    "print(f\"  Precision: {nn_test_prec:.4f}\")\n",
    "print(f\"  Recall:    {nn_test_rec:.4f}\")\n",
    "print(f\"  F1 Score:  {nn_test_f1:.4f}\")\n",
    "print(f\"  Brier Score: {nn_test_brier:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Neural Network: Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(val_accs, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Neural Network: Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_nn_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nTraining history saved to 'classification_nn_training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6d33a-ca90-4735-af5b-343ea6b6b798",
   "metadata": {},
   "source": [
    "```\n",
    "============================================================\n",
    "NEURAL NETWORK DETAILED EVALUATION (Classification)\n",
    "============================================================\n",
    "\n",
    "Confusion Matrix:\n",
    "[[447   7]\n",
    " [ 11 339]]\n",
    "\n",
    "Test Metrics:\n",
    "  Accuracy:  0.9776\n",
    "  Precision: 0.9798\n",
    "  Recall:    0.9686\n",
    "  F1 Score:  0.9741\n",
    "  Brier Score: 0.0189\n",
    "```\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Training and Validation Loss and Accuracy.png\" width=\"1200\"></td>\n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7f4f5-2483-439a-866c-293c191533b6",
   "metadata": {},
   "source": [
    "## 4. Classification Results Comparison and Visualization\n",
    "\n",
    "The performance of all models is compiled and visualized to compare key metrics: **Accuracy**, **F1 Score**, and **Training Time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8740c-cd40-4dbf-a596-9ff507ee87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_cls = pd.DataFrame(all_results_cls).T\n",
    "results_df_cls = results_df_cls.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ALL CLASSIFICATION MODELS COMPARISON (Test Set)\")\n",
    "print(\"=\"*90)\n",
    "print(results_df_cls.to_string())\n",
    "\n",
    "# Sort by test accuracy and F1 score for highlights\n",
    "results_sorted_acc = results_df_cls.sort_values('test_acc', ascending=False)\n",
    "results_sorted_f1 = results_df_cls.sort_values('test_f1', ascending=False)\n",
    "results_sorted_time = results_df_cls.sort_values('train_time')\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(f\"Best Model by Accuracy: {results_sorted_acc.index[0]} (Accuracy: {results_sorted_acc['test_acc'].iloc[0]:.4f})\")\n",
    "print(f\"Best Model by F1 Score: {results_sorted_f1.index[0]} (F1: {results_sorted_f1['test_f1'].iloc[0]:.4f})\")\n",
    "print(f\"Fastest Training: {results_sorted_time.index[0]} (Time: {results_sorted_time['train_time'].iloc[0]:.4f}s)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# 1. Test Accuracy Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = results_sorted_acc.index\n",
    "test_acc = results_sorted_acc['test_acc']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "bars1 = ax1.barh(models, test_acc, color=colors, height=0.6)\n",
    "ax1.set_xlabel('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Classification: Test Accuracy by Model', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlim([0.75, 1.0])\n",
    "for i, (bar, val) in enumerate(zip(bars1, test_acc)):\n",
    "    ax1.text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax1.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# 2. Test F1 Score Comparison\n",
    "ax2 = axes[0, 1]\n",
    "models_f1 = results_sorted_f1.index\n",
    "test_f1 = results_sorted_f1['test_f1']\n",
    "colors_f1 = plt.cm.plasma(np.linspace(0, 1, len(models_f1)))\n",
    "bars2 = ax2.barh(models_f1, test_f1, color=colors_f1, height=0.6)\n",
    "ax2.set_xlabel('Test F1 Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Classification: Test F1 Score by Model', fontsize=14, fontweight='bold', pad=15)\n",
    "ax2.set_xlim([0.7, 1.0])\n",
    "for i, (bar, val) in enumerate(zip(bars2, test_f1)):\n",
    "    ax2.text(val + 0.005, i, f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax2.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# 3. Training Time Comparison\n",
    "ax3 = axes[1, 0]\n",
    "models_time = results_sorted_time.index\n",
    "train_time_vals = results_sorted_time['train_time']\n",
    "colors_time = plt.cm.cool(np.linspace(0, 1, len(models_time)))\n",
    "bars3 = ax3.barh(models_time, train_time_vals, color=colors_time, height=0.6)\n",
    "ax3.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Classification: Training Time by Model', fontsize=14, fontweight='bold', pad=15)\n",
    "for i, (bar, val) in enumerate(zip(bars3, train_time_vals)):\n",
    "    ax3.text(val + 0.02, i, f'{val:.3f}s', va='center', fontsize=10, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "ax3.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# 4. Precision-Recall Trade-off\n",
    "ax4 = axes[1, 1]\n",
    "test_prec = results_df_cls['test_prec']\n",
    "test_rec = results_df_cls['test_rec']\n",
    "scatter = ax4.scatter(test_rec, test_prec, c=results_df_cls['test_f1'],\n",
    "                      s=300, cmap='viridis', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(results_df_cls.index):\n",
    "    offset_x = -0.002 if i % 2 == 0 else 0.002\n",
    "    offset_y = 0.002 if i % 3 == 0 else -0.002\n",
    "    ax4.annotate(model, (test_rec.iloc[i] + offset_x, test_prec.iloc[i] + offset_y),\n",
    "                 fontsize=9, ha='center', va='bottom', fontweight='bold',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7, edgecolor='gray'))\n",
    "ax4.set_xlabel('Test Recall', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Test Precision', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Classification: Precision-Recall Trade-off (color=F1 Score)', fontsize=14, fontweight='bold', pad=15)\n",
    "ax4.grid(True, alpha=0.3, linestyle='--')\n",
    "cbar = plt.colorbar(scatter, ax=ax4)\n",
    "cbar.set_label('F1 Score', fontsize=11, fontweight='bold')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "ax4.set_xlim([0.6, 1.01])\n",
    "ax4.set_ylim([0.8, 1.01])\n",
    "ax4.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "# Plotting on the subfigure\n",
    "ax_inset = inset_axes(ax4, width=\"45%\", height=\"45%\", loc='lower left',\n",
    "                      bbox_to_anchor=(0.2, 0.35, 1, 1),\n",
    "                      bbox_transform=ax4.transAxes)\n",
    "\n",
    "ax_inset.scatter(test_rec, test_prec, c=results_df_cls['test_f1'],\n",
    "                 s=300, cmap='viridis', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "offsets = {\n",
    "    'K-Nearest Neighbors': (0.00, -0.001),  \n",
    "    'Decision Tree':       (0.0, 0.004),  \n",
    "    'Random Forest':       (-0.0005, -0.0036), \n",
    "    'Support Vector Machine': (0.008, -0.004),\n",
    "    'XGBoost':             (-0.0025, 0.003),   \n",
    "    'LightGBM':            (0, 0.004),       \n",
    "    'CatBoost':            (0.00, 0.004),   \n",
    "    'Neural Network (Flax)': (0.0, 0.001)     \n",
    "}\n",
    "\n",
    "ax_inset.scatter(test_rec, test_prec, c=results_df_cls['test_f1'],\n",
    "                 s=300, cmap='viridis', alpha=0.8, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, model in enumerate(results_df_cls.index):\n",
    "    if model in offsets: \n",
    "        x_coord = test_rec.iloc[i]\n",
    "        y_coord = test_prec.iloc[i]\n",
    "        \n",
    "        offset_x, offset_y = offsets[model]\n",
    "        \n",
    "        ax_inset.annotate(model, (x_coord + offset_x, y_coord + offset_y),\n",
    "                          fontsize=8, ha='center', va='center') \n",
    "\n",
    "x1, x2, y1, y2 = 0.93, 0.975, 0.94, 0.99 \n",
    "ax_inset.set_xlim(x1, x2)\n",
    "ax_inset.set_ylim(y1, y2)\n",
    "ax_inset.grid(True, alpha=0.5, linestyle=':')\n",
    "ax_inset.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "mark_inset(ax4, ax_inset, loc1=2, loc2=1, fc=\"none\", ec=\"0.5\")\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.savefig('classification_all_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison visualization saved to 'classification_all_models_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3f724-8c39-448e-b097-7569a7335330",
   "metadata": {},
   "source": [
    "```\n",
    "==========================================================================================\n",
    "ALL CLASSIFICATION MODELS COMPARISON (Test Set)\n",
    "==========================================================================================\n",
    "                        train_time  test_acc  test_prec  test_rec  test_f1  test_brier\n",
    "QDA (JAX)                   0.9934    0.8271     0.8207    0.7714   0.7953      0.1301\n",
    "K-Nearest Neighbors         0.0031    0.9776     0.9798    0.9686   0.9741      0.0147\n",
    "Decision Tree               0.0053    0.9614     0.9518    0.9600   0.9559      0.0339\n",
    "Random Forest               0.3540    0.9776     0.9770    0.9714   0.9742      0.0195\n",
    "Naive Bayes                 0.0019    0.8072     0.8854    0.6400   0.7430      0.1701\n",
    "Support Vector Machine      0.9255    0.9577     0.9675    0.9343   0.9506      0.0289\n",
    "XGBoost                     0.0589    0.9764     0.9825    0.9629   0.9726      0.0180\n",
    "LightGBM                    0.0636    0.9776     0.9826    0.9657   0.9741      0.0177\n",
    "CatBoost                    0.1294    0.9677     0.9709    0.9543   0.9625      0.0265\n",
    "Neural Network (Flax)      12.0536    0.9776     0.9798    0.9686   0.9741      0.0189\n",
    "\n",
    "==========================================================================================\n",
    "Best Model by Accuracy: K-Nearest Neighbors (Accuracy: 0.9776)\n",
    "Best Model by F1 Score: Random Forest (F1: 0.9742)\n",
    "Fastest Training: Naive Bayes (Time: 0.0019s)\n",
    "==========================================================================================\n",
    "```\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Comparison.png\" width=\"1200\"></td>\n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a1854-1b19-41c1-ab7f-09a98c346296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
