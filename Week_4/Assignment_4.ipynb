{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba54506a-294b-4a44-8d6b-78e247259de9",
   "metadata": {},
   "source": [
    "# Programming Assignment\n",
    "> ## Project Description & Data Source\n",
    "> Please download the dataset: `O-A0038-003.xml`. This dataset contains gridded data for \"Hourly Temperature Distribution Analysis\" from the meteorological observation platform.\n",
    "> The data specifications are as follows:\n",
    "> * Each grid point represents a temperature observation in Celsius (°C).\n",
    "> * The invalid data value is -999.\n",
    "> * The resolution for both longitude and latitude is 0.03 degrees.\n",
    "> * The coordinates for the bottom-left grid point are: Longitude 120.00°, Latitude 21.88°.\n",
    "> * The data increases first along the longitude (67 values per row) and then increases along the latitude (for a total of 120 rows), forming a 67 x 120 grid of numerical values.\n",
    "> ## Part 1: Data Transformation\n",
    "> Transform the original data into two supervised learning datasets:\n",
    "> ### (a) Classification Dataset\n",
    "> * Format: (Longitude, Latitude, Label)\n",
    "> * Rules:\n",
    ">     * If the temperature observation is the invalid value -999, then label = 0.\n",
    ">     * If the temperature observation is a valid value, then label = 1.\n",
    "> ### (b) Regression Dataset\n",
    "> * Format: (Longitude, Latitude, Value)\n",
    "> * Rules:\n",
    ">     * Retain only the valid temperature observations (remove all -999 values).\n",
    ">     * value is the corresponding temperature in Celsius.\n",
    "> ## Part 2: Model Training\n",
    "> Using the two datasets prepared in Part 1, train a simple machine learning model for each task:\n",
    "> ### Classification Model:\n",
    "> * Use (Longitude, Latitude) to predict whether the grid data represents a valid value (0 or 1).\n",
    "> ### Regression Model:\n",
    "> * Use (Longitude, Latitude) to predict the corresponding temperature observation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95e53c-b3a1-4a21-8514-621c2f0dbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def convert_and_export_weather_data():\n",
    "    \"\"\"\n",
    "    Reads weather data from an XML file, converts it into classification and \n",
    "    regression datasets, and exports them into two separate CSV files.\n",
    "    \"\"\"\n",
    "    # --- 1. Read and parse the source XML data ---\n",
    "    try:\n",
    "        script_dir = Path(__file__).parent.resolve()\n",
    "        xml_file = script_dir / 'O-A0038-003.xml'\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        namespace = {'cwa': 'urn:cwa:gov:tw:cwacommon:0.1'}\n",
    "        content_str = root.find('.//cwa:Content', namespace).text\n",
    "        lines = content_str.strip().split('\\n')\n",
    "        all_floats = []\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            floats_in_line = [float(val) for val in line.split(',') if val.strip()]\n",
    "            all_floats.extend(floats_in_line)\n",
    "        temp_grid = np.array(all_floats).reshape(120, 67)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{xml_file}'. Please ensure it is in the same directory.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading or parsing the XML file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create the latitude and longitude coordinate grid ---\n",
    "    start_lon = 120.00\n",
    "    start_lat = 21.88\n",
    "    lon_resolution = 0.03\n",
    "    lat_resolution = 0.03\n",
    "    lon_points = 67\n",
    "    lat_points = 120\n",
    "    longitudes = start_lon + np.arange(lon_points) * lon_resolution\n",
    "    latitudes = start_lat + np.arange(lat_points) * lat_resolution\n",
    "\n",
    "    # --- 3. Generate the classification and regression datasets ---\n",
    "    classification_data = []\n",
    "    regression_data = []\n",
    "    for i in range(lat_points):\n",
    "        for j in range(lon_points):\n",
    "            lon = longitudes[j]\n",
    "            lat = latitudes[i]\n",
    "            temp_value = temp_grid[i, j]\n",
    "            label = 1 if temp_value != -999.0 else 0\n",
    "            classification_data.append({'longitude': lon, 'latitude': lat, 'label': label})\n",
    "            if temp_value != -999.0:\n",
    "                regression_data.append({'longitude': lon, 'latitude': lat, 'value': temp_value})\n",
    "    \n",
    "    print(\"Data conversion complete.\")\n",
    "    print(f\"Total entries in classification dataset: {len(classification_data)}.\")\n",
    "    print(f\"Total entries in regression dataset: {len(regression_data)}.\")\n",
    "\n",
    "    # --- 4. Write the data into two separate CSV files ---\n",
    "\n",
    "    # (a) Write the classification dataset\n",
    "    classification_csv_file = script_dir / \"classification_data.csv\"\n",
    "    try:\n",
    "        with open(classification_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            # Define the fieldnames (CSV header)\n",
    "            fieldnames = ['longitude', 'latitude', 'label']\n",
    "            # Create a DictWriter object to map dictionaries to CSV rows\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()  # Write the header row\n",
    "            writer.writerows(classification_data) # Write all data rows\n",
    "            \n",
    "        print(f\"\\nSuccessfully wrote classification data to: '{classification_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the classification CSV: {e}\")\n",
    "\n",
    "    # (b) Write the regression dataset\n",
    "    regression_csv_file = script_dir / \"regression_data.csv\"\n",
    "    try:\n",
    "        with open(regression_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            # Define the fieldnames\n",
    "            fieldnames = ['longitude', 'latitude', 'value']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader() # Write the header row\n",
    "            writer.writerows(regression_data) # Write all data rows\n",
    "            \n",
    "        print(f\"Successfully wrote regression data to: '{regression_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the regression CSV: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_and_export_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7abf352-0767-4e29-b35a-099b4ad6d20d",
   "metadata": {},
   "source": [
    "## Data Sampling:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 2. Data Splitting\n",
    "    X_train, X_temp, y_train_ohe, y_temp_ohe = train_test_split(X, y_ohe, test_size=0.3, random_state=42, stratify=y_ohe)\n",
    "    X_val, X_test, y_val_ohe, y_test_ohe = train_test_split(X_temp, y_temp_ohe, test_size=(1/3), random_state=42, stratify=y_temp_ohe)\n",
    "```\n",
    "* Training Data: 70 %\n",
    "* Validation Data: 20 %\n",
    "* Test Data: 10 %\n",
    "## Model: Neural Network\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 4. Model\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(2,)), # Input => Hidden Layer 1 (32 neurons)\n",
    "        Dense(32, activation='relu'),                   # Hidden Layer 1 (32 neurons) => Hidden Layer 2 (32 neurons)\n",
    "        Dense(2, activation='softmax')                  # Output Layer (Softmax)\n",
    "    ])\n",
    "```\n",
    "$$z^{[1]}=W^{[1]}X+b^{[1]}, \\text{ where }W^{[1]}: 32\\times 2, b^{[1]}: 32\\times 1$$\n",
    "$$a^{[1]}=ReLU(z^{[1]})$$\n",
    "$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}, \\text{ where }W^{[1]}: 32\\times 32, b^{[1]}: 32\\times 1$$\n",
    "$$a^{[2]}=ReLU(z^{[2]})$$\n",
    "$$z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}, \\text{ where }W^{[1]}: 2\\times 32, b^{[1]}: 2\\times 1$$\n",
    "$$\\hat{y}=Softmax(z^{[3]})$$\n",
    "```\n",
    "Model: \"sequential\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
    "│ dense (Dense)                        │ (None, 32)                  │              96 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_1 (Dense)                      │ (None, 32)                  │           1,056 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_2 (Dense)                      │ (None, 2)                   │              66 │\n",
    "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
    " Total params: 1,218 (4.76 KB)\n",
    " Trainable params: 1,218 (4.76 KB)\n",
    " Non-trainable params: 0 (0.00 B)\n",
    "```\n",
    "### Loss Function\n",
    "$$y\\text{: one-hot encoding vector of the true label}$$\n",
    "$$\\hat{y}\\text{: the probability vector prediction(softmax)}$$\n",
    "$$y_i, \\hat{y}_i\\text{: the }i\\text{-th component of the vector}$$\n",
    "\n",
    "#### 1. Euclidean Distance\n",
    "$$L_{MSE}(y,\\hat{y})=\\displaystyle\\sum_{i=1}^2 (y_i-\\hat{y_i})$$\n",
    "#### 2. Cosine Similarity\n",
    "$$Similarity(y,\\hat{y})=\\dfrac{y\\cdot\\hat{y}}{\\|y\\|\\|\\hat{y}\\|}$$\n",
    "$$L_{Similarity}(y,\\hat{y})=-Similarity(y,\\hat{y})$$\n",
    "#### 3. Cross Entropy\n",
    "$$L_{CE}(y,\\hat{y})=-\\displaystyle\\sum_{i=1}^2 y_i\\log{(\\hat{y_i})}$$\n",
    "### Optimizer: Adam\n",
    "```python\n",
    "# 5. Compile\n",
    "    model.compile(optimizer='adam',\n",
    "                  \n",
    "                  # Option 1: Euclidean Distance\n",
    "                  loss='mean_squared_error',\n",
    "                  \n",
    "                  # Option 2: Cosine Similarity\n",
    "                  #loss='cosine_similarity',\n",
    "                  \n",
    "                  # Option 3: Categorical Crossentropy\n",
    "                  #loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "```\n",
    "### Stop Criterion\n",
    "The Loss stop decreasing for 10 epochs or reach 100 Epochs.\n",
    "```python\n",
    "# 6. Train with Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b06a7-b886-4b2b-b269-a9074b14afa6",
   "metadata": {},
   "source": [
    "## Analysis of Classification\n",
    "### Metrics\n",
    "TP: True Positive\n",
    "FP: False Positive\n",
    "TN: True Negative\n",
    "FN: False Negative\n",
    "$$\\begin{array}{ccl}Accuracy&=&\\dfrac{TP+TN}{TP+FP+TN+FN}\\\\[10pt]\n",
    "Precision&=&\\dfrac{TP}{TP+FP}\\\\[10pt]\n",
    "Recall&=&\\dfrac{TP}{TP+FN}\\\\[10pt]\n",
    "Brier&=&\\dfrac{1}{N}\\displaystyle\\sum_{i=1}^N(f_i-o_i)^2\\end{array}$$\n",
    "$$\\begin{array}{ccl}N&:& \\text{ Total number of Samples }\\\\ f_i&:& \\text{ The predicted probability of Class 1 for the }i\\text{-th sample }\\\\ o_i&:& \\text{The observed outcome (0 or 1) for the }i\\text{-th sample}\\end{array}$$\n",
    "\n",
    "Comparison of 3 Methods（on test set):\n",
    "| Loss Function | Accuracy | Precision | Recall | Brier Score | Epochs to Stop |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Euclidean Distance | 0.9739 | 0.9532 | **0.9886** | 0.0164 | 100 |\n",
    "| Cosine Similarity | 0.9739 | **0.9557** | 0.9857 | 0.0178 | 100 |\n",
    "| Cross-Entropy | 0.9739 | 0.9532 | **0.9886** | **0.0157** | 100 |\n",
    "\n",
    "For the initial training capped at 100 epochs, the accuracy of all three models on the test set was identical. This suggests that for this simple classification task with a clear topographical boundary, all three loss functions were capable of guiding the model to learn a similar decision boundary. Regarding other metrics, the performance of Euclidean Distance and Cross-Entropy were nearly identical, with Cross-Entropy holding only a slight advantage in its Brier Score. However, it was observed that all three models completed the full 100 epochs, indicating they might not have fully converged. Therefore, the epoch limit was increased to 1000 for a second round of experiments.\n",
    "\n",
    "The results after extended training are as follows:\n",
    "\n",
    "Comparison of 3 Methods with more epochs（on test set):\n",
    "| Loss Function | Accuracy | Precision | Recall | Brier Score | Epochs to Stop |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Euclidean Distance | 0.9764 | 0.9559 | 0.9914 | 0.0162 | 192 |\n",
    "| Cosine Similarity | **0.9776** | **0.9611** | 0.9886 | 0.0163 | 205 |\n",
    "| Cross-Entropy | 0.9764 | 0.9534 | **0.9943** | **0.0149** | **169** |\n",
    "\n",
    "Once again, we can observe that the accuracy of the three models is nearly identical. With extended training, **Cosine Similarity** demonstrated its potential, achieving the best performance in both **Accuracy** and **Precision**. This implies that it can be a strong choice in scenarios where the cost of a false positive (FP) is high.\n",
    "\n",
    "On the other hand, **Cross-Entropy** was the **fastest** to converge, achieved the highest Recall, and produced the most **reliable** probability predictions (lowest Brier Score). This means that in situations where the cost of a false negative (FN) is high, this loss function would provide the greatest benefit.\n",
    "\n",
    "### Euclidean Distance\n",
    "#### First try\n",
    "```\n",
    "Epoch 100/100\n",
    "176/176 - 0s - 435us/step - accuracy: 0.9819 - loss: 0.0143 - val_accuracy: 0.9789 - val_loss: 0.0157\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3083 & 99 \\\\ 33 & 2413 \\end{pmatrix}$$| 0.9765 | 0.9606 | 0.9865 | 0.0168 |\n",
    "| Validation | $$\\begin{pmatrix} 884 & 25 \\\\ 9 & 690 \\end{pmatrix}$$ | 0.9789 | 0.9650 | 0.9871 | 0.0157 |\n",
    "| Test | $$\\begin{pmatrix} 437 & 17 \\\\ 4 & 346 \\end{pmatrix}$$ | 0.9739 | 0.9532 | 0.9886 | 0.0164 |\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Training_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Validation_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Test_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Second try\n",
    "\n",
    "```\n",
    "Epoch 192/1000\n",
    "\n",
    "176/176 - 0s - 449us/step - accuracy: 0.9833 - loss: 0.0130 - val_accuracy: 0.9820 - val_loss: 0.0139\n",
    "\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3088 & 94 \\\\ 30 & 2416 \\end{pmatrix}$$| 0.9780 | 0.9625 | 0.9877 | 0.0154 |\n",
    "| Validation | $$\\begin{pmatrix} 888 & 21 \\\\ 8 & 691 \\end{pmatrix}$$ | 0.9820 | 0.9705 | 0.9886 | 0.0139 |\n",
    "| Test | $$\\begin{pmatrix} 438 & 16 \\\\ 3 & 347 \\end{pmatrix}$$ | 0.9764 | 0.9559 | 0.9914 | 0.0162 |\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Training_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Training_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Correctness_Test_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_MSE_Prediction_Confidence_Test_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Cosine Similarity\n",
    "#### First try\n",
    "```\n",
    "Epoch 100/100\n",
    "176/176 - 0s - 440us/step - accuracy: 0.9796 - loss: -9.8294e-01 - val_accuracy: 0.9776 - val_loss: -9.8135e-01\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3087 & 95 \\\\ 35 & 2411 \\end{pmatrix}$$| 0.9769 | 0.9621 | 0.9857 | 0.0177 |\n",
    "| Validation | $$\\begin{pmatrix} 884 & 25 \\\\ 11 & 688 \\end{pmatrix}$$ | 0.9776 | 0.9649 | 0.9843 | 0.0171 |\n",
    "| Test | $$\\begin{pmatrix} 438 & 16 \\\\ 5 & 345 \\end{pmatrix}$$ | 0.9739 | 0.9557 | 0.9857 | 0.0178 |\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Training_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Validation_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Test_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Second try\n",
    "```\n",
    "Epoch 205/1000\n",
    "\n",
    "176/176 - 0s - 448us/step - accuracy: 0.9819 - loss: -9.8478e-01 - val_accuracy: 0.9801 - val_loss: -9.8380e-01\n",
    "\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3088 & 94 \\\\ 32 & 2414 \\end{pmatrix}$$| 0.9776 | 0.9625 | 0.9869 | 0.0157 |\n",
    "| Validation | $$\\begin{pmatrix} 889 & 20 \\\\ 10 & 689 \\end{pmatrix}$$ | 0.9813 | 0.9718 | 0.9857 | 0.0146 |\n",
    "| Test | $$\\begin{pmatrix} 440 & 14 \\\\ 4 & 346 \\end{pmatrix}$$ | 0.9776 | 0.9611 | 0.9886 | 0.0163 |\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Training_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Training_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Correctness_Test_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CS_Prediction_Confidence_Test_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Cross Entropy\n",
    "#### First Try\n",
    "```\n",
    "Epoch 100/100\n",
    "176/176 - 0s - 438us/step - accuracy: 0.9799 - loss: 0.0520 - val_accuracy: 0.9813 - val_loss: 0.0498\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3091 & 91 \\\\ 24 & 2422 \\end{pmatrix}$$| 0.9796 | 0.9638 | 0.9902 | 0.0160 |\n",
    "| Validation | $$\\begin{pmatrix} 885 & 24 \\\\ 6 & 693 \\end{pmatrix}$$ | 0.9813 | 0.9665 | 0.9914 | 0.0142 |\n",
    "| Test | $$\\begin{pmatrix} 437 & 17 \\\\ 4 & 346 \\end{pmatrix}$$ | 0.9739 | 0.9532 | 0.9886 | 0.0157 |\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Training_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Validation_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Test_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### Second Try\n",
    "```\n",
    "Epoch 169/1000\n",
    "\n",
    "176/176 - 0s - 434us/step - accuracy: 0.9817 - loss: 0.0472 - val_accuracy: 0.9820 - val_loss: 0.0472\n",
    "\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | Confusion Matrix | Accuracy | Precision | Recall | Brier Score |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | $$\\begin{pmatrix} 3097 & 85 \\\\ 22 & 2424 \\end{pmatrix}$$| 0.9810 | 0.9661 | 0.9910 | 0.0147 |\n",
    "| Validation | $$\\begin{pmatrix} 887 & 22 \\\\ 6 & 693 \\end{pmatrix}$$ | 0.9826 | 0.9692 | 0.9914 | 0.0133 |\n",
    "| Test | $$\\begin{pmatrix} 437 & 17 \\\\ 2 & 348 \\end{pmatrix}$$ | 0.9764 | 0.9534 | 0.9943 | 0.0149 |\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Training_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Training_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Correctness_Test_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Classification/NN_CE_Prediction_Confidence_Test_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14cf60-b7f0-4ab8-94ca-a828ea879a6e",
   "metadata": {},
   "source": [
    "## Analysis of Regression\n",
    "```\n",
    "Model: \"sequential\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
    "│ dense (Dense)                        │ (None, 64)                  │             192 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_1 (Dense)                      │ (None, 64)                  │           4,160 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_2 (Dense)                      │ (None, 1)                   │              65 │\n",
    "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
    " Total params: 4,417 (17.25 KB)\n",
    " Trainable params: 4,417 (17.25 KB)\n",
    " Non-trainable params: 0 (0.00 B)\n",
    "Epoch 100/100\n",
    "77/77 - 0s - 619us/step - loss: 9.6671 - mae: 2.2426 - mse: 9.6671 - val_loss: 11.1105 - val_mae: 2.4173 - val_mse: 11.1105\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | MSE | RMSE(°C) | MAE(°C) | Max Error(°C) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | 9.4897 | 3.0805 | 2.1981 | 14.6367 |\n",
    "| Validation | 11.1105 | 3.3332 | 2.4173 | 13.2554 |\n",
    "| Test | 9.1782 | 3.0296 | 2.2110 | 10.5454 |\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Training_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Training_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Validation_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Validation_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Test_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Test_Set.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Test_Set.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "From the results, we can observe that the model has successfully learned the general temperature distribution on the training set; in other words, the model's complexity is sufficient for the data.\n",
    "\n",
    "However, the error distribution plots clearly indicate that the model lacks a spatial concept. It is unable to comprehend the drastic topographical changes of the mountain ranges, only learning that temperatures in certain longitude/latitude zones might be lower, while ignoring the complex and sharp variations within those mountainous regions. This is particularly evident in the Max Error metric. In the initial 100-epoch run, the error on the test set reached 10.5°C. If this model were to be used in a practical application, it would likely lead to many complaints.\n",
    "\n",
    "Similarly, the model reached the 100-epoch limit, so the training ceiling was raised to allow for full convergence. The results are as follows:\n",
    "\n",
    "```\n",
    "Epoch 695/1000\n",
    "77/77 - 0s - 637us/step - loss: 7.0278 - mae: 1.8375 - mse: 7.0278 - val_loss: 8.2070 - val_mae: 1.9408 - val_mse: 8.2070\n",
    "Model training complete.\n",
    "```\n",
    "| Data Set | MSE | RMSE(°C) | MAE(°C) | Max Error(°C) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| Trainging | 6.8407 | 2.6086 | 1.7507 | 13.5421 |\n",
    "| Validation | 8.0210 | 2.8637 | 1.9421 | 10.9425 |\n",
    "| Test | 7.2721 | 2.6967 | 1.8694 | 8.3852 |\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Training_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Training_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Training_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Validation_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures of Regression/NN_Actual_Temperature_Test_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Predicted_Temperature_Test_Set_1000.png\" width=\"400\"></td>\n",
    "    <td><img src=\"Figures of Regression/NN_Temperature_Error_Test_Set_1000.png\" width=\"400\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "With extended training, we can see a clear improvement in the metrics, and the overall predicted trend appears smoother. The error in the mountainous regions has also been mitigated, especially concerning the extreme error points. Nevertheless, the primary source of error remains concentrated in the Central Mountain Range.\n",
    "\n",
    "The maximum error is still significant. For an application like a weather forecast, where high precision is expected by the public, a maximum error of over 8°C on the test set is still unacceptable.\n",
    "\n",
    "From this, we can deduce a key insight: longitude and latitude alone can only represent general temperature trends. To predict such \"localized\" temperatures accurately, other parameters like **altitude** and **humidity** must be considered. This demonstrates that prior domain knowledge and thorough data collection play a crucial role in the success of a machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855cea1-a4f9-47d1-830c-9cf744f82edd",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163eef9-5d0b-423d-8ba8-0b6365665632",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffc8e8-8247-433a-9754-aec3e9fb0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def convert_and_export_weather_data():\n",
    "    \"\"\"\n",
    "    Reads weather data from an XML file, converts it into classification and \n",
    "    regression datasets, and exports them into two separate CSV files.\n",
    "    \"\"\"\n",
    "    # --- 1. Read and parse the source XML data ---\n",
    "    try:\n",
    "        script_dir = Path(__file__).parent.resolve()\n",
    "        xml_file = script_dir / 'O-A0038-003.xml'\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        namespace = {'cwa': 'urn:cwa:gov:tw:cwacommon:0.1'}\n",
    "        content_str = root.find('.//cwa:Content', namespace).text\n",
    "        lines = content_str.strip().split('\\n')\n",
    "        all_floats = []\n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            floats_in_line = [float(val) for val in line.split(',') if val.strip()]\n",
    "            all_floats.extend(floats_in_line)\n",
    "        temp_grid = np.array(all_floats).reshape(120, 67)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at '{xml_file}'. Please ensure it is in the same directory.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading or parsing the XML file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Create the latitude and longitude coordinate grid ---\n",
    "    start_lon = 120.00\n",
    "    start_lat = 21.88\n",
    "    lon_resolution = 0.03\n",
    "    lat_resolution = 0.03\n",
    "    lon_points = 67\n",
    "    lat_points = 120\n",
    "    longitudes = start_lon + np.arange(lon_points) * lon_resolution\n",
    "    latitudes = start_lat + np.arange(lat_points) * lat_resolution\n",
    "\n",
    "    # --- 3. Generate the classification and regression datasets ---\n",
    "    classification_data = []\n",
    "    regression_data = []\n",
    "    for i in range(lat_points):\n",
    "        for j in range(lon_points):\n",
    "            lon = longitudes[j]\n",
    "            lat = latitudes[i]\n",
    "            temp_value = temp_grid[i, j]\n",
    "            label = 1 if temp_value != -999.0 else 0\n",
    "            classification_data.append({'longitude': lon, 'latitude': lat, 'label': label})\n",
    "            if temp_value != -999.0:\n",
    "                regression_data.append({'longitude': lon, 'latitude': lat, 'value': temp_value})\n",
    "    \n",
    "    print(\"Data conversion complete.\")\n",
    "    print(f\"Total entries in classification dataset: {len(classification_data)}.\")\n",
    "    print(f\"Total entries in regression dataset: {len(regression_data)}.\")\n",
    "\n",
    "    # --- 4. Write the data into two separate CSV files ---\n",
    "\n",
    "    # (a) Write the classification dataset\n",
    "    classification_csv_file = script_dir / \"classification_data.csv\"\n",
    "    try:\n",
    "        with open(classification_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            # Define the fieldnames (CSV header)\n",
    "            fieldnames = ['longitude', 'latitude', 'label']\n",
    "            # Create a DictWriter object to map dictionaries to CSV rows\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()  # Write the header row\n",
    "            writer.writerows(classification_data) # Write all data rows\n",
    "            \n",
    "        print(f\"\\nSuccessfully wrote classification data to: '{classification_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the classification CSV: {e}\")\n",
    "\n",
    "    # (b) Write the regression dataset\n",
    "    regression_csv_file = script_dir / \"regression_data.csv\"\n",
    "    try:\n",
    "        with open(regression_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            # Define the fieldnames\n",
    "            fieldnames = ['longitude', 'latitude', 'value']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader() # Write the header row\n",
    "            writer.writerows(regression_data) # Write all data rows\n",
    "            \n",
    "        print(f\"Successfully wrote regression data to: '{regression_csv_file}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the regression CSV: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_and_export_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a353ebd-6c80-4802-a7fe-c4ca871f58b9",
   "metadata": {},
   "source": [
    "## Classification (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8860e-db52-4038-a4bf-4539f35e2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, brier_score_loss\n",
    "from pathlib import Path\n",
    "\n",
    "def evaluate_and_plot_nn(model, X_scaled, y_ohe, dataset_name, save_dir):\n",
    "    print(f\"--- Evaluating on {dataset_name} Set ---\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_prob = model.predict(X_scaled)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    y_true = np.argmax(y_ohe, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    brier = brier_score_loss(y_true, y_prob[:, 1])\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Confidence MSE (Brier Score): {brier:.4f}\\n\")\n",
    "\n",
    "    # For plotting\n",
    "    X_unscaled = scaler.inverse_transform(X_scaled)\n",
    "    \n",
    "    # Accuracy Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title1 = f'NN_Prediction_Correctness_{dataset_name}_Set'\n",
    "    correct_predictions = (y_true == y_pred)\n",
    "    plt.scatter(X_unscaled[correct_predictions, 0], X_unscaled[correct_predictions, 1], \n",
    "                c='green', label='Correct', alpha=0.6, s=10)\n",
    "    plt.scatter(X_unscaled[~correct_predictions, 0], X_unscaled[~correct_predictions, 1], \n",
    "                c='red', label='Incorrect', alpha=0.6, s=10)\n",
    "    plt.title(f'NN Prediction Correctness ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    save_path1 = save_dir / f\"{title1}.png\"\n",
    "    plt.savefig(save_path1, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path1}\")\n",
    "\n",
    "    # Confidence Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title2 = f'NN_Prediction_Confidence_{dataset_name}_Set'\n",
    "    scatter = plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=y_prob[:, 1], \n",
    "                          cmap='coolwarm', vmin=0, vmax=1, s=10)\n",
    "    plt.colorbar(scatter, label='Probability of being Valid (Label=1)')\n",
    "    plt.title(f'NN Prediction Confidence ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True)\n",
    "    save_path2 = save_dir / f\"{title2}.png\"\n",
    "    plt.savefig(save_path2, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path2}\\n\")\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "    script_dir = Path(__file__).parent.resolve()\n",
    "    data_file_path = script_dir / 'classification_data.csv'\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{data_file_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    X = data[['longitude', 'latitude']]\n",
    "    y = data['label']\n",
    "\n",
    "    # 1. One-hot Encode\n",
    "    y_ohe = to_categorical(y, num_classes=2)\n",
    "\n",
    "    # 2. Data Splitting\n",
    "    X_train, X_temp, y_train_ohe, y_temp_ohe = train_test_split(X, y_ohe, test_size=0.3, random_state=42, stratify=y_ohe)\n",
    "    X_val, X_test, y_val_ohe, y_test_ohe = train_test_split(X_temp, y_temp_ohe, test_size=(1/3), random_state=42, stratify=y_temp_ohe)\n",
    "\n",
    "    # 3. Data Scaling\n",
    "    global scaler \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 4. Model\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(2,)), # Input => Hidden Layer 1 \n",
    "        Dense(32, activation='relu'),                   # Hidden Layer 1 => Hidden Layer 2\n",
    "        Dense(2, activation='softmax')                  # Output Layer (Softmax)\n",
    "    ])\n",
    "\n",
    "    # 5. Compile\n",
    "    model.compile(optimizer='adam',\n",
    "                  \n",
    "                  # Option 1: Euclidean Distance\n",
    "                  #loss='mean_squared_error',\n",
    "                  \n",
    "                  # Option 2: Cosine Similarity\n",
    "                  #loss='cosine_similarity',\n",
    "                  \n",
    "                  # Option 3: Categorical Crossentropy\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # 6. Train with Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining the Neural Network model...\")\n",
    "    history = model.fit(X_train_scaled, y_train_ohe,\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_scaled, y_val_ohe),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2) \n",
    "    print(\"Model training complete.\\n\")\n",
    "\n",
    "    # 7. Evaluate and Plot\n",
    "    evaluate_and_plot_nn(model, X_train_scaled, y_train_ohe, 'Training', script_dir)\n",
    "    evaluate_and_plot_nn(model, X_val_scaled, y_val_ohe, 'Validation', script_dir)\n",
    "    evaluate_and_plot_nn(model, X_test_scaled, y_test_ohe, 'Test', script_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f4ed6-6a6f-4b63-8303-638a9d2bae2b",
   "metadata": {},
   "source": [
    "## Regression (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979dd995-3f8a-4ed4-b5ed-8ed5ee316f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, max_error\n",
    "from pathlib import Path\n",
    "\n",
    "def evaluate_and_plot_regression_nn(model, X_scaled, y, dataset_name, save_dir):\n",
    "    \"\"\"\n",
    "    Evaluates the regression neural network and saves the resulting plots.\n",
    "    \"\"\"\n",
    "    print(f\"--- Evaluating on {dataset_name} Set ---\")\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_scaled).flatten() # Flatten to make it a 1D array\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    max_err = max_error(y, y_pred)\n",
    "    \n",
    "    print(f\"Temperature MSE: {mse:.4f}\")\n",
    "    print(f\"Temperature RMSE: {rmse:.4f} (°C)\")\n",
    "    print(f\"Temperature MAE: {mae:.4f} (°C)\")\n",
    "    print(f\"Max Temperature Error: {max_err:.4f} (°C)\\n\")\n",
    "\n",
    "    # For plotting, unscale the features\n",
    "    X_unscaled = scaler.inverse_transform(X_scaled)\n",
    "    \n",
    "    # --- Plotting and Saving ---\n",
    "    vmin = min(y.min(), y_pred.min())\n",
    "    vmax = max(y.max(), y_pred.max())\n",
    "    \n",
    "    # Plot 1: Actual Temperature Distribution\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title1 = f'NN_Actual_Temperature_{dataset_name}_Set'\n",
    "    scatter1 = plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=y, cmap='viridis', \n",
    "                           vmin=vmin, vmax=vmax, s=15)\n",
    "    plt.colorbar(scatter1, label='Actual Temperature (°C)')\n",
    "    plt.title(f'NN Actual Temperature Distribution ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True)\n",
    "    save_path1 = save_dir / f\"{title1}.png\"\n",
    "    plt.savefig(save_path1, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path1}\")\n",
    "\n",
    "    # Plot 2: Predicted Temperature Distribution\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title2 = f'NN_Predicted_Temperature_{dataset_name}_Set'\n",
    "    scatter2 = plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=y_pred, cmap='viridis',\n",
    "                           vmin=vmin, vmax=vmax, s=15)\n",
    "    plt.colorbar(scatter2, label='Predicted Temperature (°C)')\n",
    "    plt.title(f'NN Predicted Temperature Distribution ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True)\n",
    "    save_path2 = save_dir / f\"{title2}.png\"\n",
    "    plt.savefig(save_path2, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path2}\")\n",
    "\n",
    "    # Plot 3: Temperature Prediction Error\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    title3 = f'NN_Temperature_Error_{dataset_name}_Set'\n",
    "    errors = y_pred - y\n",
    "    error_max_abs = np.abs(errors).max()\n",
    "    scatter3 = plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], c=errors, cmap='coolwarm',\n",
    "                           vmin=-error_max_abs, vmax=error_max_abs, s=15)\n",
    "    plt.colorbar(scatter3, label='Prediction Error (°C)')\n",
    "    plt.title(f'NN Temperature Prediction Error ({dataset_name} Set)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True)\n",
    "    save_path3 = save_dir / f\"{title3}.png\"\n",
    "    plt.savefig(save_path3, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to: {save_path3}\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    seed_value = 42\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    script_dir = Path(__file__).parent.resolve()\n",
    "    data_file_path = script_dir / 'regression_data.csv'\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(data_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{data_file_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    X = data[['longitude', 'latitude']]\n",
    "    y = data['value']\n",
    "\n",
    "    # 1. Data Splitting\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/3), random_state=42)\n",
    "\n",
    "    # 2. Data Scaling\n",
    "    global scaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 3. Model Architecture\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(2,)), # Input => Hidden Layer 1\n",
    "        Dense(64, activation='relu'),                   # Hidden Layer 1 => Hidden Layer 2\n",
    "        Dense(1)                                        # Output Layer (1 neuron, linear activation)\n",
    "    ])\n",
    "\n",
    "    # 4. Compile Model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='mean_squared_error', # Standard loss for regression\n",
    "                  metrics=['mae', 'mse']) # Track Mean Absolute Error and Mean Squared Error\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # 5. Train with Early Stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"\\nTraining the Neural Network model for regression...\")\n",
    "    history = model.fit(X_train_scaled, y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=[early_stopping],\n",
    "                        verbose=2)\n",
    "    print(\"Model training complete.\\n\")\n",
    "\n",
    "    # 6. Evaluate and Plot\n",
    "    evaluate_and_plot_regression_nn(model, X_train_scaled, y_train, 'Training', script_dir)\n",
    "    evaluate_and_plot_regression_nn(model, X_val_scaled, y_val, 'Validation', script_dir)\n",
    "    evaluate_and_plot_regression_nn(model, X_test_scaled, y_test, 'Test', script_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
