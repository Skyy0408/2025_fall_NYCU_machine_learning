{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ca52af-8f43-4bc6-86b5-5634d9206c51",
   "metadata": {},
   "source": [
    "# Written Assignment\n",
    "# Programming Assignment\n",
    "> 1. Use the same code from Assignment 2 to calculate the error in approximating the derivative of the given function.\n",
    "\n",
    "Since the last program is too long to execute, I try to improve the algorithm.\n",
    "The basic logic is completely the same, but with a efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa2f85-a222-4cb0-bce9-8ddfa7138378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import time # Import time for performance comparison\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 20000\n",
    "datanum = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Select the data uniformly in [-1,1]\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum, dtype=jnp.float32)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "\n",
    "# Initialize parameters for 16 neurons\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, 16)), 'b1': jax.random.normal(b1_key, (16,)),\n",
    "    'w2': jax.random.normal(w2_key, (16, 16)), 'b2': jax.random.normal(b2_key, (16,)),\n",
    "    'w3': jax.random.normal(w3_key, (16, 1)), 'b3': jax.random.normal(b3_key, (1,))\n",
    "}\n",
    "\n",
    "# Hypoyhesis Function: Sigmoid function, with 3 layers in the hidden part.\n",
    "# Also, use itself as an activation function.\n",
    "def deep_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.sigmoid(x @ params['w1'] + params['b1'])\n",
    "    hidden2 = jax.nn.sigmoid(hidden1 @ params['w2'] + params['b2'])\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "# Loss Fucntion: 1/N*\\|y_pred-y\\|^2\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "\n",
    "# --- Create a JIT-compiled function for the ENTIRE epoch ---\n",
    "@jax.jit\n",
    "def train_epoch(params, train_data, permutation):\n",
    "    x_train, y_train = train_data\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "\n",
    "    def body_fun(step, current_params):\n",
    "        start_idx = step * batch_size\n",
    "        batch_idx = jax.lax.dynamic_slice_in_dim(permutation, start_idx, batch_size)\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        grads = jax.grad(loss_fn)(current_params, x_batch, y_batch)\n",
    "        return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, current_params, grads)\n",
    "\n",
    "    params = jax.lax.fori_loop(0, steps_per_epoch, body_fun, params)\n",
    "    return params\n",
    "\n",
    "# --- Modified Training Loop ---\n",
    "loss_history = []\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "num_train = len(x_train)\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    params = train_epoch(params, (x_train, y_train), perm)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Evaluation and Plotting ---\n",
    "x_plot = jnp.linspace(-1, 1, 500, dtype=jnp.float32)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Max Error: {max_error:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.title('Deep Sigmoid Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Derivative calculation\n",
    "def model_for_grad(x, model_params):\n",
    "    return deep_model(model_params, x).squeeze()\n",
    "\n",
    "grad_f = jax.grad(f)  \n",
    "y_d_true = jax.vmap(grad_f)(x_plot) \n",
    "model_derivative_fn = jax.grad(model_for_grad, argnums=0)\n",
    "vectorized_model_derivative = jax.vmap(model_derivative_fn, in_axes=(0, None))\n",
    "y_d_pred = vectorized_model_derivative(x_plot, params)\n",
    "\n",
    "pred_derivative_on_train = vectorized_model_derivative(x_train, params)\n",
    "true_derivative_on_train = jax.vmap(grad_f)(x_train)\n",
    "final_derivative_mse = jnp.mean((pred_derivative_on_train - true_derivative_on_train)**2)\n",
    "max_derivative_error = jnp.max(jnp.abs(y_d_pred-y_d_true))\n",
    "\n",
    "print(f\"\\n--- Final Result (Derivatives) ---\")\n",
    "print(f\"Final MSE (Derivative): {final_derivative_mse:.6f}\")\n",
    "print(f\"Final Max Error (Derivative): {max_derivative_error:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x_plot, y_d_true, label='True Derivative of Runge Function')\n",
    "plt.plot(x_plot, y_d_pred, label='Deep Sigmoid NN Derivative', linestyle='--')\n",
    "plt.title(\"Comparison of Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff91f9-c4f4-416b-bf79-7741e0972d1f",
   "metadata": {},
   "source": [
    "> 2. In this assignment, you will use a neural network to approximate both the **Runge function** and its **derivative**. Your task is to train a neural network that approximates:\n",
    "> a. The function $f(x)$ itself.\n",
    "> b. The derivative $f'(x)$.\n",
    "> You should define a **loss function** consisting of two components:\n",
    "> 1). **Function loss**: the error between the predicted $f(x)$ and the true $f'(x)$.\n",
    "> 2). **Derivative loss**: the error between the predicted $f'(x)$ and the true $f'(x)$.\n",
    "> Write a short report (1â€“2 pages) explaining method, results, and discussion including\n",
    "> * Plot the true function and the neural network prediction together.\n",
    "> * Show the training/validation loss curves.\n",
    "> * Compute and report errors (MSE or max error)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
