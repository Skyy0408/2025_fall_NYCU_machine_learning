{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e789fa5e-ead5-409c-8c0c-b7847e74d844",
   "metadata": {},
   "source": [
    "# Written Assignment\n",
    "## Question 1: What is a shallow $\\tanh$ neural network?\n",
    "A shallow $\\tanh$ neural network can be seen as a weighted combination of multiple $\\tanh$ functions, like \n",
    "\n",
    "$$\\displaystyle\\sum_i c_i\\cdot\\tanh(w_ix+b_i)$$\n",
    "\n",
    "Now, we are going to show that we can use $\\tanh$ to approximate any continuous function we want.\n",
    "\n",
    "However, it is quite difficult to start with an arbitrary function.\n",
    "\n",
    "Fortunately, we have Weierstrass Approximation Theorem:\n",
    "> ### Weierstrass Approximation Theorem:\n",
    "> Suppose $f$ is a continuous real-valued function defined on the real interval $[a, b]$. For every $ε > 0$, there exists a polynomial $p$ such that for all $x$ in $[a, b]$, we have $|f(x) − p(x)| < \\varepsilon $, or equivalently, the supremum norm $\\|f − p\\| < \\varepsilon$.\n",
    "\n",
    "It is a useful theorem, which usually is usually proven in Advanced Calculus, states that any function can be approximated by a polynomial as close as we want.\n",
    "\n",
    "Thus, the problem is transformed into the problem of proving $\\tanh$ can approximate any polynomial.\n",
    "\n",
    "Let's start from monomials.\n",
    "\n",
    "## Question 2: How to approximate $x$?\n",
    "\n",
    "Recall that the Maclaurin Series of a function $f\\in\\mathcal{C}^\\infty$ is \n",
    "\n",
    "$$f(t)=f(0)+f'(0)t+\\dfrac{f''(0)}{2!}t^2+\\dfrac{f'''(0)}{3!}t^3+\\cdots =\\displaystyle\\sum_{i=0}^\\infty \\dfrac{f^{(i)}(0)}{i!}t^i\\text{\\qquad, where }f^{(0)}(0):=f(0).$$\n",
    "\n",
    "For $\\sigma=\\tanh$, an odd function, its Maclaurin Series only has odd-powered terms:\n",
    "\n",
    "$$\n",
    "\\sigma(t)=\\sigma'(0)t+\\dfrac{\\sigma^{(3)}(0)}{3!}t^3+\\dfrac{\\sigma^{(5)}(0)}{5!}t^5+\\cdots\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\text{Consider }&\\left\\{\n",
    "    \\begin{array}{ccc}\n",
    "        \\sigma\\left(\\dfrac{hx}{2}\\right)&=&\\sigma'(0)\\left(\\dfrac{hx}{2}\\right)+\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{hx}{2}\\right)^3+\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{hx}{2}\\right)^5\\cdots\\\\[10pt]\n",
    "        \\sigma\\left(-\\dfrac{hx}{2}\\right)&=&-\\sigma'(0)\\left(\\dfrac{hx}{2}\\right)-\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{hx}{2}\\right)^3-\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{hx}{2}\\right)^5\\cdots\n",
    "    \\end{array}\\right.\\\\[30pt] \n",
    "    \\Longrightarrow & \\sigma\\left(\\dfrac{hx}{2}\\right)-\\sigma\\left(-\\dfrac{hx}{2}\\right)=2\\left(\\sigma'(0)\\left(\\dfrac{hx}{2}\\right)+\\dfrac{\\sigma'''(0)}{3!}\\left(\\dfrac{hx}{2}\\right)^3+\\cdots\\right)=\\sigma'(0)\\left(hx\\right)+O(h^3)\\\\[10pt]\n",
    "    \\Longrightarrow &x=\\dfrac{\\sigma\\left(\\dfrac{hx}{2}\\right)-\\sigma\\left(-\\dfrac{hx}{2}\\right)}{\\sigma'(0)h}+O(h^2)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, we can approximate $x$ with $\\dfrac{\\sigma\\left(\\dfrac{hx}{2}\\right)-\\sigma\\left(-\\dfrac{hx}{2}\\right)}{\\sigma'(0)h}$ with an error term $O(h^2)$.\n",
    "\n",
    "Additionally, it is a linear combination with only $2$ $\\tanh$ functions, which is easy to compute for shallow neural network.\n",
    "\n",
    "Furthermore, $O(h^2)$ can be controlled by $h$, in other words, we can approximate as closely as we want with a sufficiently small $h$.\n",
    "\n",
    "## Question 3: How about $x^3$, $x^5$, etc.?\n",
    "### Order Central Finite Difference\n",
    "\n",
    "Observe that $f'(t)=\\displaystyle\\lim_{h\\to 0}\\dfrac{f\\left(t+h\\right)-f\\left(t\\right)}{h}=\\displaystyle\\lim_{h\\to 0}\\dfrac{f\\left(t+\\frac{h}{2}\\right)-f\\left(t-\\frac{h}{2}\\right)}{h}\\qquad\\cdots(*)$\n",
    "\n",
    "Also, for \n",
    "$$\\begin{array}{ccl}f'''(t)&=&\\displaystyle\\lim_{h\\to 0}\\dfrac{f''\\left(t+\\frac{h}{2}\\right)-f''\\left(t-\\frac{h}{2}\\right)}{h}\\\\[15pt] &=&\\displaystyle\\lim_{h\\to 0}\\dfrac{\\dfrac{f'\\left(t+h\\right)-f'\\left(t\\right)}{h}-\\dfrac{f'\\left(t\\right)-f'\\left(t-h\\right)}{h}}{h}\\\\[15pt]&=&\\displaystyle\\lim_{h\\to 0}\\dfrac{f'(t+h)-2f(t)+f'(t-h)}{h^2}\\\\[15pt]&=&\\displaystyle\\lim_{h\\to 0}\\dfrac{\\dfrac{f\\left(t+\\frac{3h}{2}\\right)-f\\left(t+\\frac{h}{2}\\right)}{h}-2\\cdot \\dfrac{f\\left(t+\\frac{h}{2}\\right)-f\\left(t-\\frac{h}{2}\\right)}{h}+\\dfrac{f\\left(t-\\frac{h}{2}\\right)-f\\left(t-\\frac{3h}{2}\\right)}{h}}{h^2}\\\\[15pt]&=&\\displaystyle\\lim_{h\\to 0}\\dfrac{f\\left(t+\\frac{3h}{2}\\right)-3f\\left(t+\\frac{h}{2}\\right)+3f\\left(t-\\frac{h}{2}\\right)-f\\left(t-\\frac{3h}{2}\\right)}{h^3}\\end{array}$$\n",
    "by extending $(*)$ to $f^{(n+1)}(t)=\\displaystyle\\lim_{h\\to 0}\\dfrac{f^{(n)}\\left(t+\\frac{h}{2}\\right)-f^{(n)}\\left(t-\\frac{h}{2}\\right)}{h}, \\forall n\\in\\mathbb{N}$.\n",
    "\n",
    "For $x^3$, \n",
    "\n",
    "$$\\begin{array}{rl}\\text{Consider }&\\left\\{\\begin{array}{ccccccccc}\\sigma\\left(\\dfrac{3hx}{2}\\right)&=&\\sigma'(0)\\left(\\dfrac{3hx}{2}\\right)&+&\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{3hx}{2}\\right)^3&+&\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{3hx}{2}\\right)^5&+&\\cdots\\\\[10pt]\\sigma\\left(\\dfrac{hx}{2}\\right)&=&\\sigma'(0)\\left(\\dfrac{hx}{2}\\right)&+&\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{hx}{2}\\right)^3&+&\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{hx}{2}\\right)^5&+&\\cdots\\\\[10pt] \\sigma\\left(-\\dfrac{hx}{2}\\right)&=&-\\sigma'(0)\\left(\\dfrac{hx}{2}\\right)&-&\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{hx}{2}\\right)^3&-&\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{hx}{2}\\right)^5&-&\\cdots\\\\[10pt]\\sigma\\left(-\\dfrac{3hx}{2}\\right)&=&-\\sigma'(0)\\left(\\dfrac{3hx}{2}\\right)&-&\\dfrac{\\sigma^{(3)}(0)}{3!}\\left(\\dfrac{3hx}{2}\\right)^3&-&\\dfrac{\\sigma^{(5)}(0)}{5!}\\left(\\dfrac{3hx}{2}\\right)^5&-&\\cdots\\end{array}\\right.\\\\[60pt] \\Longrightarrow & \\sigma\\left(\\dfrac{3hx}{2}\\right)-3\\sigma\\left(\\dfrac{hx}{2}\\right)+3\\sigma\\left(-\\dfrac{hx}{2}\\right)-\\sigma\\left(\\dfrac{3hx}{2}\\right)=-\\dfrac{48}{6}\\sigma^{(3)}(0)\\left(\\dfrac{hx}{2}\\right)^3+O(h^5)=-\\sigma^{(3)}(0)\\left(hx\\right)^3+O(h^5)\\\\[10pt]\\Longrightarrow &x^3=\\dfrac{-\\left(\\sigma\\left(\\dfrac{3hx}{2}\\right)-3\\sigma\\left(\\dfrac{hx}{2}\\right)+3\\sigma\\left(-\\dfrac{hx}{2}\\right)-\\sigma\\left(\\dfrac{3hx}{2}\\right)\\right)}{\\sigma^{(3)}(0)h^3}+O(h^2)\\end{array}$$\n",
    "\n",
    "Again, we can approximate $x^3$ by $4$ $\\tanh$ functions as closely as we want with a sufficiently small $h$.\n",
    "\n",
    "Denote $\\delta_h^p[f](t):=\\displaystyle\\sum_{i=0}^p(-1)^i\\begin{pmatrix}p\\\\i\\end{pmatrix}f\\left(t+\\left(\\dfrac{p}{2}-i\\right)h\\right)$, which is the general form of the  numerator.\n",
    "\n",
    "This leads to the general approximation with $p+1$ $\\tanh$ functions for odd powers p: \n",
    "\n",
    "$$x^p\\approx \\dfrac{\\delta_{hx}^p[\\sigma](0)}{\\sigma^{(p)}(0)h^p}$$\n",
    "\n",
    "## Question 4: Furthermore, how about $x^2, x^4, $ etc.?\n",
    "We can skip the $x^0$ term, since it can be matched by the network's bias term, $b$.\n",
    "\n",
    "Observe that $$(x+\\alpha)^3-(x-\\alpha)^3=(x^3+3\\alpha x^2+3\\alpha^2 x+\\alpha^3)-(x^3-3\\alpha x^2+3\\alpha^2 x-\\alpha^3)=6\\alpha x^2+2\\alpha^3.$$\n",
    "\n",
    "Then, solving for $x^2$ gives: $$x^2 = \\frac{(x+\\alpha)^3 - (x-\\alpha)^3 - 2\\alpha^3}{6\\alpha}$$By applying our previous approximation for cubic terms, this becomes:$$x^2 \\approx \\frac{ \\frac{\\delta_{h(x+\\alpha)}^3[\\sigma](0)}{\\sigma^{(3)}(0)h^3} - \\frac{\\delta_{h(x-\\alpha)}^3[\\sigma](0)}{\\sigma^{(3)}(0)h^3} - 2\\alpha^3 }{6\\alpha}$$\n",
    "\n",
    "For $x^4$, by $$\\begin{array}{rcl}(x+\\alpha)^5-(x-\\alpha)^5&=&\\displaystyle\\sum_{i=0}^5\\begin{pmatrix}5\\\\i\\end{pmatrix}x^{5-i}\\alpha^i-\\sum_{i=0}^5\\begin{pmatrix}5\\\\i\\end{pmatrix}x^{5-i}(-\\alpha)^i\\\\[20pt]&=&2\\displaystyle\\sum_{i\\text{:odd}, i\\leq 5}\\begin{pmatrix}5\\\\i\\end{pmatrix}x^{5-i}\\alpha^i\\\\[20pt]&=&2\\left(\\begin{pmatrix}5\\\\1\\end{pmatrix}x^4\\alpha+\\begin{pmatrix}5\\\\3\\end{pmatrix}x^2\\alpha^3+\\begin{pmatrix}5\\\\5\\end{pmatrix}\\alpha^5\\right)\\\\[20pt]&=&2\\left(5x^4\\alpha+\\begin{pmatrix}5\\\\3\\end{pmatrix}x^2\\alpha^3+\\begin{pmatrix}5\\\\5\\end{pmatrix}\\alpha^5\\right)\\end{array}$$\n",
    "Similarly, we can solve for $x^4$: $$x^4 = \\frac{1}{10\\alpha}\\left((x+\\alpha)^5-(x-\\alpha)^5-2\\binom{5}{3}x^2\\alpha^3-2\\binom{5}{5}\\alpha^5\\right)$$\n",
    "\n",
    "This process can be generalized into a recursive formula:\n",
    "\n",
    "$$\\begin{array}{ccl}x^{2n}&=&\\dfrac{1}{2\\alpha(2n+1)}\\left((x+\\alpha)^{2n+1}-(x-\\alpha)^{2n+1}-2\\displaystyle\\sum_{k=0}^{n-1}\\begin{pmatrix}2n+1\\\\2n+1-2k\\end{pmatrix}x^{2k}\\alpha^{2n+1-2k}\\right)\\\\[10pt]&=&\\dfrac{1}{2\\alpha(2n+1)}\\left((x+\\alpha)^{2n+1}-(x-\\alpha)^{2n+1}-2\\displaystyle\\sum_{k=0}^{n-1}\\begin{pmatrix}2n+1\\\\2k\\end{pmatrix}x^{2k}\\alpha^{2(n-k)+1}\\right)\\end{array}$$\n",
    "\n",
    "## Let's see Lemmas\n",
    "\n",
    "\n",
    "> ### Lemma 3.1 \n",
    "> Let $k\\in\\mathbb{N}_0$ and $s\\in 2\\mathbb{N}-1$. Then it holds that for all $\\varepsilon>0$, there exists a shallow $\\tanh$ neural network $\\psi_{s,\\varepsilon}:[-M,M]\\to\\mathbb{R}^{\\frac{s+1}{2}}$ of width $\\dfrac{s+1}{2}$ such that $$\\max_{p\\leq s,\\ p\\text{: odd}}\\left\\|f_p-(\\psi_{s,\\varepsilon})_{\\frac{p+1}{2}}\\right\\|_{W^{k,\\infty}}\\leq\\varepsilon.$$\n",
    "> Moreover, the weights of $\\psi_{s,\\varepsilon}$ scale as $O\\left(\\varepsilon^{-\\frac{s}{2}}(2(s+2)\\sqrt{2M})^{s(s+3)}\\right)$ for small $\\varepsilon$ and large $s$.\n",
    "\n",
    "The phrase \"For all $\\varepsilon > 0$\" means that no matter how small an error tolerance ($\\varepsilon$) we desire, we can always find a corresponding neural network ($\\psi_{s,\\varepsilon}$) that meets this accuracy requirement.\n",
    "\n",
    "The function $\\psi_{s,\\varepsilon}$ represents the neural network that approximates the monomial. Its core structure is built upon the central finite difference formula, which approximates $x^p$ using a specific combination of tanh functions: \n",
    "\n",
    "$$\\dfrac{\\delta_{hx}^p[\\sigma](0)}{\\sigma^{(p)}(0)h^p}$$\n",
    "\n",
    "The \"width\" of the network refers to the number of neurons($\\tanh$ functions) in its hidden layer.\n",
    "\n",
    "> ### Lemma 3.2\n",
    "> Let $k\\in\\mathbb{N}_0$ and $s\\in 2\\mathbb{N}-1$ and $M>0$. For every $\\varepsilon>0$, there exists a shallow $\\tanh$ neural network $\\psi_{s,\\varepsilon}:[-M,M]\\to\\mathbb{R}^s$ of width $\\dfrac{3(s+1)}{2}$ such that $$\\max_{p\\leq s}\\left\\|f_p-(\\psi_{s,\\varepsilon})_p\\right\\|_{W^{k,\\infty}}\\leq \\varepsilon.$$\n",
    "> Furthermore, the weghts scale as $O\\left(\\varepsilon^{-\\frac{s}{2}}(\\sqrt{M}(s+2))^{3\\frac{s(s+3)}{2}}\\right)$ for small $\\varepsilon$ and large $s$.\n",
    "\n",
    "Again, ε represents the desired error tolerance between the target monomial and our neural network's approximation.\n",
    "Moreover, we have \n",
    "\n",
    "$$\\psi_{s,\\varepsilon}=\\dfrac{1}{2\\alpha(2n+1)}\\left(\\frac{\\delta_{h(x+\\alpha)}^{2n+1}[\\sigma](0)}{\\sigma^{(2n+1)}(0)h^{2n+1}} - \\frac{\\delta_{h(x-\\alpha)}^{2n+1}[\\sigma](0)}{\\sigma^{(2n+1)}(0)h^{2n+1}}-2\\displaystyle\\sum_{k=0}^{n-1}\\begin{pmatrix}2n+1\\\\2k\\end{pmatrix}x^{2k}\\alpha^{2(n-k)+1}\\right)$$\n",
    "\n",
    "# Programming Assignment\n",
    "> 1. Use the same code from Assignment 2 to calculate the error in approximating the derivative of the given function.\n",
    "\n",
    "Since the previous program's execution time was too long, I decided to improve the algorithm's efficiency. The underlying logic remains the same, but the computation is more efficient. The revised program is shown below.\n",
    "\n",
    "```python \n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import time # Import time for performance comparison\n",
    "\n",
    "# The Runge Function\n",
    "def f(x):\n",
    "    return 1/(1+25*x**2)\n",
    "grad_f = jax.grad(f)\n",
    "vmap_grad_f = jax.vmap(grad_f)\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 40000 # Stop Criterion\n",
    "datanum = 10001\n",
    "batch_size = 32\n",
    "n = 16\n",
    "\n",
    "# Data\n",
    "x_train = jnp.linspace(-1.0, 1.0, datanum, dtype=jnp.float32)\n",
    "y_train = f(x_train)\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Validation Data\n",
    "key, validation_key = jax.random.split(key)\n",
    "x_validation = jax.random.uniform(validation_key, shape=(4000,), minval=-1.0, maxval=1.0)\n",
    "y_validation = f(x_validation)\n",
    "\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "# Initialize parameters\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, n)), 'b1': jax.random.normal(b1_key, (n,)),\n",
    "    'w2': jax.random.normal(w2_key, (n, n)), 'b2': jax.random.normal(b2_key, (n,)),\n",
    "    'w3': jax.random.normal(w3_key, (n, 1)), 'b3': jax.random.normal(b3_key, (1,))\n",
    "}\n",
    "\n",
    "def deep_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.tanh(x @ params['w1'] + params['b1'])       # tanh and sigmoid are equivalent\n",
    "    hidden2 = jax.nn.tanh(hidden1 @ params['w2'] + params['b2']) # the tanh function passes through the origin, which can sometimes lead to faster convergence.\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "# Derivative calculation\n",
    "def model_for_grad(x, model_params):\n",
    "    return deep_model(model_params, x).squeeze()\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    predictions = deep_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "\n",
    "# --- Create a JIT-compiled function for the ENTIRE epoch ---\n",
    "@jax.jit\n",
    "def train_epoch(params, train_data, permutation):\n",
    "    x_train, y_train = train_data\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "\n",
    "    def body_fun(step, current_params):\n",
    "        start_idx = step * batch_size\n",
    "        batch_idx = jax.lax.dynamic_slice_in_dim(permutation, start_idx, batch_size)\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        grads = jax.grad(loss_fn)(current_params, x_batch, y_batch)\n",
    "        return jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, current_params, grads)\n",
    "\n",
    "    params = jax.lax.fori_loop(0, steps_per_epoch, body_fun, params)\n",
    "    return params\n",
    "\n",
    "# --- Modified Training Loop ---\n",
    "loss_history = []\n",
    "key, shuffle_key = jax.random.split(key)\n",
    "num_train = len(x_train)\n",
    "print(\"\\n---Start Training---\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    shuffle_key, perm_key = jax.random.split(shuffle_key)\n",
    "    perm = jax.random.permutation(perm_key, num_train)\n",
    "    \n",
    "    params = train_epoch(params, (x_train, y_train), perm)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "x_plot = jnp.linspace(-1, 1, 500, dtype=jnp.float32)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_model(params, x_train).squeeze()\n",
    "final_mse = loss_fn(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "y_pred_validation= deep_model(params, x_validation).squeeze()\n",
    "validation_mse = loss_fn(params, x_validation, y_validation)\n",
    "validation_max_error = jnp.max(jnp.abs(y_pred_validation - y_validation))\n",
    "\n",
    "\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final Training MSE: {final_mse:.6f}\")\n",
    "print(f\"Final Training Max Error: {max_error:.6f}\")\n",
    "print(f\"Final Validation MSE: {validation_mse:.6f}\")\n",
    "print(f\"Final Training Max Error: {validation_max_error:.6f}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Sigmoid NN', linestyle='--')\n",
    "plt.scatter(x_validation, y_validation, label='Validation', color='orange', s=10, alpha=0.6)\n",
    "plt.title('Deep Sigmoid Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs, 1000), loss_history)\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_d_true = vmap_grad_f(x_plot) \n",
    "y_d_true_validation = vmap_grad_f(x_validation) \n",
    "model_derivative_fn = jax.grad(model_for_grad, argnums=0)\n",
    "vectorized_model_derivative = jax.vmap(model_derivative_fn, in_axes=(0, None))\n",
    "y_d_pred = vectorized_model_derivative(x_plot, params)\n",
    "y_d_pred_validation = vectorized_model_derivative(x_validation, params)\n",
    "\n",
    "pred_derivative_on_train = vectorized_model_derivative(x_train, params)\n",
    "pred_derivative_on_validation = vectorized_model_derivative(x_validation, params)\n",
    "true_derivative_on_train = vmap_grad_f(x_train)\n",
    "true_derivative_on_validation = vmap_grad_f(x_validation)\n",
    "final_derivative_mse = jnp.mean((pred_derivative_on_train - true_derivative_on_train)**2)\n",
    "final_derivative_mse_validation = jnp.mean((pred_derivative_on_validation - true_derivative_on_validation)**2)\n",
    "max_derivative_error = jnp.max(jnp.abs(y_d_pred-y_d_true))\n",
    "max_derivative_error_validation = jnp.max(jnp.abs(y_d_pred_validation-y_d_true_validation))\n",
    "\n",
    "\n",
    "print(f\"\\n--- Final Result (Derivatives) ---\")\n",
    "print(f\"Final Training MSE (Derivative): {final_derivative_mse:.6f}\")\n",
    "print(f\"Final Training Max Error (Derivative): {max_derivative_error:.6f}\")\n",
    "print(f\"Final Validation MSE (Derivative): {final_derivative_mse_validation:.6f}\")\n",
    "print(f\"Final Validation Max Error (Derivative): {max_derivative_error_validation:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x_plot, y_d_true, label='True Derivative of Runge Function')\n",
    "plt.plot(x_plot, y_d_pred, label='Deep Sigmoid NN Derivative', linestyle='--')\n",
    "plt.scatter(x_validation, y_validation, label='Validation', color='orange', s=10, alpha=0.6)\n",
    "plt.title(\"Comparison of Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "The result is shown below.\n",
    "```\n",
    "Training finished in 123.84 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Training MSE: 0.000000.                 # This is also the Final Loss since loss = MSE in this program.\n",
    "Final Training Max Error: 0.001592\n",
    "Final Validation MSE: 0.000000\n",
    "Final Training Max Error: 0.001575\n",
    "\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 0.000095\n",
    "Final Training Max Error (Derivative): 0.052462\n",
    "Final Validation MSE (Derivative): 0.000101\n",
    "Final Validation Max Error (Derivative): 0.052195\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/revised.png\" alt=\"Deep Sigmoid Network Approximation\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/revised_d.png\" alt=\"Comparison of Derivatives\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "We can see that the approximation of the derivative failed, even though the plot of the function itself appears to be a good fit.\n",
    "\n",
    "> 2. In this assignment, you will use a neural network to approximate both the **Runge function** and its **derivative**. Your task is to train a neural network that approximates:\n",
    "> a. The function $f(x)$ itself.\n",
    "> b. The derivative $f'(x)$.\n",
    "> You should define a **loss function** consisting of two components:\n",
    "> 1). **Function loss**: the error between the predicted $f(x)$ and the true $f'(x)$.\n",
    "> 2). **Derivative loss**: the error between the predicted $f'(x)$ and the true $f'(x)$.\n",
    "> Write a short report (1–2 pages) explaining method, results, and discussion including\n",
    "> * Plot the true function and the neural network prediction together.\n",
    "> * Show the training/validation loss curves.\n",
    "> * Compute and report errors (MSE or max error).\n",
    "\n",
    "## Consider the MSE of derivative.\n",
    "### Modify it directly\n",
    "We just need to add some lines in our last program.\n",
    "\n",
    "Initially, I adjusted the loss function to\n",
    "\n",
    "```python\n",
    "model_derivative_fn = jax.grad(model_for_grad, argnums=0)\n",
    "vmap_model_derivative = jax.vmap(model_derivative_fn, in_axes=(0, None))\n",
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    pred_y = deep_model(params, x).squeeze()\n",
    "    loss_f = jnp.mean((pred_y - y)**2)\n",
    "\n",
    "    true_dy = vmap_grad_f(x)\n",
    "    pred_dy = vmap_model_derivative(x, params)\n",
    "    loss_df = jnp.mean((pred_dy - true_dy)**2)\n",
    "\n",
    "    return (1-rho)*loss_f + rho * loss_df\n",
    "```\n",
    ", and I set \n",
    "```python\n",
    "rho = 0.6 # Weight for derivative loss\n",
    "```\n",
    " The result is shown below:\n",
    "```\n",
    "Training finished in 211.79 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Loss: 7.186441e-08                                          # Scientific Notation For a Smaller Number\n",
    "Final Training MSE: 1.880752e-09\n",
    "Final Training Max Error: 1.214743e-04\n",
    "Final Validation MSE: 1.872495e-09\n",
    "Final Training Max Error: 1.212358e-04\n",
    "\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 1.185105e-07\n",
    "Final Training Max Error (Derivative): 1.282930e-03\n",
    "Final Validation MSE (Derivative): 1.222936e-07\n",
    "Final Validation Max Error (Derivative): 1.272224e-03\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/0.6.png\" alt=\"Weight: 0.6, Runge\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/0.6_d.png\" alt=\"Weight: 0.6, Derivative\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Even when increasing the derivative loss weight to\n",
    "```python\n",
    "rho = 0.9 # Weight for derivative loss\n",
    "```\n",
    ", the result remained almost the same.\n",
    "```\n",
    "Training finished in 212.95 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Loss: 1.656618e-07\n",
    "Final Training MSE: 4.041164e-09\n",
    "Final Training Max Error: 1.850724e-04\n",
    "Final Validation MSE: 4.037293e-09\n",
    "Final Training Max Error: 1.849532e-04\n",
    "\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 1.836190e-07\n",
    "Final Training Max Error (Derivative): 1.404524e-03\n",
    "Final Validation MSE (Derivative): 1.888974e-07\n",
    "Final Validation Max Error (Derivative): 1.405239e-03\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/0.9.png\" alt=\"Weight: 0.9, Runge\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/0.9_d.png\" alt=\"Weight: 0.9, Derivative\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "### Deeper and Broader\n",
    "Drawing from the experience of the previous assignment, I tried to increase the model's capacity by adding more neurons per layer and deepening the network.\n",
    "```python\n",
    "n = 32\n",
    "params = {\n",
    "    'w1': jax.random.normal(w1_key, (1, n)), 'b1': jax.random.normal(b1_key, (n,)),\n",
    "    'w2': jax.random.normal(w2_key, (n, n)), 'b2': jax.random.normal(b2_key, (n,)),\n",
    "    'w3': jax.random.normal(w3_key, (n, n)), 'b3': jax.random.normal(b3_key, (n,)),\n",
    "    'w4': jax.random.normal(w4_key, (n, n)), 'b4': jax.random.normal(b4_key, (n,)),\n",
    "    'w5': jax.random.normal(w5_key, (n, 1)), 'b5': jax.random.normal(b5_key, (1,)),\n",
    "}\n",
    "\n",
    "def deep_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.tanh(x @ params['w1'] + params['b1'])       # tanh and sigmoid are equivalent\n",
    "    hidden2 = jax.nn.tanh(hidden1 @ params['w2'] + params['b2']) # Plus, tanh passes the origin.\n",
    "    hidden3 = jax.nn.tanh(hidden2 @ params['w3'] + params['b3'])\n",
    "    hidden4 = jax.nn.tanh(hidden3 @ params['w4'] + params['b4'])\n",
    "    output = hidden4 @ params['w5'] + params['b5']\n",
    "    return output\n",
    "```\n",
    "```\n",
    "Training finished in 741.53 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Loss: 3.510794e-08\n",
    "Final Training MSE: 1.843843e-10\n",
    "Final Training Max Error: 2.962351e-05\n",
    "Final Validation MSE: 1.848033e-10\n",
    "Final Training Max Error: 2.944469e-05\n",
    "\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 5.838452e-08\n",
    "Final Training Max Error (Derivative): 1.318753e-03\n",
    "Final Validation MSE (Derivative): 6.328137e-08\n",
    "Final Validation Max Error (Derivative): 1.294933e-03\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/0.9.png\" alt=\"Weight: 0.9, Runge\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/0.9_d.png\" alt=\"Weight: 0.9, Derivative\" height=\"400\"/>\n",
    "</div>\n",
    "However, this did not yield a significant improvement.\n",
    "\n",
    "Therefore, I try to modify the loss function again with the 3 hidden layers version.\n",
    "## Adaptive Weight\n",
    "### First Taste\n",
    "Observed that the largest prediction error for the derivative occurred in the region [-0.25, 0.25], where the magnitude of the true derivative is highest. This suggests that this is the most challenging region for the model to learn.\n",
    "```python\n",
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    pred_y = deep_model(params, x).squeeze()\n",
    "    loss_f = jnp.mean((pred_y - y)**2)\n",
    "    \n",
    "    true_dy = vmap_grad_f_true(x)\n",
    "    pred_dy = vmap_model_derivative(x, params)\n",
    "    \n",
    "    weights = jnp.square(true_dy) + 1.0 # |f'|+1, we add one to ensure that regions where the derivative is close to zero are not ignored during training.\n",
    "    loss_df = jnp.mean(weights * (pred_dy - true_dy)**2)+1 \n",
    "\n",
    "    return loss_f + rho * loss_df\n",
    "```\n",
    "```\n",
    "Training finished in 330.92 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Training MSE: 8.058843e-09\n",
    "Final Training Max Error: 1.857877e-04\n",
    "Final Validation MSE: 8.112671e-09\n",
    "Final Training Max Error: 1.850724e-04\n",
    "\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 1.031996e-06\n",
    "Final Training Max Error (Derivative): 4.230924e-03\n",
    "Final Validation MSE (Derivative): 1.061334e-06\n",
    "Final Validation Max Error (Derivative): 4.214019e-03\n",
    "```\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/adaptive weight.png\" alt=\"Using Adaptive Weight\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/adaptive weight_d.png\" alt=\"Using Asaptive Weight\" height=\"400\"/>\n",
    "</div>\n",
    "With this new loss function, the training did not converge stably using a large, fixed learning rate.\n",
    "\n",
    "### Armijo Line Search\n",
    "Therefore, I attempted to use Armijo Line Search to find an adaptive step size.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/Armijo Rule.png\" alt=\"Armijo Rule\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "> See [Lecture 3: Steepest and Gradient Descent-Part I/Instructor: Dimitrios Katselis](https://katselis.web.engr.illinois.edu/ECE586/Lecture3.pdf).\n",
    "\n",
    "```python\n",
    "import jax.lax\n",
    "def armijo_line_search(params, grads, x_batch, y_batch, sigma=0.1, beta=0.5, alpha_init=1.0):\n",
    "    p = jax.tree_util.tree_map(lambda g: -g, grads)\n",
    "    grad_p_products = jax.tree_util.tree_map(lambda g, p_leaf: jnp.sum(g * p_leaf), grads, p)\n",
    "    leaves = jax.tree_util.tree_leaves(grad_p_products)\n",
    "    grad_dot_p_scalar = jnp.sum(jnp.array(leaves))\n",
    "    current_loss = loss_fn(params, x_batch, y_batch)\n",
    "    def cond_fun(alpha):\n",
    "        new_params = jax.tree_util.tree_map(lambda p_old, p_dir: p_old + alpha * p_dir, params, p)\n",
    "        return loss_fn(new_params, x_batch, y_batch) > current_loss + sigma * alpha * grad_dot_p_scalar\n",
    "    def body_fun(alpha):\n",
    "        return alpha * beta\n",
    "    final_alpha = jax.lax.while_loop(cond_fun, body_fun, alpha_init)\n",
    "    return final_alpha\n",
    "```\n",
    "```\n",
    "Training finished in 192.59 seconds. # Using v5e1 on colab\n",
    "\n",
    "--- Final Result ---\n",
    "Final Training MSE: 4.450141e-07\n",
    "Final Training Max Error: 2.286375e-03\n",
    "Final Validation MSE: 4.317105e-07\n",
    "Final Validation Max Error: 2.052814e-03\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 4.765892e-05\n",
    "Final Training Max Error (Derivative): 4.574680e-02\n",
    "Final Validation MSE (Derivative): 4.482902e-05\n",
    "Final Validation Max Error (Derivative): 5.431034e-02\n",
    "```\n",
    "Nevertheless, implementing the Armijo rule significantly increased the runtime.\n",
    "Initially, I ran the program on my local machine (Macbook Air M4), but the process stalled around Epoch 23000. \n",
    "I then moved the experiment to Google Colab, but the runtime was still excessively long. \n",
    "Consequently, I opted to use Optax, a dedicated optimization library for JAX, as a more practical alternative.\n",
    "\n",
    "### Optax\n",
    "The final program is shown below.\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import optax \n",
    "\n",
    "# --- The Runge Function ---\n",
    "def f(x):\n",
    "    return 1 / (1 + 25 * x**2)\n",
    "\n",
    "grad_f_true = jax.grad(f)\n",
    "vmap_grad_f_true = jax.vmap(grad_f_true)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "initial_learning_rate = 0.005 \n",
    "epochs = 40000  \n",
    "datanum = 10000\n",
    "batch_size = 32\n",
    "n = 32        \n",
    "r = 0.5\n",
    "rho = 0.6     \n",
    "\n",
    "# --- Data Generation ---\n",
    "x_center = jnp.linspace(-0.5, 0.5, int(datanum * r))\n",
    "x_sides = jnp.concatenate([jnp.linspace(-1.0, -0.5, int(datanum * (1-r)/2)), jnp.linspace(0.5, 1.0, int(datanum *(1-r)/2))])\n",
    "x_train = jnp.concatenate([x_center, x_sides])\n",
    "x_train = jnp.sort(x_train)\n",
    "y_train = f(x_train)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Validation Data\n",
    "key, validation_key = jax.random.split(key)\n",
    "x_validation = jax.random.uniform(validation_key, shape=(6000,), minval=-1.0, maxval=1.0)\n",
    "y_validation = f(x_validation)\n",
    "\n",
    "# --- Optimizer Setup ---\n",
    "lr_schedule = optax.exponential_decay(\n",
    "    init_value=initial_learning_rate,\n",
    "    transition_steps=2000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = optax.adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# --- Initialize Parameters ---\n",
    "key, w1_key, b1_key, w2_key, b2_key, w3_key, b3_key = jax.random.split(key, 7)\n",
    "glorot_normal_initializer = jax.nn.initializers.glorot_normal()\n",
    "params = {\n",
    "    'w1': glorot_normal_initializer(w1_key, (1, n)), 'b1': jnp.zeros(n,),\n",
    "    'w2': glorot_normal_initializer(w2_key, (n, n)), 'b2': jnp.zeros(n,),\n",
    "    'w3': glorot_normal_initializer(w3_key, (n, 1)), 'b3': jnp.zeros(1,)\n",
    "}\n",
    "opt_state = optimizer.init(params) \n",
    "\n",
    "# --- Model Definition ---\n",
    "def deep_model(params, x):\n",
    "    x = x.reshape(-1, 1)\n",
    "    hidden1 = jax.nn.tanh(x @ params['w1'] + params['b1'])\n",
    "    hidden2 = jax.nn.tanh(hidden1 @ params['w2'] + params['b2'])\n",
    "    output = hidden2 @ params['w3'] + params['b3']\n",
    "    return output\n",
    "\n",
    "def mse(params, x, y):\n",
    "    predictions = deep_model(params, x).squeeze()\n",
    "    return jnp.mean((predictions - y)**2)\n",
    "\n",
    "# --- Derivative Calculation  ---\n",
    "def model_for_grad(x, model_params):\n",
    "    return deep_model(model_params, x).squeeze()[()]\n",
    "model_derivative_fn = jax.grad(model_for_grad, argnums=0)\n",
    "vmap_model_derivative = jax.vmap(model_derivative_fn, in_axes=(0, None))\n",
    "\n",
    "# --- Loss Function with Adaptive Weights ---\n",
    "@jax.jit\n",
    "def loss_fn(params, x, y):\n",
    "    pred_y = deep_model(params, x).squeeze()\n",
    "    loss_f = jnp.mean((pred_y - y)**2)\n",
    "    \n",
    "    true_dy = vmap_grad_f_true(x)\n",
    "    pred_dy = vmap_model_derivative(x, params)\n",
    "    \n",
    "    weights = jnp.square(true_dy) + 1.0\n",
    "    loss_df = jnp.mean(weights * (pred_dy - true_dy)**2)\n",
    "\n",
    "    return (1-rho)*loss_f + rho * loss_df\n",
    "\n",
    "# --- JIT-compiled Training Step for Optax ---\n",
    "@jax.jit\n",
    "def train_epoch(epoch_state, train_data):\n",
    "    params, opt_state, key = epoch_state\n",
    "    x_train, y_train = train_data\n",
    "    \n",
    "    key, perm_key = jax.random.split(key)\n",
    "    perm = jax.random.permutation(perm_key, len(x_train))\n",
    "    \n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    \n",
    "    def body_fun(step, val):\n",
    "        params, opt_state = val\n",
    "        batch_idx = jax.lax.dynamic_slice_in_dim(perm, step * batch_size, batch_size)\n",
    "        x_batch, y_batch = x_train[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, x_batch, y_batch)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params) # params needed for weight decay etc.\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        \n",
    "        return (params, opt_state)\n",
    "\n",
    "    final_params, final_opt_state = jax.lax.fori_loop(0, steps_per_epoch, body_fun, (params, opt_state))\n",
    "    \n",
    "    return final_params, final_opt_state, key\n",
    "\n",
    "# --- Training Loop ---\n",
    "loss_history = []\n",
    "print(\"\\n--- Start Training with Optax and All Optimizations ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "epoch_state = (params, opt_state, key)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    params, opt_state, key = train_epoch(epoch_state, (x_train, y_train))\n",
    "    epoch_state = (params, opt_state, key)\n",
    "    \n",
    "    if epoch % 1000 == 0: \n",
    "        loss = loss_fn(params, x_train, y_train)\n",
    "        loss_history.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Evaluation ---\n",
    "x_plot = jnp.linspace(-1, 1, 500, dtype=jnp.float32)\n",
    "y_true = f(x_plot)\n",
    "y_pred = deep_model(params, x_plot).squeeze()\n",
    "y_pred_train = deep_model(params, x_train).squeeze()\n",
    "final_loss = loss_fn(params, x_train, y_train)\n",
    "final_mse = mse(params, x_train, y_train)\n",
    "max_error = jnp.max(jnp.abs(y_pred_train - y_train))\n",
    "\n",
    "y_pred_validation= deep_model(params, x_validation).squeeze()\n",
    "validation_mse = mse(params, x_validation, y_validation)\n",
    "validation_max_error = jnp.max(jnp.abs(y_pred_validation - y_validation))\n",
    "\n",
    "\n",
    "print(f\"\\n--- Final Result ---\")\n",
    "print(f\"Final Loss: {final_loss:.6e}\")\n",
    "print(f\"Final Training MSE: {final_mse:.6e}\")\n",
    "print(f\"Final Training Max Error: {max_error:.6e}\")\n",
    "print(f\"Final Validation MSE: {validation_mse:.6e}\")\n",
    "print(f\"Final Validation Max Error: {validation_max_error:.6e}\")\n",
    "\n",
    "# --- Plotting ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_true, label='True Runge Function')\n",
    "plt.plot(x_plot, y_pred, label='Deep Tanh NN', linestyle='--')\n",
    "plt.scatter(x_validation, y_validation, label='Validation', color='orange', s=10, alpha=0.6)\n",
    "plt.title('Deep Network Approximation')\n",
    "plt.legend(); plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot both training and validation loss\n",
    "plt.plot(range(0, epochs, 1000), loss_history, label='Training Loss')\n",
    "plt.title('Training Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.yscale('log'); plt.grid(True); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Derivatives Evaluation ---\n",
    "y_d_true = vmap_grad_f_true(x_plot) \n",
    "y_d_true_validation = vmap_grad_f_true(x_validation) \n",
    "y_d_pred = vmap_model_derivative(x_plot, params)\n",
    "y_d_pred_validation = vmap_model_derivative(x_validation, params)\n",
    "\n",
    "pred_derivative_on_train = vmap_model_derivative(x_train, params)\n",
    "pred_derivative_on_validation = vmap_model_derivative(x_validation, params)\n",
    "true_derivative_on_train = vmap_grad_f_true(x_train)\n",
    "true_derivative_on_validation = vmap_grad_f_true(x_validation)\n",
    "final_derivative_mse = jnp.mean((pred_derivative_on_train - true_derivative_on_train)**2)\n",
    "final_derivative_mse_validation = jnp.mean((pred_derivative_on_validation - true_derivative_on_validation)**2)\n",
    "max_derivative_error = jnp.max(jnp.abs(y_d_pred - y_d_true))\n",
    "max_derivative_error_validation = jnp.max(jnp.abs(y_d_pred_validation - y_d_true_validation))\n",
    "\n",
    "\n",
    "print(f\"\\n--- Final Result (Derivatives) ---\")\n",
    "print(f\"Final Training MSE (Derivative): {final_derivative_mse:.6e}\")\n",
    "print(f\"Final Training Max Error (Derivative): {max_derivative_error:.6e}\")\n",
    "print(f\"Final Validation MSE (Derivative): {final_derivative_mse_validation:.6e}\")\n",
    "print(f\"Final Validation Max Error (Derivative): {max_derivative_error_validation:.6e}\")\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x_plot, y_d_true, label='True Derivative of Runge Function')\n",
    "plt.plot(x_plot, y_d_pred, label='Deep Tanh NN Derivative', linestyle='--')\n",
    "plt.scatter(x_validation, y_d_true_validation, label='Validation (True)', color='orange', s=10, alpha=0.6) # Scatter plot for derivative\n",
    "plt.title(\"Comparison of Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "Training finished in 286.69 seconds.\n",
    "\n",
    "--- Final Result ---\n",
    "Final Loss: 2.727338e-07\n",
    "Final Training MSE: 8.710958e-10\n",
    "Final Training Max Error: 6.997585e-05\n",
    "Final Validation MSE: 8.561231e-10\n",
    "Final Validation Max Error: 6.990880e-05\n",
    "--- Final Result (Derivatives) ---\n",
    "Final Training MSE (Derivative): 1.789333e-07\n",
    "Final Training Max Error (Derivative): 1.592696e-03\n",
    "Final Validation MSE (Derivative): 1.770964e-07\n",
    "Final Validation Max Error (Derivative): 1.596153e-03\n",
    "```\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"./Optimal_Sigmoid/optax 32 0.5.png\" alt=\"Using Adaptive Weight\" height=\"400\"/>\n",
    "<img src=\"./Optimal_Sigmoid/optax 32 0.5_d.png\" alt=\"Using Asaptive Weight\" height=\"400\"/>\n",
    "</div>\n",
    "\n",
    "This approach successfully achieved our goal, yielding excellent results for both the function and its derivative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
