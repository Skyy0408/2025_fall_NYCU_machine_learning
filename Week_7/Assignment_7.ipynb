{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e01a83-3b1f-4a43-b3e0-a02310d0ad7f",
   "metadata": {},
   "source": [
    "# Assignment 1: Equivalence of Score Matching (ESM) and (ISM)\n",
    "\n",
    "This document, based on the paper [Vincent]Denoising_Score_Matching.pdf, explains the mathematical equivalence between Explicit Score Matching (ESM) and Implicit Score Matching (ISM).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Basic Definitions\n",
    "\n",
    "First, we define the relevant notation:\n",
    "\n",
    "* **Model probability density:** $p(x;\\theta)$, parameterized by $\\theta$.\n",
    "* **True data probability density:** $q(x)$ (unknown).\n",
    "* **Model Score Function:**\n",
    "    $$\\psi(x;\\theta) = \\nabla_x \\log p(x;\\theta)$$\n",
    "* **True data Score Function:**\n",
    "    $$\\nabla_x \\log q(x)$$\n",
    "\n",
    "## 2. Explicit Score Matching (ESM)\n",
    "\n",
    "The objective of ESM is to directly minimize the expected L2 distance between the model score and the true data score.\n",
    "\n",
    "**ESM Objective Function (Eq. 2):**\n",
    "$$J_{ESMq}(\\theta) = \\mathbb{E}_{q(x)} \\left[ \\frac{1}{2} || \\psi(x;\\theta) - \\nabla_x \\log q(x) ||^2 \\right]$$\n",
    "\n",
    "This objective is intuitive but **cannot be computed directly**, as we do not know the true data score $\\nabla_x \\log q(x)$.\n",
    "\n",
    "## 3. Implicit Score Matching (ISM)\n",
    "\n",
    "ISM, proposed by Hyv√§rinen (2005), is an equivalent objective function that cleverly avoids the need to compute $\\nabla_x \\log q(x)$.\n",
    "\n",
    "**ISM Objective Function (Eq. 3):**\n",
    "$$J_{ISMq}(\\theta) = \\mathbb{E}_{q(x)} \\left[ \\text{tr}(\\nabla_x \\psi(x;\\theta)) + \\frac{1}{2} || \\psi(x;\\theta) ||^2 \\right]$$\n",
    "\n",
    "Where $\\text{tr}(\\nabla_x \\psi(x;\\theta))$ is the trace of the Jacobian matrix of the model score function $\\psi$, i.e., $\\sum_{i} \\frac{\\partial \\psi_i(x;\\theta)}{\\partial x_i}$.\n",
    "\n",
    "This objective function **is computable**, as it only depends on the model score $\\psi(x;\\theta)$ and its derivatives, and the data distribution $q(x)$ (for the expectation $\\mathbb{E}_{q(x)}$, which can be estimated via a sample mean).\n",
    "\n",
    "## 4. Equivalence Proof (ESM $\\Leftrightarrow$ ISM)\n",
    "\n",
    "To prove that $J_{ESMq}(\\theta)$ and $J_{ISMq}(\\theta)$ are equivalent for minimizing $\\theta$, we show that they differ only by a constant that does not depend on $\\theta$.\n",
    "\n",
    "1.  We expand the ESM objective function $J_{ESMq}(\\theta)$:\n",
    "    $$J_{ESMq}(\\theta) = \\mathbb{E}_{q(x)} \\left[ \\frac{1}{2} ||\\psi(x;\\theta)||^2 - \\psi(x;\\theta)^T (\\nabla_x \\log q(x)) + \\frac{1}{2} ||\\nabla_x \\log q(x)||^2 \\right]$$\n",
    "\n",
    "2.  This expression can be split into three terms:\n",
    "    * (a) $\\mathbb{E}_{q(x)} \\left[ \\frac{1}{2} ||\\psi(x;\\theta)||^2 \\right]$\n",
    "    * (b) $\\mathbb{E}_{q(x)} \\left[ - \\psi(x;\\theta)^T (\\nabla_x \\log q(x)) \\right]$\n",
    "    * (c) $\\mathbb{E}_{q(x)} \\left[ \\frac{1}{2} ||\\nabla_x \\log q(x)||^2 \\right]$\n",
    "\n",
    "3.  Observe the third term (c). It depends only on the true data distribution $q(x)$ and not on the model parameters $\\theta$. Therefore, it is a constant (let $C = (c)$) and can be ignored during minimization.\n",
    "\n",
    "4.  We focus on transforming the second term (b), using $\\nabla_x \\log q(x) = \\frac{\\nabla_x q(x)}{q(x)}$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    (b) &= \\mathbb{E}_{q(x)} \\left[ - \\psi(x;\\theta)^T \\frac{\\nabla_x q(x)}{q(x)} \\right] \\\\\n",
    "    &= -\\int q(x) \\left[ \\psi(x;\\theta)^T \\frac{\\nabla_x q(x)}{q(x)} \\right] dx \\\\\n",
    "    &= -\\int \\psi(x;\\theta)^T \\nabla_x q(x) dx\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "5.  Apply **integration by parts** (or the Gaussian divergence theorem) to the expression above, assuming $q(x)\\psi(x;\\theta)$ vanishes at the boundaries:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    (b) &= - \\left[ \\psi(x;\\theta)^T q(x) \\right]_{-\\infty}^{\\infty} + \\int q(x) (\\nabla_x \\cdot \\psi(x;\\theta)) dx \\\\\n",
    "    &= 0 + \\int q(x) \\text{tr}(\\nabla_x \\psi(x;\\theta)) dx \\\\\n",
    "    &= \\mathbb{E}_{q(x)} \\left[ \\text{tr}(\\nabla_x \\psi(x;\\theta)) \\right]\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    *Note: The divergence $\\nabla_x \\cdot \\psi$ is equal to the trace of the Jacobian $\\text{tr}(\\nabla_x \\psi)$.*\n",
    "\n",
    "6.  Substitute the results for (a) and (b) back into the expression for $J_{ESMq}(\\theta)$ (ignoring the constant $C$):\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    J_{ESMq}(\\theta) &= \\mathbb{E}_{q(x)} \\left[ \\text{tr}(\\nabla_x \\psi(x;\\theta)) \\right] + \\mathbb{E}_{q(x)} \\left[ \\frac{1}{2} ||\\psi(x;\\theta)||^2 \\right] + C \\\\\n",
    "    J_{ESMq}(\\theta) &= \\mathbb{E}_{q(x)} \\left[ \\text{tr}(\\nabla_x \\psi(x;\\theta)) + \\frac{1}{2} || \\psi(x;\\theta) ||^2 \\right] + C\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "7.  We find that:\n",
    "    $$J_{ESMq}(\\theta) = J_{ISMq}(\\theta) + C$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "The Explicit Score Matching (ESM) and Implicit Score Matching (ISM) objective functions differ only by a constant $C$ that does not depend on $\\theta$. Therefore, minimizing $J_{ESMq}(\\theta)$ is equivalent to minimizing $J_{ISMq}(\\theta)$.\n",
    "\n",
    "The key advantage of ISM is that it transforms the dependency on the unknown term $\\nabla_x \\log q(x)$ into a computation involving the second-order derivatives (trace of the Hessian) of the model $\\psi(x;\\theta)$, which makes the objective function fully known and computable.\n",
    "\n",
    "---\n",
    "Also, [this](https://bobondemon.github.io/2022/01/08/Estimation-of-Non-Normalized-Statistical-Models-by-Score-Matching/) site explain it explicitly.\n",
    "\n",
    "# Question:\n",
    "1. Does there exist other score function?\n",
    "2. What about its efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e3250-74de-48a8-8c69-403083b161c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
