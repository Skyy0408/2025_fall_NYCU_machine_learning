{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9df1e02-e867-4737-8960-a26ff28cbf03",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "> ## Week 1: \n",
    "\n",
    "<figure id=\"Week_1.png\">\n",
    "    <img src=\"Week_1.png\" alt=\"Week_1\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 1</b>: The Problem in Week 1.</figcaption>\n",
    "</figure>\n",
    "\n",
    "I've already answered it in the same assignment, by \n",
    "\n",
    "(a) See: [What Happened When Gradient is almost zero?](https://docs.google.com/presentation/d/1siUFXARYRpNiMeSRwgFbt7mZVjkMPhR5od09w0Z8xaU/edit?slide=id.g31470fd33a_0_33\\#slide=id.g31470fd33a_0_33)\n",
    "\n",
    "(b) See: [【機器學習2021】類神經網路訓練不起來怎麼辦 (二)： 批次 (batch) 與動量 (momentum) 22:37}](https://www.youtube.com/watch?v=zzbr1h9sF54&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=5)\n",
    "\n",
    "(c) See: [【機器學習2021】類神經網路訓練不起來怎麼辦 (三)：自動調整學習速率 (Learning Rate) 00:00}](https://www.youtube.com/watch?v=HYUXEeh3kwY&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=7)\n",
    "\n",
    "> ## Week 2:\n",
    "I was wondering how to fix the problem(the oscillation on the both sides, which is called the Runge Phenomenon) in Figure 10 below. I've tried to change the way I select the data but it doesn't work.\n",
    "\n",
    "<figure id=\"Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001\">\n",
    "    <img src=\"Fourier_0.01_20000_10001_3.png\" alt=\"Fourier 3 Layers\" style=\"width: 70%;\">\n",
    "    <figcaption><b>Figure 10</b>: Fourier Learning rate:0.01 Layer:3 Epoch:20000 Datanum:10001.</figcaption>\n",
    "</figure>\n",
    "\n",
    "### Answer by Perplexity:\n",
    "\n",
    "[Six strategies for defeating the Runge Phenomenon in Gaussian radial basis functions on a finite interval](https://www.sciencedirect.com/science/article/pii/S0898122110007947)\n",
    "\n",
    "[lecture 15: Chebyshev Polynomials for Optimal Interpolation](https://personal.math.vt.edu/embree/math5466/lecture15.pdf)\n",
    "\n",
    "[Matrix method and the suppression of Runge’s phenomenon Shui-Fa Shen, Wei-Liang Qian](https://scipost.org/SciPostPhysCore.7.2.034/pdf)\n",
    "[Example for interpolation](https://math.umd.edu/~petersd/460/html/interpol_ex.html)\n",
    "\n",
    "> ## Week 3: None.\n",
    "\n",
    "> ## Week 4: None.\n",
    "\n",
    "> ## Week 5: None.\n",
    "\n",
    "> ## Week 6: How can the max error be reduced without incorporating additional features like altitude or humidity?\n",
    "\n",
    "Analysis: Based on the results, even the best-performing model exhibits a maximum error of nearly 9°C. This suggests we've reached the performance limit of what can be achieved with the current feature set. In my opinion, it's likely impossible to develop a \"perfect\" model with such a low max error without providing it with more informative data. The fundamental limitation appears to be the absence of crucial predictive variables, not the model architecture itself.\n",
    "\n",
    "### Answer by Perplexity:\n",
    "\n",
    "Use GNN First,\n",
    "\n",
    "[Weather prediction using Spatial-Temporal Graph Neural\n",
    "Networks](https://projects.cs.uct.ac.za/honsproj/cgi-bin/view/2023/amir_gaibie_hammerschlag.zip/documents/literature_review-hamza.pdf)\n",
    "\n",
    "[Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/html/2511.00049v1)\n",
    "\n",
    "Then, there are some skills:\n",
    "\n",
    "[A generalizable and accessible approach to machine learning with global satellite imagery](https://www.nature.com/articles/s41467-021-24638-z)\n",
    "\n",
    "[Ensemble Weather Forecasting](https://www.sciencedirect.com/science/article/pii/S0360544224015603)\n",
    "\n",
    "[Spatial Interpolation and Temperature Prediction](https://rmets.onlinelibrary.wiley.com/doi/10.1002/joc.8641)\n",
    "\n",
    "> ## Week 7: Abou Explicit Score Matching(ESM) and Implicit Score Matching(ISM), \n",
    "1. Does there exist other score function?\n",
    "2. What about its efficiency?\n",
    "\n",
    "### **1. Do other score functions exist?**\n",
    "\n",
    "**Yes, at least 5 major variants**:[1][2][3][4]\n",
    "\n",
    "| Method | Key Difference |\n",
    "|--------|---|\n",
    "| **ESM/ISM** | Direct Hessian computation |\n",
    "| **DSM** (Denoising) | Add noise, avoid Hessian[5][6] |\n",
    "| **SSM** (Sliced) | Random projections, high-D scalable[1][7] |\n",
    "| **FD-SM** (Finite-Difference) | Function evaluations only, parallelizable[2][8] |\n",
    "| **Fisher/Spectral** | Specialized variants[9][10] |\n",
    "\n",
    "**Most practical**: DSM in DDPM, SSM for high-dimensions, FD-SM for memory efficiency.[2][6][1]\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Efficiency Comparison**\n",
    "\n",
    "#### **Computational Complexity**[3][1][2]\n",
    "\n",
    "| Method | Time | Memory | Parallelizable |\n",
    "|--------|------|--------|---|\n",
    "| **ESM/ISM** | $$O(D^2)$$ | High | No |\n",
    "| **DSM** | $$O(D)$$ | Low | Yes |\n",
    "| **SSM** | $$O(MD)$$ | Moderate | Partial |\n",
    "| **FD-SM** | $$O(D)$$ | Low | **Yes** |\n",
    "\n",
    "**Practical speedup**: FD-SM **2-3× faster** than ESM.[11][2]\n",
    "\n",
    "***\n",
    "\n",
    "#### **Statistical Efficiency**[4][12][3]\n",
    "\n",
    "**Key finding** (Koehler et al. 2023):[3][4]\n",
    "- Score matching efficiency **comparable to MLE when isoperimetric constant is small**[4]\n",
    "- Can be **arbitrarily worse** with large isoperimetric constant[3]\n",
    "- Advantage: **Avoids computing intractable partition function** (NP-hard)[13]\n",
    "\n",
    "***\n",
    "\n",
    "### **When to Use Each?**\n",
    "\n",
    "| Use Case | Method | Citation |\n",
    "|----------|--------|----------|\n",
    "| **Diffusion models** | DSM | [1][6] |\n",
    "| **High-dimensional** | SSM | [1][7] |\n",
    "| **Limited memory** | FD-SM | [2][8] |\n",
    "| **Parallel GPU** | FD-SM | [2] |\n",
    "\n",
    "***\n",
    "\n",
    "[1](http://proceedings.mlr.press/v115/song20a/song20a-supp.pdf)\n",
    "[2](https://dl.acm.org/doi/10.5555/3495724.3497333)\n",
    "[3](https://par.nsf.gov/servlets/purl/10430923)\n",
    "[4](https://openreview.net/pdf?id=TD7AnQjNzR6)\n",
    "[5](https://bobondemon.github.io/2022/03/06/A-Connection-Between-Score-Matching-and-Denoising-Autoencoders/)\n",
    "[6](https://chen-hao-chao.github.io/dlsm/)\n",
    "[7](https://bobondemon.github.io/2022/03/06/Sliced-Score-Matching-A-Scalable-Approach-to-Density-and-Score-Estimation/)\n",
    "[8](https://arxiv.org/abs/2007.03317)\n",
    "[9](https://arxiv.org/pdf/2506.06542.pdf)\n",
    "[10](https://pubs.acs.org/doi/10.1021/acsomega.0c05041)\n",
    "[11](https://ml.cs.tsinghua.edu.cn/~tianyu/FDSM/FD-Poster.pdf)\n",
    "[12](https://arxiv.org/pdf/2210.00726.pdf)\n",
    "[13](https://proceedings.neurips.cc/paper_files/paper/2023/file/c11f8d40c119867e30e3421f696f931d-Paper-Conference.pdf)\n",
    "\n",
    "> ## Week 8:\n",
    "I have attended a speech which mentioned the Brownian Motion and Heat Equation, also, it is used in the financial field. However, there are planty of unexpected incidents happened in recent year, is it also suitable to use Brownian Motion to explain or estimate the stock market?\n",
    "\n",
    "### Answer by Perplexity:\n",
    "\n",
    "#### **Applicability of Brownian Motion in Stock Market Forecasting: English Markdown Version**\n",
    "\n",
    "##### **Core Question**\n",
    "\n",
    "With recent abnormal market events (COVID-19, geopolitical shocks, drastic interest-rate changes), **is the Brownian Motion model still applicable?**\n",
    "\n",
    "---\n",
    "\n",
    "##### **Positive Perspectives** ✓\n",
    "\n",
    "| Advantage                             | Literature Support                                                                                                      |\n",
    "| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Simple and efficient**              | *Financial Modeling with Geometric Brownian Motion* (2024)[1]: “Its simplicity justifies its continued widespread use.” |\n",
    "| **Short-term prediction is feasible** | *Georgia Tech* (2024)[2]: “For a 6-month window, AAPL and other stocks achieved MAPE = 6.87%.”                          |\n",
    "| **Standard for option pricing**       | *Merton’s Jump-Diffusion Model*[3]: Black–Scholes remains the industry gold standard.                                   |\n",
    "| **Effective during stable periods**   | Wolfram Demonstrations[4]: Performs relatively accurately when markets are normal.                                      |\n",
    "\n",
    "---\n",
    "\n",
    "##### **Negative Perspectives** ✗\n",
    "\n",
    "| Limitation                                     | Literature Support                                                                                   | Empirical Evidence                                           |\n",
    "| ---------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| **Fails to capture extreme events**            | SCIRP 2024[1]: “Cannot explain market crashes and price jumps.”                                      | COVID-19 drop of 35%, GBM-predicted probability < 0.001%     |\n",
    "| **Constant-volatility assumption breaks down** | QFE University 2023[5]: “Volatility is not constant, especially in high-volatility regimes.”         | GARCH/SV outperform GBM during crises[6]                     |\n",
    "| **Fat-tail problem**                           | University of Potsdam 2020[7]: “GBM cannot reproduce empirically observed fat-tailed distributions.” | Actual returns have skewness ≠ 0 and excess kurtosis > 0[7]  |\n",
    "| **Incorrect normal-distribution assumption**   | QFE 2023[5]: “Returns are non-normal; extreme events are under-estimated.”                           | 2008 crisis & 2020 crash confirm this[5]                     |\n",
    "| **Cannot model jumps**                         | Merton (1976)[4]: “Only with jump processes can crashes be modeled.”                                 | Earnings surprises and geopolitical shocks cause jumps[4][3] |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Empirical Comparison Table**\n",
    "\n",
    "##### **GBM vs Alternative Models**\n",
    "\n",
    "| Model                                  | Best Use Case                  | Limitations                              | Literature          |\n",
    "| -------------------------------------- | ------------------------------ | ---------------------------------------- | ------------------- |\n",
    "| **GBM**                                | Normal markets, teaching       | Fat tails, no jumps, constant volatility | SCIRP 2024[1]       |\n",
    "| **Merton Jump-Diffusion**              | Option pricing, crisis periods | Simplified jump assumptions              | Merton 1976[4][3]   |\n",
    "| **Fractional Brownian Motion (FBM)**   | Long-term trends, long memory  | Computationally complex                  | Bratian 2021[8]     |\n",
    "| **Multifractal Brownian Motion (MBM)** | Post-COVID markets             | Parameter estimation difficulty          | Frezza 2020[9]      |\n",
    "| **GARCH/SV**                           | Volatility clustering          | No jumps                                 | U. Arkansas 2021[6] |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Market Efficiency After COVID-19**\n",
    "\n",
    "**Multifractal analysis** (2020, 15 global indices)[9] shows:\n",
    "\n",
    "* **Asian markets**: fully recovered efficiency ✓\n",
    "* **US/EU markets**: efficiency not restored ✗\n",
    "* **Conclusion**: GBM’s market-efficiency assumption does **not** hold post-pandemic\n",
    "\n",
    "---\n",
    "\n",
    "#### **Practical Recommendations**\n",
    "\n",
    "##### **Layered Strategy** [1][4][5]\n",
    "\n",
    "```\n",
    "✓ Tier 1 (Essential Upgrades)\n",
    "  → Add jump processes: Merton Model [3][9]\n",
    "  → Time-varying volatility: GARCH/SV [7]\n",
    "\n",
    "✓ Tier 2 (Advanced)\n",
    "  → Extreme Value Theory (EVT) + ML\n",
    "  → Fractal models for long memory [4][8]\n",
    "\n",
    "✗ Not Recommended\n",
    "  → Using GBM alone for risk management [1][2][8]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Literature Sources**\n",
    "\n",
    ":\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "| Conclusion                                         | Evidence     |\n",
    "| -------------------------------------------------- | ------------ |\n",
    "| **Standalone GBM is outdated**                     | [1][5][7]    |\n",
    "| **Hybrid models are necessary**                    | [4][8][6][9] |\n",
    "| **Jumps must be included during crises**           | [1][9][3]    |\n",
    "| **GBM acceptable as a baseline in normal periods** | [1][2]       |\n",
    "\n",
    "> **Yalincak (2005)**: “Although GBM’s limitations are clear, its simplicity ensures widespread adoption. **No single model solves everything**.”[1]\n",
    "\n",
    "[1](https://benfeifke.com/posts/feature-engineering-with-latitude-and-longitude/) [2](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/75987646/d31eaa95-6503-4b6d-8937-73dba4e17c98/NN_Temperature_Error_Test_Set.jpg) [3](https://skoge.folk.ntnu.no/prost/proceedings/dycops-2022/files/0055.pdf) [4](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/75987646/ed40203e-1994-46ef-9dc7-cb026fd35242/JAX_Temperature_Error_Test_Set.jpg) [5](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/75987646/fc77efd6-bddd-448c-843b-13e4bf2ca414/NN_Temperature_Error_Validation_Set.jpg) \n",
    "[6](https://www.numdam.org/item/10.1016/j.crma.2006.05.007.pdf) [7](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/75987646/79e66977-2169-4fe5-b11b-6a0b8e206c44/JAX_Temperature_Error_Training_Set.jpg) [8](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/75987646/cc90709b-1b4b-437d-a880-951418a8bff7/NN_Temperature_Error_Training_Set.jpg) [9](https://www.sciencedirect.com/science/article/abs/pii/S0950705124015697) [10](https://qfeuniversity.com/interview-question-on-geometric-brownian-motion-gbm/) [11](https://demonstrations.wolfram.com/OptionPricesInMertonsJumpDiffusionModel/) [12](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5086026) [13](https://www.scirp.org/journal/paperinformation?paperid=132102) [14](https://quant-next.com/the-merton-jump-diffusion-model-for-option-pricing/) [15](https://pmc.ncbi.nlm.nih.gov/articles/PMC9761193/) [16](https://www.reddit.com/r/math/comments/rcqqz/why_is_a_geometric_brownian_motion_with_with/) [17](https://www.csie.ntu.edu.tw/~lyuu/finance1/2018/20180516.pdf) [18](https://www.airitilibrary.com/Article/Detail/U0001-2706200900284600) [19](https://www.agnld.uni-potsdam.de/papers/2020_StoSanBasKocMe_Entropy22.pdf)\n",
    "\n",
    "Week 9: None\n",
    "\n",
    "Week 10: If every stochastic diffusion process has a corresponding probability flow ODE that preserves its marginal distributions, does this mean randomness is fundamentally unnecessary for generative modeling?\n",
    "\n",
    "## Week 10: Is Randomness Fundamentally Unnecessary for Generative Modeling?\n",
    "\n",
    "### **Short Answer: No. Randomness remains essential, but for nuanced reasons.**\n",
    "\n",
    "Although stochastic diffusion processes theoretically have equivalent probability flow ODEs with identical marginal distributions, **randomness is not fundamentally unnecessary** in practice. Research reveals both approaches have distinct advantages, with randomness playing crucial roles in specific contexts.[1][2][3][4][5][6]\n",
    "\n",
    "---\n",
    "\n",
    "## **Theoretical Equivalence vs Practical Differences**\n",
    "\n",
    "### **✓ Mathematical Equivalence**[2][3][7][1]\n",
    "\n",
    "| Fact | Evidence |\n",
    "|------|----------|\n",
    "| **Identical marginal distributions** | [Yang Song 2021][3]: ODE and SDE trajectories **share the same marginal distributions** |\n",
    "| **Mathematically derivable** | [Chen et al. 2023][1]: Probability flow ODE obtained by removing stochastic terms from SDE |\n",
    "| **Interchangeable sampling** | [CTM 2023][7]: Same model sampled via SDE or ODE solvers |\n",
    "\n",
    "**Probability Flow ODE**:[3][7]\n",
    "$$\n",
    "d\\mathbf{x} = \\left[f(\\mathbf{x},t) - \\frac{1}{2}g^2(t)\\nabla_{\\mathbf{x}}\\log p_t(\\mathbf{x})\\right]dt\n",
    "$$\n",
    "\n",
    "This is **deterministic** (no stochastic term $$dW_t$$) yet maintains marginal equivalence.[7][3]\n",
    "\n",
    "***\n",
    "\n",
    "### **✗ Practical Importance of Randomness**\n",
    "\n",
    "Recent research (2023-2025) reveals randomness remains necessary across multiple dimensions:[4][5][6]\n",
    "\n",
    "***\n",
    "\n",
    "## **1. Sample Diversity**\n",
    "\n",
    "### **Stochastic Advantage**[5][8][9][4]\n",
    "\n",
    "| Metric | Deterministic (ODE/DDIM) | Stochastic (SDE/DDPM) | Citation |\n",
    "|--------|----------------------|-------------------|----------|\n",
    "| **Output variation** | Identical from same seed | Unique each time | [APXML 2025][4] |\n",
    "| **Mode collapse risk** | Higher | Lower | [Gaubil 2024][8] |\n",
    "| **Creative applications** | Repetitive | High diversity | [Milvus 2025][10] |\n",
    "\n",
    "**Evidence**:[9][5]\n",
    "- **Stochastic samplers produce better FID** on ImageNet generative tasks[5]\n",
    "- **Diversity optimization needed**: Without explicit optimization, deterministic sampling produces similar outputs[9]\n",
    "\n",
    "> **[APXML 2025]**: \"Stochastic samplers typically generate **more diverse outputs** from the same initial latent noise. Injected randomness creates variations in the sampling path, resulting in slightly different final samples.\"[4]\n",
    "\n",
    "***\n",
    "\n",
    "## **2. Discretization Error Mitigation**\n",
    "\n",
    "### **Randomness as Regularization**[6][5]\n",
    "\n",
    "**Key Finding** (2024):[5]\n",
    "\n",
    "| Sampling Steps | Deterministic Error | Randomness Effect |\n",
    "|---|---|---|\n",
    "| **50 steps** | Significant variance underestimation | Substantially improves estimates |\n",
    "| **100 steps** | Still biased | Mitigates bias effectively |\n",
    "| **1000 steps** | Reduced bias | Minimal difference |\n",
    "\n",
    "**'s Empirical Results**:[5]\n",
    "1. **Deterministic flow models exhibit bias**: Continuous underestimation of variance in finite discretization\n",
    "2. **Randomness alleviates bias**: Increasing diffusion coefficient scale (enhanced randomness) significantly improves variance estimation\n",
    "3. **Robustness trade-off**: Stochastic sampling maintains robustness with coarse discretization; deterministic methods fail\n",
    "\n",
    "***\n",
    "\n",
    "## **3. Convergence Rate and Dimensionality**\n",
    "\n",
    "### **ODE Theoretical Advantage**[11][1][2]\n",
    "\n",
    "| Method | Dimension Dependency | Citation |\n",
    "|--------|---|---|\n",
    "| **Probability flow ODE + corrector** | $$O(\\sqrt{d})$$ | [Chen et al. 2023][1] |\n",
    "| **DDPM (pure SDE)** | $$O(d)$$ | [Chen et al. 2023][1] |\n",
    "| **Heun solver (ODE)** | First Wasserstein convergence bound | [Beyler 2025][11] |\n",
    "\n",
    "**Breakthrough Results**:[1][2]\n",
    "- **Minimax optimality**: Probability flow ODE achieves near-minimax optimal sample complexity[2]\n",
    "- **Polynomial time guarantees**: First complete polynomial-time bounds for probability flow ODE[1]\n",
    "\n",
    "> **[Cai et al. 2025]**: \"Our result shows diffusion models can **learn any distribution** without restrictive assumptions.\"[2]\n",
    "\n",
    "**Critical caveat**: These advantages require **exact score functions**. In practice, score estimation errors dominate.[11][2]\n",
    "\n",
    "***\n",
    "\n",
    "## **4. Training Phase Still Requires Randomness**[12][13][14]\n",
    "\n",
    "| Technique | Randomness Necessity | Reason | Citation |\n",
    "|-----------|---|---|---|\n",
    "| **Dropout regularization** | Essential | Prevents overfitting | [Milvus 2025][13] |\n",
    "| **Stochastic depth** | Essential | Forces feature diversity | [Milvus 2025][13] |\n",
    "| **Data augmentation** | Essential | Enhances generalization | [D²C 2024][12] |\n",
    "| **EMA smoothing** | Indirect benefit | Requires stochastic training dynamics | [NVIDIA 2024][14] |\n",
    "\n",
    "**Evidence**:[13][14]\n",
    "- **Dropout rates 0.1-0.3** critical in Stable Diffusion attention blocks[13]\n",
    "- **Training dynamics cannot be replaced** by deterministic processes[14]\n",
    "\n",
    "***\n",
    "\n",
    "## **5. Use Case Trade-offs**\n",
    "\n",
    "### **When to Choose What?**[10][15][4]\n",
    "\n",
    "| Priority | Recommended Method | Citation |\n",
    "|---|---|---|\n",
    "| **Speed** | DDIM (η=0) | 50 steps ≈ DDPM-1000 steps quality[15][10] |\n",
    "| **Reproducibility** | Deterministic ODE | Fixed seed → fixed output[4] |\n",
    "| **Creative diversity** | Stochastic SDE | Avoid mode collapse[8] |\n",
    "| **Highest fidelity** | DDPM (T=1000) | Best known fidelity[15] |\n",
    "| **Few-step sampling** | Optimized ODE solver | DPM-Solver++, UniPC[4] |\n",
    "\n",
    "**Hybrid Strategies**:[4][5]\n",
    "- **Predictor-corrector**: ODE prediction + stochastic correction\n",
    "- **Configurable randomness**: DDIM's $$\\eta$$ parameter controls stochasticity level[15][16]\n",
    "\n",
    "***\n",
    "\n",
    "## **6. Latest Findings (2024-2025)**\n",
    "\n",
    "### **New Understanding of Randomness**[17][6][5]\n",
    "\n",
    "| Research Direction | Finding | Citation |\n",
    "|---|---|---|\n",
    "| **Piecewise Deterministic Markov Processes** | Deterministic motion + stochastic jumps combination | [ICLR 2024][17] |\n",
    "| **Stochastic Sampling from Deterministic Flows** | Can derive SDE families from deterministic flows | [arXiv 2024][5] |\n",
    "| **Closing ODE-SDE Gap** | Distance between two critical for performance | [Royal Society 2025][6] |\n",
    "\n",
    "**'s Key Conclusions** (2024):[5]\n",
    "1. Stochastic sampling tends to produce better FID\n",
    "2. Randomness makes FID robust to discretization\n",
    "3. Stochastic sampling improves diversity at all guidance levels\n",
    "4. Optimal performance emerges from balancing both approaches\n",
    "\n",
    "***\n",
    "\n",
    "## **Summary: Why Randomness Remains Necessary**\n",
    "\n",
    "| Reason | Evidence Strength | Representative Papers |\n",
    "|---|---|---|\n",
    "| **Sample diversity** | ⭐⭐⭐⭐⭐ | [4][5][8][9] |\n",
    "| **Discretization robustness** | ⭐⭐⭐⭐ | [1][5] |\n",
    "| **Training regularization** | ⭐⭐⭐⭐⭐ | [13][14] |\n",
    "| **Mode collapse prevention** | ⭐⭐⭐⭐ | [8][18] |\n",
    "| **Limited-step robustness** | ⭐⭐⭐⭐ | [4][5] |\n",
    "\n",
    "***\n",
    "\n",
    "## **Final Answer**\n",
    "\n",
    "**Marginal equivalence ≠ Randomness irrelevance**\n",
    "\n",
    "While **theoretically** probability flow ODEs and SDEs share marginal distributions:[3][7]\n",
    "\n",
    "✓ **Sampling stage**:\n",
    "- Deterministic methods (DDIM) excel in speed and reproducibility[10][15]\n",
    "- Stochastic methods (DDPM) excel in diversity and robustness[8][4][5]\n",
    "- **Best practice**: Choose based on application requirements[15][4]\n",
    "\n",
    "✓ **Training stage**:\n",
    "- Randomness via dropout, augmentation **remains indispensable**[14][13]\n",
    "- Training dynamics' randomness fundamentally affects model quality[14]\n",
    "\n",
    "✓ **Theory**:\n",
    "- ODEs offer **dimension-dependent advantages** in convergence[1][2]\n",
    "- But require **exact score functions**, rarely achievable in practice[11][2]\n",
    "\n",
    "***\n",
    "\n",
    "### **References**\n",
    "\n",
    ":\n",
    "\n",
    "[1](https://openreview.net/forum?id=KD6MFeWSAd)\n",
    "[2](https://arxiv.org/pdf/2503.09583.pdf)\n",
    "[3](https://yang-song.net/blog/2021/score/)\n",
    "[4](https://apxml.com/courses/advanced-diffusion-architectures/chapter-6-advanced-sampling-optimization/stochastic-sampling-variants)\n",
    "[5](https://arxiv.org/html/2410.02217v1)\n",
    "[6](https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2024.0503)\n",
    "[7](https://arxiv.org/html/2310.02279v3)\n",
    "[8](https://www.jgaubil.com/docs/gamma_ddim.pdf)\n",
    "[9](https://openaccess.thecvf.com/content/CVPR2024/papers/Miao_Training_Diffusion_Models_Towards_Diverse_Image_Generation_with_Reinforcement_Learning_CVPR_2024_paper.pdf)\n",
    "[10](https://milvus.io/ai-quick-reference/how-do-deterministic-sampling-methods-like-ddim-differ-from-stochastic-ones)\n",
    "[11](https://arxiv.org/abs/2508.03210)\n",
    "[12](https://arxiv.org/html/2507.05914v2)\n",
    "[13](https://milvus.io/ai-quick-reference/what-regularization-techniques-can-be-applied-to-diffusion-models)\n",
    "[14](https://developer.nvidia.com/blog/rethinking-how-to-train-diffusion-models/)\n",
    "[15](https://apxml.com/courses/intro-diffusion-models/chapter-5-sampling-generation-process/tradeoffs-ddpm-ddim)\n",
    "[16](https://arxiv.org/pdf/2010.02502.pdf)\n",
    "[17](https://openreview.net/forum?id=IIoH8bf5BA)\n",
    "[18](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)\n",
    "\n",
    "# Problem 2\n",
    "A major limitation of traditional reinforcement learning is its heavy reliance on extensive interactions with the real environment. For safety-critical applications such as autonomous driving, this requirement becomes impractical: real-world interaction is slow, expensive, and often dangerous, even if the learned policy can eventually reach a high success rate(by [\n",
    "Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic](https://openreview.net/forum?id=HygQBn0cYm)). To bypass these issues, my project adopts a model-predictive perspective and leverages a world model to replace most real-world interactions with simulated rollouts.\n",
    "\n",
    "Specifically, I focus on reproducing the DFM-KM MPC method from [Separating the World Model and Ego Models for Self-Driving](https://arxiv.org/abs/2204.07184). This framework separates the environment dynamics and the ego-vehicle dynamics: the ego state is predicted by a differentiable kinematic model, while a stochastic environment model predicts other vehicles. During planning, gradients are propagated only through the ego-model, allowing stable optimization without requiring the environment model to be differentiable. The cost function is implemented with differentiable masks, making the system interpretable and enabling us to verify whether the model learns the “right” behaviors.\n",
    "\n",
    "The goal of this project is to first reproduce the DFM-KM MPC pipeline and evaluate whether we can achieve performance comparable to the results reported in the original paper. This serves as a simplified and feasible first step toward building safer model-based decision-making systems for autonomous driving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
